Code complexity is a multidimensional concept that goes beyond the mere file structure of a project. It encompasses various factors that influence the intricacy of codebases. These factors include algorithmic complexity, such as the presence of nested loops or conditional statements that affect the execution flow and processing time. Additionally, the coupling and cohesion between classes and modules impact complexity. Highly interconnected components with extensive dependencies tend to introduce intricacies. Cyclomatic complexity, a metric based on control flow, evaluates the number of possible execution paths, revealing the intricacy of decision-making processes. Code duplication can also contribute to complexity, as it requires maintenance efforts and increases the chances of inconsistencies. Furthermore, the proper utilization of design patterns and architectural principles can simplify or complicate a project. It's important to consider these factors when assessing code complexity, allowing for a more accurate understanding beyond the surface-level file structure.
Using above information we will try to find which is the most complex project.
There are total 4 projects. Below we will get details about each of these projects one by one: 

1: Project Name : CreativeCaptions.ai-Language-model-that-can-create-creative-captions
This project contains 6 main files, namely /models_folder/dummy.txt, /README.md, /main.py, /main2.py, /modules.py, /requirements.txt
Below we will get details about each of these files one by one:
	
File no 1: /models_folder/dummy.txt


File no 2: /README.md
[Language-model-that-can-create-creative-captions] CreativeCaptions.ai Language Model that creates catchy, exciting, innovative, captivating, creative and engaging captions instead of just a description of the picture. Samples 1. Output 2. Output How to use it Environment setup Folder Structure Download the repository and make sure we have below file structure. CreativeCaptions.ai-Language-model-that-can-create-creative-captions/ | ├── images/ | |__ Images5.png | |__ Images6.png | ├── models_folder/ | |__ gpt2_medium_joke_insta.pt | |__ .. | ├── main.py | ├── main2.py | ├── modules.py | └── requirements.txt Change your working directory to CreativeCaptions.ai-Language-model-that-can-create-creative-captions. Download the model gpt2_medium_joke_insta.pt from https://www.kaggle.com/datasets/raj401/creativetext and store it in models_folder Dependencies Install below libraries. pip install sentencepiece pip install transformers pip install torch pip install fastapi pip install starlette pip install aiofiles pip install python-multipart pip install Pillow pip install uvicorn Inference Everything is set up now. We can now run python main2.py images/Image6.png from terminal to see the output for Image6. Next we wrap this codebase inside FastAPI kind of microservice. And we can use this as an API. To launch it on localhost http://127.0.0.1:8000, we run below cmd from the terminal. uvicorn main:app --reload Below is a demo of how our page looks. How it works: It addresses the problem in two steps: It converts an image into simple caption It converts simple captions into something more interesting/creative text. For the 1st step, I am using Encoder-Decoder type model(pretrained model from hugging face). For the 2nd step, I have used GPT-2 Model from Hugging face and fine-tuned it. In this step our model tries to change a sentence in a humorous/creative text given any input word or words it has never seen before. For this task , I took two datasets (short jokes from reddit + movie title puns). Below is some samples from the datasets Type Jokes from reddit movie title puns Output For the Jokes in our dataset we took only those which were question,answer types and started with Why,When,How,etc. Then processed all the data in this format <|soq|> question <|sep|> answer <|endoftext|> It looks like an input to Question answering system , only the whole string is treated as one string , instead of getting different token_type_ids for Questions and Asnwers References/Datasources short jokes from reddit: https://www.kaggle.com/datasets/abhinavmoudgil95/short-jokes movie title puns: https://www.kaggle.com/datasets/mikahama/movie-title-puns?select=humor_titles.csv Go You can’t perform that action at this time.

File no 3: /main.py
from main2 import * app = FastAPI(title="Image Captioning API", description="An API for generating caption for image.") class ImageCaption(BaseModel): caption: str @app.post("/predict/", response_model=ImageCaption) def predict(file: UploadFile = File(...)): # Load the image file into memory contents = file.file.read() image = Image.open(io.BytesIO(contents)) result = predict_step([image]) result = [clean_text(st) for st in result] return JSONResponse(content={"caption": result}) # Redirect the user to the documentation @app.get("/", include_in_schema=False) def index(): return RedirectResponse(url="/docs")

File no 4: /main2.py
from modules import * model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning") feature_extractor = ViTFeatureExtractor.from_pretrained("nlpconnect/vit-gpt2-image-captioning") tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning") device = torch.device("cuda" if torch.cuda.is_available() else "cpu") model.to(device) max_length = 16 num_beams = 4 gen_kwargs = {"max_length": max_length, "num_beams": num_beams} def predict_step(images_): images = [] for image_ in images_: i_image = image_ if i_image.mode != "RGB": i_image = i_image.convert(mode="RGB") images.append(i_image) pixel_values = feature_extractor(images=images, return_tensors="pt").pixel_values pixel_values = pixel_values.to(device) output_ids = model.generate(pixel_values, **gen_kwargs) preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True) preds = [pred.strip() for pred in preds] preds1 = make_creative(preds[0],64,3) return [preds[0]] + preds1 # HElper Function def choose_from_top(probs, n=5): ind = np.argpartition(probs, -n)[-n:] top_prob = probs[ind] top_prob = top_prob / np.sum(top_prob) # Normalize choice = np.random.choice(n, 1, p = top_prob) token_id = ind[choice][0] return int(token_id) ## Start Creative Captioning class config: Tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium') # Model Loading model_inst = GPT2LMHeadModel.from_pretrained('gpt2-medium') special_tokens_dict = {'pad_token': '<PAD>','bos_token':'<soq>','sep_token':'<eoq>'} num_added_toks = config.Tokenizer.add_special_tokens(special_tokens_dict) print('We have added', num_added_toks, 'tokens') model_inst.resize_token_embeddings(len(config.Tokenizer)) #loading Model state models_path = "models_folder\gpt2_medium_joke_insta.pt" #model.load_state_dict(torch.load(models_path, map_location=torch.device('cpu'))) model_inst.load_state_dict(torch.load(models_path, map_location=torch.device('cpu'))) model_inst.to(device) def make_creative(start_of_joke,length_of_joke=96,number_of_jokes=2): joke_num = 0 model_inst.eval() all_jokes = [] with torch.no_grad(): for joke_idx in range(number_of_jokes): joke_finished = False cur_ids = torch.tensor(config.Tokenizer.encode(start_of_joke)).unsqueeze(0).to(device) for i in range(length_of_joke): outputs = model_inst(cur_ids, labels=cur_ids) loss, logits = outputs[:2] softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding if i < 3: n = 20 else: n = 3 next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) select the next word cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence if next_token_id in config.Tokenizer.encode('<|endoftext|>'): joke_finished = True break if joke_finished: joke_num = joke_num + 1 output_list = list(cur_ids.squeeze().to('cpu').numpy()) output_text = config.Tokenizer.decode(output_list) #print(output_text+'\n') all_jokes.append(output_text) return all_jokes # # Start Predicting # predict("How do you feel",64,1) def clean_text(st): # 'a man kicking a soccer ball on a field <PAD> a man kicking a soccer ball on a field <|endoftext|>' st=st.split("<eoq>")[-1] for k,j in enumerate([" ", ""]): for i in ["<soq>", "<eoq>" , "<|endoftext|>", "<PAD>"]: if k ==0: st= st.replace(j+i+j, " ") else: st= st.replace(j+i+j, "") return st.strip() if __name__ == "__main__": path = sys.argv[1] img = Image.open(path) result = predict_step([img]) result = [clean_text(st) for st in result] for r in result: print(r)

File no 5: /modules.py
from fastapi import FastAPI, File, UploadFile from fastapi.responses import JSONResponse, HTMLResponse, RedirectResponse from pydantic import BaseModel import io import json import requests from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer import torch from PIL import Image # Preliminaries import os import sys import numpy as np import pandas as pd #transformers from transformers import GPT2LMHeadModel from transformers import GPT2Tokenizer # Pytorch import torch import torch.nn as nn #warnings import warnings warnings.filterwarnings('ignore') # My Module #import config

File no 6: /requirements.txt
﻿aiofiles==23.1.0 anyio==3.7.0 certifi==2023.5.7 charset-normalizer==3.1.0 click==8.1.3 colorama==0.4.6 fastapi==0.95.2 filelock==3.12.0 fsspec==2023.5.0 h11==0.14.0 huggingface-hub==0.15.1 idna==3.4 Jinja2==3.1.2 MarkupSafe==2.1.3 mpmath==1.3.0 networkx==3.1 numpy==1.24.3 packaging==23.1 pandas==2.0.2 Pillow==9.5.0 pydantic==1.10.8 python-dateutil==2.8.2 python-multipart==0.0.6 pytz==2023.3 PyYAML==6.0 regex==2023.5.5 requests==2.31.0 sentencepiece==0.1.99 six==1.16.0 sniffio==1.3.0 starlette==0.27.0 sympy==1.12 tokenizers==0.13.3 torch==2.0.1 tqdm==4.65.0 transformers==4.29.2 typing_extensions==4.6.3 tzdata==2023.3 urllib3==2.0.2 uvicorn==0.22.0

2: Project Name : EV-Enrouting
This project contains 6 main files, namely /report/Challenges_EV.pdf, /report/InternCertificateAman.pdf, /report/Solutions_EV.pdf, /src/configs.py, /src/utils.py, /README.md
Below we will get details about each of these files one by one:
	
File no 1: /report/Challenges_EV.pdf
EV-Enrouting/report/Challenges_EV.pdf at main · ar8372/EV-Enrouting · GitHub Skip to content Toggle navigation Sign up In this repository All GitHub ↵ Jump to ↵ No suggested jump to results In this repository All GitHub ↵ Jump to ↵ In this user All GitHub ↵ Jump to ↵ In this repository All GitHub ↵ Jump to ↵ Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. ar8372 / EV-Enrouting Public Notifications Fork 0 Star 5 Permalink main Switch branches/tags Branches Tags Name already in use A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch? EV-Enrouting/report/Challenges_EV.pdf Go to file Go to file T Go to line L Copy path Copy permalink This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository. Cannot retrieve contributors at this time 541 KB Download Open with Desktop Download Delete file Sorry, something went wrong. Reload? Sorry, we cannot display this file. Sorry, this file is invalid so it cannot be displayed. Viewer requires iframe. Go You can’t perform that action at this time.

File no 2: /report/InternCertificateAman.pdf
EV-Enrouting/report/InternCertificateAman.pdf at main · ar8372/EV-Enrouting · GitHub Skip to content Toggle navigation Sign up In this repository All GitHub ↵ Jump to ↵ No suggested jump to results In this repository All GitHub ↵ Jump to ↵ In this user All GitHub ↵ Jump to ↵ In this repository All GitHub ↵ Jump to ↵ Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. ar8372 / EV-Enrouting Public Notifications Fork 0 Star 5 Permalink main Switch branches/tags Branches Tags Name already in use A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch? EV-Enrouting/report/InternCertificateAman.pdf Go to file Go to file T Go to line L Copy path Copy permalink This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository. Cannot retrieve contributors at this time 517 KB Download Open with Desktop Download Delete file Sorry, something went wrong. Reload? Sorry, we cannot display this file. Sorry, this file is invalid so it cannot be displayed. Viewer requires iframe. Go You can’t perform that action at this time.

File no 3: /report/Solutions_EV.pdf
EV-Enrouting/report/Solutions_EV.pdf at main · ar8372/EV-Enrouting · GitHub Skip to content Toggle navigation Sign up In this repository All GitHub ↵ Jump to ↵ No suggested jump to results In this repository All GitHub ↵ Jump to ↵ In this user All GitHub ↵ Jump to ↵ In this repository All GitHub ↵ Jump to ↵ Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. ar8372 / EV-Enrouting Public Notifications Fork 0 Star 5 Permalink main Switch branches/tags Branches Tags Name already in use A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch? EV-Enrouting/report/Solutions_EV.pdf Go to file Go to file T Go to line L Copy path Copy permalink This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository. Cannot retrieve contributors at this time 2.31 MB Download Open with Desktop Download Delete file Sorry, something went wrong. Reload? Sorry, we cannot display this file. Sorry, this file is invalid so it cannot be displayed. Viewer requires iframe. Go You can’t perform that action at this time.

File no 4: /src/configs.py
import numpy as np class _globals: def __init__(self): self.size = 1000 self.noise = False self.battery = True self.find_optimal_CS = False self.static_quality_score = False self.dynamic_quality_score = False self.demo = False #---------------------------- self.tile_size = 10 self.drag = False self.Last_CS = (-100, -100) self.HOME = [] self.AGENT = 0 self.Grid_shape = (50,50) #(100,100) # later set automatically self.no_actions = 4 self.terminatos = [] self.optimal_path = np.zeros((50,50)) self.non_path_reward = -80 ######################################## self.optimal_value = 0 self.state =0 self.can_initial_reward = -1 ## self.optimal_policy = 0 def load(self, no): pass """ ## FUNCTIONALITIES size = 1000 #1000 #500 noise = False battery = True find_optimal_CS = False static_quality_score = False dynamic_quality_score = False demo = False ##################################### tile_size = 10 drag = False Last_CS = (-100,-100) # something which is not in 50*50 HOME = [] #0 AGENT = 0 Grid_shape = (50,50) #(100,100) # later set automatically no_actions = 4 terminatos = [] optimal_path = np.zeros((50,50)) non_path_reward = -80 ######################################## optimal_value = 0 state =0 can_initial_reward = -1 ## optimal_policy = 0 """

File no 5: /src/utils.py
import numpy as np import pandas as pd import cv2 import matplotlib.pyplot as plt import seaborn as sns import random #sns.set(rc={'figure.figsize':(11.7,8.27)}) #sns.set(rc={'figure.figsize':(8,10)}) #plt.rcParams['figure.figsize']= (5,5) #!pip install plotly #!pip install cufflinks from plotly import __version__ print(__version__) import cufflinks as cv from plotly.offline import download_plotlyjs, init_notebook_mode, plot,iplot init_notebook_mode(connected=True) cv.go_offline() #pd.DataFrame(new_value).iplot(kind='surface',) import sys sys.setrecursionlimit(4000) #2000 print(sys.getrecursionlimit()) size = 500 width= 10 # Noise from sklearn.datasets import make_blobs import time from configs import * def CreateNoiseImage(input_image): global size global noise_matrix data = make_blobs(n_samples= 2800, n_features= 13, centers= 4, cluster_std =3, random_state=1) #plt.figure(figsize=(8,8)) #plt.scatter(data[0][:,0],data[0][:,1], c = data[1], cmap='rainbow') if size==1000: data1=(((data[0]-np.min(data[0]))/(np.max(data[0])-np.min(data[0])))*860).astype(int) elif size== 500: data1=(((data[0]-np.min(data[0]))/(np.max(data[0])-np.min(data[0])))*460).astype(int) data1[:,1] += 10 noise_matrix = -1*np.ones((size,size,3)) for i,j in data1[:,:2]: noise_matrix[j,i,:] = -5 import matplotlib.pyplot as plt """ im = plt.imread('../Images/img4.jpg')[:size,:size,:] #[:1000,:1000,:] # img4.jpg plt.figure(figsize=(10,10)) implot = plt.imshow(im) plt.scatter(data1[:,0],data1[:,1], c = data[1], cmap='rainbow') plt.axis('off') plt.savefig("../Images/img4_noise.jpg",bbox_inches='tight',transparent=True, pad_inches=0) plt.show() """ fig = plt.figure() im = input_image plt.figure(figsize=(10,10)) implot = plt.imshow(im) plt.scatter(data1[:,0],data1[:,1], c = data[1], cmap='rainbow') plt.axis('off') plt.savefig("../Images/img4_noise.jpg",bbox_inches='tight',transparent=True, pad_inches=0) plt.close() out_image = plt.imread("../Images/img4_noise.jpg") #plt.show() return out_image def Calculate_noise(img): global width global size mean_img = img.copy() img_transition_value = np.zeros((size//width,size//width)) for i in range(0,size,width): for j in range(0,size,width): # got all the points t = img[i:i+width,j:j+width,:] no_5 = (t==-5).sum() #pixel_value = np.max(img[i:i+width,j:j+width,:]) #if pixel_value==-5: # print(pixel_value) if no_5 <= 3: #200 didn't work mean_img[i:i+width,j:j+width,:] = 250 else: mean_img[i:i+width,j:j+width,:] = 0 #pixel_value #kernel = np.ones((5,5), np.uint8) #img_dilation = cv2.dilate(mean_img, kernel, iterations=1) img_dilation = mean_img.copy() img_transition_value=np.where(img_dilation<230,0,255) # 230 #cv2.imwrite("dilated_5_2.jpg",img_transition_value) # final image look BGR looks black and white img_transition_value=img_transition_value[::width,::width,0] img_transition_value=np.where(img_transition_value<120,-7,-1) # 230 return img_transition_value img = cv2.imread("../Images/img4.jpg")[:size,:size,:] #[:1000,:1000,:] def Calculate_transition_matrix(img): global noise_matrix global width global size global noise mean_img = img.copy() img_transition_value = np.zeros((size//width,size//width)) for i in range(0,size,width): for j in range(0,size,width): # got all the points pixel_value = np.mean(img[i:i+width,j:j+width,:]) if pixel_value >210: #200 didn't work mean_img[i:i+width,j:j+width,:] = 250 else: mean_img[i:i+width,j:j+width,:] = 0 #pixel_value #kernel = np.ones((5,5), np.uint8) #img_dilation = cv2.dilate(mean_img, kernel, iterations=1) img_dilation = mean_img.copy() img_transition_value=np.where(img_dilation<230,0,255) # 230 #cv2.imwrite("dilated_5_2.jpg",img_transition_value) # final image look BGR looks black and white img_transition_value=img_transition_value[::width,::width,0] img_transition_value=np.where(img_transition_value<120,-80,-1) # 230 ## noise #i1 = cv2.imread("noise1.PNG") #i1=cv2.resize(i1,(size,size)) #img_transition_value = np.minimum(i2,img_transition_value) if noise == True: i2=Calculate_noise(noise_matrix) img_transition_value = np.minimum(img_transition_value,i2 ) ## #print("final reward") #sns.heatmap(img_transition_value) #plt.show() return img_transition_value def Sweep(Value, Policy,terminators,transition_value): # create transition matrix globally that is not possible since we will create dynamic wall New_Value = Value.copy() delta = 0 x,y = Value.shape for i in range(x): # x left = 0 right = 0 top = 0 bottom = 0 for j in range(y): # y # (i,j) if (i,j) in terminators: pass else: same= Value[i,j] if i==0: left = same else: left = Value[i-1,j] if i==x-1: right= same else: right = Value[i+1,j] if j==0: top= same else: top = Value[i,j-1] if j==y-1: bottom= same else: bottom = Value[i ,j+1] # if (i,j) in red: # transition_reward = -10 # else: # transition_reward = -1 transition_reward = transition_value[i,j] total_value = Policy[(y)*i+j,0]*(transition_reward+left) + Policy[(y)*i+j,1]*(transition_reward+right) + Policy[(y)*i+j,2]*(transition_reward+top) + Policy[(y)*i+j,3]*(transition_reward+bottom) #print(total_value, New_Value[i,j]) delta = max(delta, np.abs(total_value- Value[i,j])) New_Value[i,j] = total_value #print("Sweep") return New_Value, delta def Improve_Policy(Value, Policy): New_Policy = Policy.copy() #terminators = (0,1), (0,2),(0,3), (2,0),(2,1),(2,2) x,y = Value.shape for i in range(x): for j in range(y): # y # (i,j) left = 0 right = 0 top = 0 bottom = 0 same= Value[i,j] if i==0: left = same else: left = Value[i-1,j] if i==x-1: right= same else: right = Value[i+1,j] if j==0: top= same else: top = Value[i,j-1] if j==y-1: bottom= same else: bottom = Value[i ,j+1] my_list = [left,right, top,bottom] max_val = max(my_list) my_list = np.array(my_list) my_list = (my_list==max_val).astype(int) my_list = my_list/np.sum(my_list) New_Policy[(y)*i+j,:] = my_list #print("Improve_Policy") return New_Policy call =0 def Evaluate_value(Value, Policy,theta,terminators,transition_value): Policy = Policy.copy() theta = theta terminators = terminators.copy() Policy = Policy.copy() global call call += 1 #terminators = (0,1), (0,2),(0,3), (2,0),(2,1),(2,2) delta = float(np.inf) while delta> theta: delta = 0 Value, delta = Sweep(Value, Policy, terminators,transition_value) #print(delta,theta, delta>theta) #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> New_Policy = Improve_Policy(Value, Policy) if np.all(Policy==New_Policy): #print("Evaluate_value_") return Value, New_Policy, theta, terminators else: # repeat #print("Evaluate_value") return Evaluate_value(Value, New_Policy,theta, terminators,transition_value) def Calculate_optimal(Grid_shape,no_actions, terminators,img_mask): global img global find_optimal_CS global Charging_time_CS global static_quality_score global dynamic_quality_score global transition_value global actual_dynamic_overhead global actual_static_overhead # we can't add charging time before we calculate rem overhead x,y = Grid_shape State_action = np.ones((x*y,no_actions))*(1/no_actions) ####### L R T B Value = np.zeros((x,y)) theta = 0.1 #0.001 transition_value = Calculate_transition_matrix(img_mask) # for i,j in HOME: # transition_value[i,j] = 0 for no, (i,j) in enumerate(HOME): transition_value[i,j] = 0 if static_quality_score == True or dynamic_quality_score == True: if i != 0: # not touching left edge transition_value[i,j-1] = min(transition_value[i,j-1], -1*(actual_static_overhead[no]+actual_dynamic_overhead[no])-1) if i != 49: # not touching right edge transition_value[i,j+1] = min(transition_value[i,j+1], -1*(actual_static_overhead[no]+actual_dynamic_overhead[no])-1) if j != 0: # not touching top edge transition_value[i-1,j] = min(transition_value[i-1,j], -1*(actual_static_overhead[no]+actual_dynamic_overhead[no])-1) if j != 49: # not touching bottom edge transition_value[i+1,j] = min(transition_value[i+1,j], -1*(actual_static_overhead[no]+actual_dynamic_overhead[no])-1) # print("REWARD of each state:-") # sns.heatmap(transition_value) # plt.show() # Value[17,24] = 30 # terminators.append((17,24)) # print("terminators",terminators) #transition_value[25,25] = 1 #img[25*width-10-70:25*width+10-70,25*width-10:25*width+10,:] = 0 #Value[HOME] = 0 # No matter how low you make it (-4000) this will become optimal point because everywhere else value is calculated l,m,th,tr =Evaluate_value(Value,State_action,79, terminators, transition_value) #l:- value function m:- optimal action if find_optimal_CS == False and dynamic_quality_score== False: print('-'*40) print("calculating FUTURE REWARD of each state...") print("-"*40) print(l) print('FUTURE REWARD/Value Functions:-') # g2=sns.heatmap(l) # g2.tick_params(left=False) # remove the ticks #tick_params(left=False) # remove the ticks # plt.tight_layout() # plt.show() if dynamic_quality_score == False: f,(ax1,ax2) = plt.subplots(1,2,sharey=True,figsize=(18,6)) g1 = sns.heatmap(transition_value,ax=ax1) g1.set_ylabel('') g1.set_xlabel('') g2 = sns.heatmap(l,ax=ax2) g2.set_ylabel('') g2.set_xlabel('') plt.show() # layout1 = cv.Layout( # height=500, # width=500 # ) if static_quality_score == True and dynamic_quality_score == False: # 3d plot new_value = (l-np.min(l))/(np.max(l)-np.min(l)) pd.DataFrame(new_value).iplot(kind='surface',) # layout= layout1) #print("Calculate_optimal") # 80 for -100 , 9.5 for -10 , 100 for -1000 60 for -80 return l,m def recheck(x,y): global HOME global optimal_value global AGENT global dynamic_quality_score global actual_dynamic_overhead global actual_static_overhead global terminators global state global battery global Overhead_time_CS global Charging_time_CS global no_steps global stats_df index_no = HOME.index((x,y)) time_taken = -1* optimal_value[AGENT[0],AGENT[1]] rem_overhead = max(0,Overhead_time_CS[index_no] - time_taken) stats_df.loc[index_no,"CS_pos"] = str((x,y)) stats_df.loc[index_no,"travel_time"] = time_taken stats_df.loc[index_no, "rem_overhead"] = rem_overhead stats_df.loc[index_no, "total time"] = time_taken + rem_overhead + Charging_time_CS[index_no] if rem_overhead <= 0: # means none can be better than this cs. # stop #print(f"Overhead on the CS is {Overhead_time_CS[index_no]}") #print(f"Time taken to reach the CS is {time_taken}") #print(f"No of steps is {no_steps}") #print(f"Overhead on reaching CS {(x,y)} is: {rem_overhead} thus Total Time: {time_taken + rem_overhead}") print(stats_df) if battery == True: #print(f"REACHED_DEST [{x},{y}] in {no_steps} steps, time taken {-1*optimal_value[AGENT[0],AGENT[1]]} minutes") print(f"REACHED_DEST [{x},{y}] in {no_steps} steps, time taken {time_taken + rem_overhead + Charging_time_CS[index_no]} minutes") else: print(f"REACHED_DEST [{x},{y}] in {no_steps} steps, with reward of {optimal_value[AGENT[0],AGENT[1]]-rem_overhead-Charging_time_CS[index_no]}") print('-'*40) return else: # change transition matrix #print(f"Overhead on the CS is {Overhead_time_CS[index_no]}") #print(f"Time taken to reach the CS is {time_taken}") #print(f"No of steps is {no_steps}") #print(f"Overhead on reaching CS {(x,y)} is: {rem_overhead} thus Total Time: {time_taken + rem_overhead}") #print("Calculating...") print(stats_df) actual_dynamic_overhead[index_no] = rem_overhead actual_static_overhead[index_no] = Charging_time_CS[index_no] state = 112 """ Grid_shape = transition_matrix.shape no_actions = 4 optimal_value,optimal_policy = Calculate_optimal(Grid_shape,no_actions, terminators,img_mask) """ optimal_path = np.zeros((50,50)) collect_pts = [] def trace_path(optimal_value, optimal_policy,initial_pt): global optimal_path global width global collect_pts global dynamic_quality_score global Last_CS x,y = initial_pt # 48,25 if (x,y) in HOME: #== HOME: if dynamic_quality_score == True and (Last_CS != (x,y) or (x,y)== HOME[-1]): # we will check once again Last_CS = (x,y) recheck(x,y) else: # stop print("REACHED HOME") return if x<0 or x>= 50 or y<0 or y>= 50: # stop execution return # Plot a1 = width*y b1 = width*x a2 = width*y + width b2 = width*x + width # policy_index = x*50 + y l,r,t,b = optimal_policy[policy_index,:] max_index = (l,r,t,b).index(max(l,r,t,b)) # pick the first optimal index indexes = [i for i,j in enumerate((l,r,t,b)) if j==max(l,r,t,b)] max_index = random.choice(indexes) if max_index==0: # left #cv2.circle(img, ((a1+a2)//2,(b1+b2)//2),3,(0,250,0),2) #cv2.line(img,((a1+a2)//2,b1),((a1+a2)//2,b2), (0,0,0),3) trace_path(optimal_value, optimal_policy,(x-1,y)) optimal_path[x-1:x,y:y+1] += 1 elif max_index==1: # right #cv2.circle(img, ((a1+a2)//2,(b1+b2)//2),3,(0,250,0),2) #cv2.line(img,((a1+a2)//2,b1),((a1+a2)//2,b2), (0,0,0),3) trace_path(optimal_value, optimal_policy,(x,y+1)) optimal_path[x:x+1,y+1:y+2] += 1 elif max_index==2: # top #cv2.circle(img, ((a1+a2)//2,(b1+b2)//2),3,(0,250,0),2) #cv2.line(img,(a1,(b1+b2)//2),(a2,(b1+b2)//2), (0,0,0),3) trace_path(optimal_value, optimal_policy,(x,y-1)) optimal_path[x:x+1,y-1:y] += 1 elif max_index==3: # bottom #cv2.circle(img, ((a1+a2)//2,(b1+b2)//2),3,(0,250,0),2) #cv2.line(img,(a1,(b1+b2)//2),(a2,(b1+b2)//2), (0,0,0),3) trace_path(optimal_value, optimal_policy,(x,y+1)) optimal_path[x:x+1,y+1:y+2] += 1 else: print("ERROR") def draw_path(): global optimal_path global img global width global size print('calculating') Full_matrix = (np.repeat(np.repeat(optimal_path,width,axis=0),width,axis=1)*500).astype(int) img = ((np.clip((img - Full_matrix.reshape(size,size,1)).astype(int),0,255)).astype(int)).astype(np.uint8) def smart_trace(): #print('tracing path..') print("="*80) global optimal_value global optimal_policy global width global size global battery global dynamic_quality_score global actual_dynamic_overhead global Last_CS global no_checked_CS global no_steps global stats_df global Overhead_time_CS global HOME global Charging_time_CS if width == 20: w = 10 elif width == 10: w = 10 print("this is optimal pollicy") print(optimal_policy) T =np.argmax(optimal_policy, axis=1) x_dict = {0:-1,1:1,2:0,3:0} y_dict = {0:0,1:0,2:-1,3:1} x_axis=np.vectorize(x_dict.get)(T) y_axis=np.vectorize(y_dict.get)(T) #f_x ,f_y = HOME in_x, in_y = AGENT no_steps = 0 if width == 20: r = 2 elif width == 10: r = 3 while (in_x,in_y) not in HOME: #in_x != f_x or in_y != f_y: no_steps += 1 m = w*in_x l = w*in_y cv2.circle(img, ((2*l+w)//2,(2*m+w)//2),r,(0,255,0),5) new_loc = 50*in_x + in_y in_x += x_axis[new_loc] in_y += y_axis[new_loc] #print("action",x_axis[new_loc],y_axis[new_loc]) #print("reached",in_x,in_y) if l<0 or l>999 or m <0 or m>999: print('Not found',(2*l + w),(2*m + w)) return x,y = in_x, in_y # 48,25 if (x,y) in HOME: #== HOME: if dynamic_quality_score == True and (Last_CS != (x,y)) and no_checked_CS != len(actual_dynamic_overhead): # we will check once again no_checked_CS += 1 Last_CS = (x,y) recheck(x,y) else: # stop rem_overhead = 0 Charge_t = 0 if dynamic_quality_score == True: index_no = HOME.index((x,y)) time_taken = -1* optimal_value[AGENT[0],AGENT[1]] rem_overhead = max(0,Overhead_time_CS[index_no] - time_taken) stats_df.loc[index_no,"CS_pos"] = str((x,y)) stats_df.loc[index_no,"travel_time"] = time_taken stats_df.loc[index_no, "rem_overhead"] = rem_overhead stats_df.loc[index_no, "total time"] = time_taken + rem_overhead + Charging_time_CS[index_no] Charge_t = Charging_time_CS[index_no] print(stats_df) if battery == True: print(f"REACHED_DEST [{in_x},{in_y}] in {no_steps} steps, time taken {-1*optimal_value[AGENT[0],AGENT[1]]+rem_overhead+Charge_t} minutes") else: print(f"REACHED_DEST [{in_x},{in_y}] in {no_steps} steps, with reward of {optimal_value[AGENT[0],AGENT[1]]-rem_overhead-Charge_t}") print('-'*40) return def Calculate_Cost(index): global drag global state global HOME global AGENT global optimal_value global img global Grid_shape global no_action global terminators global optimal_path global width global size global optimal_policy global time_limit img_mask = cv2.imread("img4.jpg")[:size,:size,:] terminators = [(index[0],index[1])] HOME = terminators v,p= Calculate_optimal(Grid_shape,no_actions, terminators,img_mask) return v,p def Optimize_CS(): global non_path_reward #-80 global tile_size List_indices = [] List_costs = [] img_mask = cv2.imread("img4.jpg")[:size,:size,:] transition_value = Calculate_transition_matrix(img_mask) """ HOME = [(3,3)] for i,j in HOME: transition_value[i,j] = 0 """ mask = transition_value> non_path_reward+2 #print(np.where(mask==1)) # pick tile_size which can divide 50 like 10/50/2 etc no_subcell = 50//tile_size for i in range(0,no_subcell): for j in range(0,no_subcell): # (i,j) # pick su x,y = np.where(mask[i*tile_size: i*tile_size+tile_size, j*tile_size: j*tile_size+tile_size]) x = (i*tile_size) + x y = (j*tile_size) + y my_list = list(zip(x,y)) if my_list == []: print("no road here") else: print(my_list) length = len(my_list) if length %2 != 0: # odd no of terms middle = my_list[(length-1)//2] else: # even no of terms middle = my_list[(length//2) -1] temp_transition_value = (transition_value[i*tile_size: i*tile_size+tile_size, j*tile_size: j*tile_size+tile_size]).copy() print(f"Calculating cost for {middle}") temp_transition_value[middle[0]-(i*tile_size),middle[1]-(j*tile_size)] = 10 plt.figure(figsize=(3,2)) sns.heatmap(temp_transition_value) plt.show() List_indices.append(middle) v,p = Calculate_Cost(middle) cost = np.sum(v[mask]) List_costs.append(cost) #print(np.random.choice(my_list, size=8)) print(middle,"=",cost) print("#"*80) print(List_indices) print("+"*40) print(List_costs) print("+"*40) print("Optimal CS location") print(List_indices[List_costs.index(max(List_costs))], max(List_costs)) Calculate_Cost(List_indices[List_costs.index(max(List_costs))]) return [List_indices[List_costs.index(max(List_costs))]] def draw(event, x, y, flag, param): global drag global state global HOME global AGENT global optimal_value global img global Grid_shape global no_action global terminators global optimal_path global width global size global optimal_policy global find_optimal_CS global Last_CS global count if width == 20: # big rectangle w = 10 elif width == 10: w = 10 if event == cv2.EVENT_FLAG_LBUTTON: drag = True if event == cv2.EVENT_LBUTTONUP: drag=False #cv2.circle(img, (x,y),20,(0,0,0),3) if state == 111: """ AGENT = [y//w, x//w] print("AGENT location") print(AGENT) cv2.rectangle(img, (x+w,y-w),(x-w, y+w), (250,0,0),3 ) smart_trace() state=3 """ #- if demo== True: x = AGENT[1]*w y = AGENT[0]*w else: AGENT = [y//w, x//w] #- print("AGENT location") print(AGENT) cv2.rectangle(img, (x+w,y-w),(x-w, y+w), (250,0,0),3 ) # Limit Battery t = optimal_value[AGENT[0],AGENT[1]] if t< -1*time_limit and battery == True: print(f"Nearest Charging station is at {-1*t} second distance") print('*'*80) else: smart_trace() state=3 if state == 0: if find_optimal_CS == True: HOME= Optimize_CS()[::-1] x = HOME[0][1]*w y = HOME[0][0]*w find_optimal_CS = False else: #- if demo == True: temp = HOME[count][0]*w, HOME[count][1]*w x = HOME[count][1]*w y = HOME[count][0]*w count += 1 else: temp = (y//w,x//w) HOME.append(temp) print("CS locations") print(HOME) cv2.rectangle(img, (x+w,y-w),(x-w,y+w),(0,0,250),3) #state =1 elif state == 1: # draw car if demo== True: x = AGENT[1]*w y = AGENT[0]*w else: AGENT = [y//w, x//w] #- print("AGENT location") print(AGENT) cv2.rectangle(img, (x+w,y-w),(x-w,y+w),(250,0,0),3) #state = 2 elif state == 2: state = 3 # show color terminators = HOME #print("terminating locations") #print(terminators) img_mask = cv2.imread("../Images/img4.jpg")[:size,:size,:] #[:1000,:1000,:] # img4.jpg # this is the fresh copy not affected by any thing #print() #print("Calculating...") optimal_value,optimal_policy = Calculate_optimal(Grid_shape,no_actions, terminators,img_mask) l,m = optimal_value.shape # (100,100) #img[HOME[0]*10:HOME[0]*10+10,HOME[1]*10:HOME[0]*10+10,:] = (255,0,0) it is just to verify that we are indeed at right location # Method1 # Limit Battery t = optimal_value[AGENT[0],AGENT[1]] if t< -1*time_limit and battery == True: print(f"Nearest Charging station is at {-1*t} second distance") print('*'*80) else: smart_trace() #smart_trace() #plt.figure(figsize=(8,3), dpi=80) # sns.heatmap(optimal_value) # g2=sns.heatmap(optimal_value) # g2.tick_params(left=False) # remove the ticks #tick_params(left=False) # remove the ticks # plt.tight_layout() # plt.show() # Method2 """ trace_path(optimal_value,optimal_policy,AGENT) draw_path() f, axes = plt.subplots(1, 2) sns.heatmap( data=optimal_value, ax=axes[0]) sns.heatmap( data=optimal_path, ax=axes[1]) """ elif state == 3: #start game pass elif state == 4: #print("CAlled state 4") # # New map img state=5 if dynamic_quality_score == True: img_mask = cv2.imread("../Images/img4.jpg")[:size,:size,:] img = img_mask.copy() else: img_mask = img.copy() # img4.jpg # this is the fresh copy not affected by any thing for H in HOME: cv2.rectangle(img, (H[1]*w+w,H[0]*w-w),(H[1]*w-w,H[0]*w+w),(0,0,250),3) cv2.rectangle(img, (AGENT[1]*w+w,AGENT[0]*w-w),(AGENT[1]*w-w,AGENT[0]*w+w),(250,0,0),3) #print() #print("Calculating...") optimal_value,optimal_policy = Calculate_optimal(Grid_shape,no_actions, terminators,img_mask) l,m = optimal_value.shape # (100,100) # Method1 # Limit Battery t = optimal_value[AGENT[0],AGENT[1]] if t< -1*time_limit and battery == True: print(f"Nearest Charging station is at {-1*t} second distance") print('*'*80) else: smart_trace() #smart_trace() #plt.figure(figsize=(8,3), dpi=80) # g2=sns.heatmap(optimal_value) # g2.tick_params(left=False) # remove the ticks # plt.tight_layout() # plt.show() # Method2 [it was making linear search which was computationally heavy] """ #trace_path(optimal_value,optimal_policy,AGENT) #img[HOME[0]*10:HOME[0]*10+10,HOME[1]*10:HOME[0]*10+10,:] = (255,0,0) it is just to verify that we are indeed at right location #draw_path() #f, axes = plt.subplots(1, 2) #sns.heatmap( data=optimal_value, ax=axes[0]) #sns.heatmap( data=optimal_path, ax=axes[1]) """ if event == cv2.EVENT_MOUSEMOVE: if drag ==True: #state=4 cv2.circle(img, (x,y),5,(0,0,0),15) #cv2.circle(img, (x,y),2,(0,0,0),4) if state == 4 and dynamic_quality_score == True: # # New map img print("NEXT-->") state=5 if dynamic_quality_score == True: img_mask = cv2.imread("../Images/img4.jpg")[:size,:size,:] img = img_mask.copy() else: img_mask = img.copy() # img4.jpg # this is the fresh copy not affected by any thing for H in HOME: cv2.rectangle(img, (H[1]*w+w,H[0]*w-w),(H[1]*w-w,H[0]*w+w),(0,0,250),3) cv2.rectangle(img, (AGENT[1]*w+w,AGENT[0]*w-w),(AGENT[1]*w-w,AGENT[0]*w+w),(250,0,0),3) #print() #print("Calculating...") optimal_value,optimal_policy = Calculate_optimal(Grid_shape,no_actions, terminators,img_mask) l,m = optimal_value.shape # (100,100) # Method1 # Limit Battery t = optimal_value[AGENT[0],AGENT[1]] if t< -1*time_limit and battery == True: print(f"Nearest Charging station is at {-1*t} second distance") print('*'*80) else: smart_trace()

File no 6: /README.md
EV-Enrouting Find optimal path for electric vehicle using spatial information. Preprocessing :- Our algorithm solves 3 main problems: Finding Best Route Finding Optimal Location to setup a Charging Station Dealing with Overhead on Charging Stations Below we will see demo of these:- 1.Finding Best Route: a) Base problem Our algorithm gives us the best route which will take the shortest path out of all the available charging stations. b) Now with traffic data The benefit of this grid approach over graph approach is that we can very easily put layers of information on top and our algorithm will work fine. Like here we have added traffic information on top of it. c) With remaining battery We can also pass the information about available battery power and it will tell us if there is any charging station within our battery limit. We have given remaining battery of 100 minutes and now we see that many times it is not showing us any path because it is out of range of 100 minutes. Let's look at details of how it is working:- 2.Finding Optimal Location to setup a Charging Station: Finding the optimal location to set up a charging station is very tricky and we have to look at various factors, like where there is more demand and which is geographically the most feasible location from all places. So to solve this we applied three approaches. a) Brute force approach b) Sliding Window Technique c) Subblocks Technique a) In brute force approach we calculated total return by putting CS at each point of grid (50*50) and the point corresponding to maximum total return is the optimal point. b) In sliding window we took a window of 10*10 and moved over this 50*50 matrix. c) In Sub-blocks Technique we divided our whole grid into 4 sub grids (upper left, upper right, lower left, lower right). Then we calculated the return of the median point for each sub grid. We repeat the same for subgrid with max total return. [Optimality] Brute Force Approach >> Sliding Window Technique >> Sub-blocks method [Speed] Sub-blocks method >> Sliding Window Technique >> Brute Force Approach (So there is tradeoff between Speed and Optimality) Sliding Window Techniqe Sliding Window with traffic note: we see that due to traffic the optimal charging station position has changed, which makes sense. 3.Overhead on Charging stations: For this I have defined two types of overhead on the charging stations. a) Dynamic overhead b) Static Overhead Dynamic Overhead tells how many cars are there in the queue, i.e. if we reach the Charging station now then after how much time we will get the turn. Static Overhead tells about on an average when a vehicle is plugged in for charging how much time it takes to get fully charged. Together these two help us find a charging station which is best for us at that current moment. [case1] : only Static Overhead Charging Station Static Overhead travel time total time Cs1 50 20 70 Cs2 10 28 38 Cs3 5 38 43 So our algorithm will choose Cs2 >> Cs3 >> Cs1 Let’s verify below. [case2] : both Dynamic and Static Overhead Charging Station Dynamic Overhead travel time remaining overhead:- max(0, Dynamic_overhead-travel_time) Static Overhead Total Overhead:- rem_overhead + static_overhead Total time (total_overhead + travel_time Cs1 40 20 20 50 70 90 Cs2 48 28 20 10 30 58 Cs3 27 38 0 5 5 43 So our algorithm will choose Cs3 >> Cs2 >> Cs1 [ In this I have applied greedy search which allows us to find optimal CS without the need of calculating travel_time of each and every CS] note: Greedy Search in our case gives optimal solution, details of this method can be found in the report Theory Theory Notebook link: https://colab.research.google.com/drive/1zSd24UZs3tKTVcDbJ61VH8CpsZ8QZA8d?usp=sharing Go You can’t perform that action at this time.

3: Project Name : Framework3
This project contains 43 main files, namely /resources/temp.py, /src-framework3/auto_exp.py, /src-framework3/auto_feat.py, /src-framework3/create_folds.py, /src-framework3/create_folds_again.py, /src-framework3/custom_classes.py, /src-framework3/custom_models.py, /src-framework3/datasets_create.py, /src-framework3/datasets_init.py, /src-framework3/dummy_exp.py, /src-framework3/ensemble.py, /src-framework3/exp_infos.txt, /src-framework3/experiment.py, /src-framework3/feature_generator.py, /src-framework3/feature_picker.py, /src-framework3/find_roots.py, /src-framework3/global_variables.py, /src-framework3/grab.py, /src-framework3/info.txt, /src-framework3/keys.py, /src-framework3/metrics.py, /src-framework3/model_dispatcher.py, /src-framework3/optuna_search.py, /src-framework3/output.py, /src-framework3/predict.py, /src-framework3/preprocess.py, /src-framework3/pull.py, /src-framework3/push.py, /src-framework3/ref.txt, /src-framework3/removed_dup_in_dict.py, /src-framework3/requirements.txt, /src-framework3/run.sh, /src-framework3/seed_it.py, /src-framework3/settings.py, /src-framework3/show_importance.py, /src-framework3/show_input.py, /src-framework3/show_table.py, /src-framework3/split_input.py, /src-framework3/submit.py, /src-framework3/submit_one.py, /src-framework3/update_table.py, /src-framework3/utils.py, /README.md
Below we will get details about each of these files one by one:
	
File no 1: /resources/temp.py


File no 2: /src-framework3/auto_exp.py
import random import numpy as np import pandas as pd import time import os import gc # import quantumrandom import ast # for literal import tensorflow as tf import pickle from datetime import datetime from utils import * from feature_picker import * from experiment import * from settings import * def make_selection_prep(my_list, shuffle=True, random_state=24): # The first thing we do is set random state to make it reproducible: fix_random(random_state) # finds all possible SELECTION possible # col_list = ["f_00","f_01","f_02","f_05"] new_list = [] for i in range(2 ** len(my_list)): val = int(bin(i)[2:]) temp_list = [] counter = 1 while val != 0: if val % 10 == 1: # take it temp_list.append(my_list[-counter]) val = val // 10 counter += 1 new_list.append(temp_list) ch = random.choice(new_list) if shuffle == True: random.shuffle(ch) return ch # for list of list we must append each other but that is a manual task def make_selection(my_list, shuffle=True, random_state=24): print("start-->") # The first thing we do is set random state to make it reproducible: fix_random(random_state) # finds all possible SELECTION possible # col_list = ["f_00","f_01","f_02","f_05"] i = random.randint(0,2 ** len(my_list)) val = int(bin(i)[2:]) temp_list = [] counter = 1 while val != 0: if val % 10 == 1: # take it temp_list.append(my_list[-counter]) val = val // 10 counter += 1 ch = temp_list if shuffle == True: random.shuffle(ch) print("done-->") return ch # for list of list we must append each other but that is a manual task def generate_random_no(adder="--|--"): # makes sure each time we are at differnet random state # random_state should only be used for reproducibility and should not give a better model seed = int(datetime.now().strftime("%H%M%S")) # seed selected based on current time if adder != "--|--": seed += adder # datetime can't give new no in vicinity of fraction of seconds so introducing this adder os.environ["PYTHONHASHSEED"] = str(seed) np.random.seed(seed) random.seed(seed) tf.random.set_seed( seed ) # f"The truth value of a {type(self).__name__} is ambiguous. " return seed # np.random.randint(3, 1000) # it should return 5 def sanity_check(table, *rest): global counter counter += 1 for index, row in table.iterrows(): row = row.values # row assert len(rest) == len(row[1:]) same = True for i,(val1, val2) in enumerate(zip(rest, row[1:])): # i=5 optimize_on [0,2] special case #print(i, val1, val2) #best way to check if allow duplicates if not isinstance(val1, list): if val1 != val2: # "fold5" "fold10" same = False break if isinstance(val1, list) and i != 5: # list of lenght 2 # val2 is a list inform of string '[10,0]' but not '[10]' val2 = ast.literal_eval(val2) # val2 = val2[1:-1].split(',') # it is a list if val1[1] == 1: # order matters if val1[0] != val2: same = False break else: # order don't matter set1 = set(val1[0]) set2 = set(val2) if set1 != set2: same = False break # elif i == 5: # # if this elif is not written then it will consider all different optimize on duplicate # # it is optimize on # val2 = ast.literal_eval(val2) # if val1 != val2: # same = False # break if same == True: # duplicate print(row, "found in table") return True return False def auto_select(): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() if comp_name == "amex": amex = amex_settings() elif comp_name == "amex2": amex = amex2_settings() elif comp_name == "amex3": amex = amex3_settings() elif comp_name == "amex4": amex = amex4_settings() elif comp_name == "getaroom": amex = getaroom_settings() else: raise Exception(f"comp name {comp_name} not valid") # ============================================================= # Features # # ============================================================== # seed it Fix random no for this auto_exp random_no = ( true_random(100) ) # never gives same random value quite random done to bring variabiligy # random_no = 111 # ============================================================ # each set must have a name in features for better visualisation # auto_features = ['ragnar'] # clean_features = amex.feature_dict2['ragnar'] """ auto_features = [] clean_features = [] v = list(amex.feature_dict2.keys()) auto_features = make_selection(v, shuffle=False, random_state=random_no) #amex.feature_keys #auto_features2 = random.choice([[random.choice(v)], []]) # f_base should always be there # # # deal with empty list while auto_features == []: print("features empty!!") #============================================================ # seed it random_no = (true_random(100)) #random_no = 111 #============================================================ auto_features = make_selection(v, shuffle=False, random_state = random_no) print() print(auto_features) print() # #------------------------------------------------ # for list of list we must append each other but that is a manual task #auto_features = ['ragnar', 'date_filter', 'cat_last_interact'] auto_features = ['ragnar' ]#, 'cat_last_interact'] """ #auto_features.append(random.choice( list(amex.feature_dict2.keys()) )) auto_features = random.choices( list(amex.feature_dict.keys())) clean_features = [] for f in auto_features: clean_features += amex.feature_dict[f] #clean_features += amex.filter_feature[auto_features[0]] #clean_features += amex.feature_dict2[auto_features[1]] # auto_features2 = ['filter4', 'filter5', 'filter6'] # for f in auto_features2: # clean_features += amex.filter_feature[f] # #clean_features += amex.feature_dict2[f] # auto_features += auto_features2 auto_features = list(set(auto_features)) clean_features = list(set(clean_features)) #------------------------------------------------- auto_features2 = random.choices( list(amex.filtered_features.keys())) clean_features2 = [] for f in auto_features2: clean_features2 += amex.filtered_features[f] #clean_features += amex.filter_feature[auto_features[0]] #clean_features += amex.feature_dict2[auto_features[1]] # auto_features2 = ['filter4', 'filter5', 'filter6'] # for f in auto_features2: # clean_features += amex.filter_feature[f] # #clean_features += amex.feature_dict2[f] # auto_features += auto_features2 auto_features2 = list(set(auto_features2)) clean_features2 = list(set(clean_features2)) auto_features = list(set(auto_features + auto_features2)) clean_features = list(set(clean_features + clean_features2)) # if len(clean_features) > 2000: # # cut it # fix_random(random_no) # clean_features = random.choices(clean_features, k=2000) # auto_features.append('trim') # print(len(clean_features),"no of features") # ------------------------------------------------------ print("seed:", random_no) print() # ============================================================= # prep_list # # ===================================-=========================== #auto_prep = make_selection_prep(amex.prep_list, shuffle=True, random_state=random_no) auto_prep = ["Mi", "Ro", "Sd", "Lg"] fix_random(random_no) auto_prep = random.choice(auto_prep) auto_prep= random.choice([[auto_prep], [], []]) # move prep towards empty since gives better performance #auto_prep = [] print("Preprocessing:") print(auto_prep) print() # ============================================================= # optimize_on # # ============================================================== # The first thing we do is set random state to make it reproducible: fix_random(random_no) fold_name = random.choice([i for i in ['fold3', 'fold5']]) #amex.fold_list]) # fold20 too much folds # fold5: then train on 80% data, fold3: then train on 66% data fold_name = "fold5" #fold_name = random.choice(["fold10", fold_name]) # move towards "fold10" #fold_name = random.choice(["fold3", "fold5"]) #random.choice([i for i in amex.fold_list]) #fold_name = "fold10" # fold_name : "fold3" fold_nos = [i for i in range(int(fold_name.split("d")[1]))] # fold_nos = [0,1,2] # for now let's pick only one [1] fix_random(random_no) optimize_on = [random.choice(fold_nos)] # [0,2] # optimize_on = make_selection(fold_nos, shuffle=False, random_state=random_no) # while optimize_on == [] or len(optimize_on) == len(fold_nos): # don't optimize on all folds or empty # optimize_on = make_selection(fold_nos, shuffle=False, random_state=random_no) print("optimizing on fold name:") print(fold_name) print() print("optimizing on fold:") print(optimize_on) print() # # The first thing we do is set random state to make it reproducible: # fix_random(random_no) # optimize_on = random.choice([i for i in range(locker["no_folds"])]) # print("optimizing on fold:") # print(optimize_on) # print() # ============================================================== print(auto_features, auto_prep, fold_name, optimize_on,"Is it duplicate") return clean_features,auto_features, auto_prep, fold_name, optimize_on # ================================================ def custom_select(exp_no, model_name): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() # ================================================ # Selecting Features # ================================================ #amex = amex_settings() if comp_name == "amex": amex = amex_settings() elif comp_name == "amex2": amex = amex2_settings() elif comp_name == "amex3": amex = amex3_settings() else: raise Exception(f"comp name {comp_name}not valid") # ============================================================= # Features # # ============================================================== # no need to seed try: #input("Work on this part model name not defined") #model_name = input("Model Name: ") #print(f"../configs/configs-{comp_name}/auto_exp_tables/auto_exp_table_{model_name}.csv") auto_exp_table = pd.read_csv(f"../configs/configs-{comp_name}/auto_exp_tables/auto_exp_table_{model_name}.csv") assert auto_exp_table.model_name.values[0] == model_name except: raise Exception("There must be an auto_exp_table else we can't custom train!") # model_name feature_names prep_list fold_name optimize_on with_gpu #.feature_names[exp_no] # works when index and exp no same so will work only for Table since it captures all exp feature_names = ast.literal_eval(auto_exp_table[auto_exp_table.exp_no == exp_no].feature_names.values[0]) prep_list = ast.literal_eval(auto_exp_table[auto_exp_table.exp_no == exp_no].prep_list.values[0]) fold_name = auto_exp_table[auto_exp_table.exp_no == exp_no].fold_name.values[0] optimize_on = ast.literal_eval(auto_exp_table[auto_exp_table.exp_no == exp_no].optimize_on.values[0]) print("Retrieved:=>") print(feature_names, prep_list, fold_name, optimize_on) #====================================================================== #amex = amex_settings() clean_features = [] for f in feature_names: clean_features += amex.feature_dict[f] auto_prep = prep_list return clean_features, feature_names, auto_prep, fold_name, optimize_on def RUN_EXP(exp_no="--|--"): global no_exp, repeat, counter with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() # ================================================ # Selecting Features # ================================================ locker = load_pickle(f"../configs/configs-{comp_name}/locker.pkl") current_dict = load_pickle( f"../configs/configs-{locker['comp_name']}/current_dict.pkl" ) e_no = current_dict["current_exp_no"] print("Current Exp No :", e_no) print() # ================================================ # Running # ================================================ #model_name = "lgb" #"xgb" model_name = random.choice(["xgbr", "cbr", "gbmr", "rfr"]) model_name = "k1" comp_type = "regression" metrics_name = "getaroom_metrics" n_trials = 5 # we change this when there is great change in parameter set like , optimized on different parameter range with_gpu = True aug_type = "aug2" _dataset = "DigitRecognizerDataset" use_cutmix = False if exp_no != "--|--": note = f"repeat_exp_{exp_no}_with_{n_trials}_trials" else: note = f"Leap_" # ================================================ # ================================================ # ================================================ # Get Settings # ================================================ if exp_no != "--|--": # it's custom clean_features, auto_features, auto_prep, fold_name, optimize_on = custom_select(exp_no, model_name) print("Old Exp No :", exp_no) else: clean_features, auto_features, auto_prep, fold_name, optimize_on = auto_select() # e = Agent( # useful_features=clean_features, # model_name=model_name, # comp_type=comp_type, # metrics_name=metrics_name, # n_trials=n_trials, # prep_list=auto_prep, # fold_name = fold_name, # optimize_on=optimize_on, # with_gpu=with_gpu, # aug_type=aug_type, # _dataset=_dataset, # use_cutmix=use_cutmix, # note=note, # ) # Sanity check have we done the same experiment before: # What things to check: # 1> clean_features [order don't matter] # 2> model_name # 3> prep_list [order matters] # 4> optimize_on # each set must have a name in features for better visualisation # Table already created: Table = pd.DataFrame(columns=['exp_no','model_name','feature_names','prep_list',"optimize_on"]) try: auto_exp_table = pd.read_csv( f"../configs/configs-{comp_name}/auto_exp_tables/auto_exp_table_{model_name}.csv" ) except: print("Creating Table:") auto_exp_table = pd.DataFrame( columns=[ "exp_no", "n_trials", "model_name", "feature_names", "prep_list", "fold_name", "optimize_on", "with_gpu", ] ) auto_exp_table["exp_no"] = auto_exp_table["exp_no"].astype(int) #auto_exp_table["optimize_on"] = auto_exp_table["optimize_on"].astype(int) if not sanity_check( auto_exp_table, n_trials, model_name, [auto_features, 0], [auto_prep, 1], fold_name, optimize_on, with_gpu, ): # true means auto_exp_table.loc[auto_exp_table.shape[0], :] = [ int(e_no), n_trials, model_name, auto_features, auto_prep, fold_name, optimize_on, with_gpu, ] auto_exp_table["exp_no"] = auto_exp_table["exp_no"].astype(int) auto_exp_table["n_trials"] = auto_exp_table["n_trials"].astype(int) #auto_exp_table["optimize_on"] = auto_exp_table["optimize_on"].astype(int) print() print(auto_exp_table) e = Agent( useful_features=clean_features, model_name=model_name, comp_type=comp_type, metrics_name=metrics_name, n_trials=n_trials, prep_list=auto_prep, fold_name = fold_name, optimize_on=optimize_on, with_gpu=with_gpu, aug_type=aug_type, _dataset=_dataset, use_cutmix=use_cutmix, note=note, ) print("=" * 40) print("Useful_features:", clean_features) # critical part e.run() counter = 0 # resetting auto_exp_table.to_csv( f"../configs/configs-{comp_name}/auto_exp_tables/auto_exp_table_{model_name}.csv", index=False ) del e gc.collect() # Make prediction also os.system(f"python predict.py") return True else: print("Duplicate SET found!!!") if counter > 100: raise Exception("Too many times duplicte found ") no_exp += 1 #del e # already delted gc.collect() return False def Timer(rem_time, expected_time, exp_type, repeat): """ rem_time = 12 * 60 * 60 # Run it for 3 hours expected_time = 1 * 60 * 60 exp_type = "custom" # "auto" repeat = [261, 215] """ current = 0 no_exp = len(repeat) time_list = [] while (exp_type == "auto" and rem_time >= expected_time) or (exp_type == "custom" and current < no_exp): print("%" * 40) start_time = time.time() # ============================================ if exp_type == "custom": val = repeat[current] else: val = "--|--" #============================================= if RUN_EXP(val): # ============================================ end_time = time.time() rem_time -= end_time - start_time time_list.append(end_time - start_time) expected_time = np.max(time_list) # taking upper bound print(f"Experiment Done in {end_time- start_time} seconds") print("Rem Time:", rem_time) print("Expected Time:", expected_time) for i in range(5): print() else: print("=" * 40) print("Sanity Check Failed!, Trying again") print("=" * 40) current += 1 gc.collect() # break # #================================================ # # Saving # #================================================ # # # version data # from datetime import datetime # version_name = datetime.now().strftime("%Y%m%d-%H%M%S") # version_name += "_fresh" # print(f"Versioning at {version_name}") # # # configs # os.system(f"kaggle datasets version -m {version_name} -p /kaggle/configs/configs-{comp_name}/") # # # models # # #!kaggle datasets version -m {version_name} -p /kaggle/models/models-{comp_name}/ -r zip -q # # # src # # #!kaggle datasets version -m {version_name} -p /kaggle/src-{framework_name}/ no_exp = 0 passed = 0 counter = 0 if __name__ == "__main__": rem_time = 10 * 60 * 60 # Run it for 3 hours expected_time = 0.0001 * 60 * 60 # expected one hour to finish one exp_type ="auto" # "auto" # "auto" "custom" repeat = [284, 277, 278, 279, 282] Timer(rem_time, expected_time, exp_type, repeat) # lgbmc f_base, f_max, f_min, f_avg, f_last [] fold20 optimize_on 15 with_gpu = True

File no 3: /src-framework3/auto_feat.py
from optuna_search import OptunaOptimizer from feature_generator import features from feature_picker import Picker import os import sys import gc import pickle import pandas as pd import tracemalloc # from custom_models import UModel # from custom_models import * from utils import * from settings import * from experiment import Agent from show_importance import Importance if __name__ == "__main__": while True: """ # Feature selection process Starts """ with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() Table = load_pickle(f"../configs/configs-{comp_name}/Table.pkl") last_exp_no = -1 if last_exp_no == -1: row_e = Table[Table.exp_no == list(Table.exp_no.values)[-1]] last_exp_no = row_e.exp_no.values[0] else: row_e = Table[Table.exp_no == last_exp_no] # we get exp no of the latest experiment auto_feat_dict = load_json(f"../configs/configs-{comp_name}/auto_feat.json") print(last_exp_no) useful_features = auto_feat_dict[str(last_exp_no)] #useful_features += amzcomp1_settings().feature_dict["ver2"] useful_features = list(set(useful_features)) """ # Feature selection process Ends """ # ========================================================== model_name = "xgbr" # -----s---> """ [ "lgr", "lir", "xgb", "xgbc", "xgbr", "cbc", "cbr", "mlpc", "rg", "ls", "knnc", "dtc", "adbc", "gbmc" , "gbmr, "hgbc", "lgb", "lgbmc", "lgbmr", "rfc" , "rfr", # --------------->["k1", "k2", "k3", "tez1", "tez2", "p1" ,"pretrained"] """ comp_type = ( "2class" # -------->["regression", "2class","multi_class", "multi_label"] ) metrics_name = "amzcomp1_metrics" # --------->["getaroom_metrics", "amex_metric","amex_metric_mod", "accuracy","f1","recall","precision", "auc", "logloss","auc_tf","mae","mse","rmse","msle","rmsle","r2"] n_trials = 20 #30 # ------------> no of times to run optuna prep_list = [ "Sd", ] # ------> ["SiMe", "SiMd",~ "SiMo", "Mi", "Ro", "Sd", "Lg"] <= _prep_list prep_list = [] fold_name = "fold5" # ['fold3', 'fold5', 'fold10', 'fold20'] optimize_on = [random.choice(range(5))] #[4] # fold on which optimize # 0,1,2,4 with_gpu = True aug_type = "aug2" # "aug1", "aug2", "aug3", "aug4" _dataset = "DigitRecognizerDataset" # "BengaliDataset", "ImageDataset", "DigitRecognizerDataset", "DigitRecognizerDatasetTez2" use_cutmix = False # CALLBACKS # lgbmClassifiers callback: # https://lightgbm.readthedocs.io/en/latest/Python-API.html#callbacks """ ############################ change learning rate custom schedulers :=>cosine_decay , exponential_decay, simple_decay built in scheduler :=>"ReduceLROnPlateau" # EarlyStopping, ############################# save model :=>chk_pt : whether to use checkpoint or not ############################### stop training :=>terminate_on_NaN >Callbacks monitor a particular varaibable and stops exectuion when it crosses fixed value , >early_top also monitors a particular value but it stops when it stops improving it has some patience builtin callback :=>early_stopping custom callback :=>myCallback1 """ # don't use early_stopping with cyclic decay lr because model gets good and bad periodically and it doesn't mean we should terminate. # "swa", "cosine_decay", "exponential_decay", "simple_decay", "ReduceLROnPlateau", "chk_pt", "terminate_on_NaN", "early_stopping", "myCallback1" callbacks_list = ["terminate_on_NaN"] # ["exponential_decay", "terminate_on_NaN"] # [swa,early_stop] # ----------------------------------------------------------- note = "ragnar" e = Agent( useful_features=useful_features, model_name=model_name, comp_type=comp_type, metrics_name=metrics_name, n_trials=n_trials, prep_list=prep_list, fold_name = fold_name, optimize_on=optimize_on, with_gpu=with_gpu, aug_type=aug_type, _dataset=_dataset, use_cutmix=use_cutmix, callbacks_list=callbacks_list, note=note, ) print("=" * 40) print("Useful_features:", useful_features) e.run() del e # ------------------------------------------------------------- # exp_list = ["1"] # ----------------> [1,2,3,4] # e.show(exp_list) """ {'learning_rate': 0.010821262164314453, 'max_depth': 16, 'min_child_weight': 5, 'subsample': 0.4521783648128741, 'n_estimators': 500, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist', 'gpu_id': 0, 'predictor': 'gpu_predictor'} """ # Make prediction also os.system(f"python predict.py") # Let's save auto features direction = "minimize" technique = "bagging" # "weighted_mean" , "best" , "mean", "top50", "bagging" last_exp_no += 1 f = Importance(exp_no=last_exp_no) # helps when doing weighted mean base_features = None type_importance = "fold" #"fold", "opt" pick = None # pick top 2 trials out of 5 top = None threshold = 1 #f.show(technique= technique, top=top, threshold=threshold, direction=direction, pick = pick, type_importance= type_importance, base_features=base_features) useful_features = f.give(technique= technique, top=top, threshold=threshold, direction=direction, pick = pick, type_importance= type_importance, base_features=base_features) auto_feat_dict[str(last_exp_no)] = useful_features save_json(f"../configs/configs-{comp_name}/auto_feat.json", auto_feat_dict) #raise Exception('stop')

File no 4: /src-framework3/create_folds.py
import pandas as pd from sklearn import model_selection from iterstrat.ml_stratifiers import MultilabelStratifiedKFold import os import sys import pickle from collections import defaultdict from utils import * """ import os import sys import pickle with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() with open(f"../configs/configs-{comp_name}/locker.pkl", "rb") as f: a = pickle.load(f) """ if __name__ == "__main__": with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() with open(f"../configs/configs-{comp_name}/locker.pkl", "rb") as f: a = pickle.load(f) # # test = pd.read_csv(f"../input/input-{comp_name}/test.csv") # test = pd.read_parquet(f"../input/input-{comp_name}/test.parquet") # # test.to_csv(f"../configs/configs-{comp_name}/test.csv",index=False) # test.to_parquet(f"../configs/configs-{comp_name}/test.parquet", index=False) # df = pd.read_csv(f"../input/input-{comp_name}/train.csv") # df = df.sample(frac=1).reset_index(drop=True) # use later as it shuffles index # do it when you don't have id column or do it before creating Id columns # because for id col 0,1,2,3 it makes it look bad 12,3,0,111,... # for string id it is ok if a["comp_type"] == "multi_label": # df = df.sample(fracc=1).reset_index(drop=True) raise Exception("Now we have fold_dict instead of no_fold so change multi_label according in create folds") mskf = MultilabelStratifiedKFold( n_splits=a["no_folds"], shuffle=True, random_state=23 ) for fold, (train_idx, val_idx) in enumerate( mskf.split(df[a["id_name"]].values, df[a["target_name"]].values) ): print(len(train_idx), len(val_idx)) df.loc[val_idx, "fold"] = fold elif a["comp_type"] in ["binary", "multiclass", "2class"]: df = pd.read_parquet(f"../input/input-{comp_name}/train.parquet") if a["id_name"] in df.columns: # always create new ID column for train # keep it simple df.drop(a["id_name"], axis=1, inplace=True) # reset for all fix_random(231) # to make result reproducible df = df.sample(frac=1).reset_index(drop=True) # use later as it changes index df.index.name = a["id_name"] df = df.sort_index().reset_index() # sorting is very necessary for f,v in a['fold_dict'].items(): # 'fold5', 5 print(f,v,"=============>") df[f] = 1 kf = model_selection.StratifiedKFold( n_splits=v, shuffle=True, random_state=23 ) target_name = a["target_name"] for fold, (train_idx, val_idx) in enumerate( kf.split(X=df, y=df[target_name].values) ): print(fold+1,":",len(train_idx), len(val_idx)) df.loc[val_idx, f] = fold df[f] = df[f].astype('int8') # bydefault it is int64 print() else: df = pd.read_parquet(f"../input/input-{comp_name}/train.parquet") if a["id_name"] in df.columns: # always create new ID column for train # keep it simple df.drop(a["id_name"], axis=1, inplace=True) # reset for all fix_random(231) # to make result reproducible df = df.sample(frac=1).reset_index(drop=True) # use later as it changes index df.index.name = a["id_name"] df = df.sort_index().reset_index() # sorting is very necessary for f,v in a['fold_dict'].items(): # 'fold5', 5 print(f,v,"=============>") df[f] = 1 kf = model_selection.KFold( n_splits=v, shuffle=True, random_state=23 ) target_name = a["target_name"] for fold, (train_idx, val_idx) in enumerate( kf.split(X=df, y=df[target_name].values) ): print(fold+1,":",len(train_idx), len(val_idx)) df.loc[val_idx, f] = fold df[f] = df[f].astype('int8') # bydefault it is int64 print() if a["data_type"] in ["image_path", "image_df"]: # df.to_csv(f"../configs/configs-{comp_name}/my_folds.csv", index=False) df.to_parquet(f"../configs/configs-{comp_name}/my_folds.parquet", index=False) useful_features = [a["id_name"]] with open( f"../configs/configs-{a['comp_name']}/useful_features_l_1.pkl", "wb" ) as f: pickle.dump(useful_features, f) elif a["data_type"] == "tabular": # df.to_csv(f"../configs/configs-{comp_name}/my_folds.csv", index=False) df.to_parquet(f"../input/input-{comp_name}/my_folds.parquet", index=False) # test = pd.read_csv(f"../input/input-{comp_name}/test.csv") # Now no need to save test in cnfigs as we are saving it in input only [heavy files in input folder] #test = pd.read_parquet(f"../input/input-{comp_name}/test.parquet") ## test.to_csv(f"../configs/configs-{comp_name}/test.csv", index=False) #test.to_parquet(f"../configs/configs-{comp_name}/test.parquet", index=False) test = pd.read_parquet(f"../input/input-{comp_name}/test.parquet") useful_features = test.drop(a["id_name"], axis=1).columns.tolist() with open( f"../configs/configs-{a['comp_name']}/useful_features_l_1.pkl", "wb" ) as f: pickle.dump(useful_features, f) # --------------------------------dump current current_dict = defaultdict() current_dict["current_level"] = 1 current_dict["current_feature_no"] = 0 current_dict["current_exp_no"] = 0 current_dict["current_ens_no"] = 0 with open(f"../configs/configs-{a['comp_name']}/current_dict.pkl", "wb") as f: pickle.dump(current_dict, f) # --------------------------------dump features_dict feat_dict = defaultdict() feat_dict["base"] = [useful_features, 0] #feat_dict["l_1_f_0"] = [useful_features, 0, "base"] with open(f"../configs/configs-{a['comp_name']}/features_dict.pkl", "wb") as f: pickle.dump(feat_dict, f) # ---------------------------------dump Table Table = pd.DataFrame( columns=[ "exp_no", "model_name", "bv", "bp", "random_state", "with_gpu", "aug_type", "_dataset", "use_cutmix", "callbacks_list", "features_list", "level_no", "oof_fold_name", "opt_fold_name", "fold_no", "no_iterations", "prep_list", "metrics_name", "seed_mean", "seed_std", # ---\ "fold_mean", "fold_std", "pblb_single_seed", "pblb_all_seed", "pblb_all_fold", "notes", ] ) with open(f"../configs/configs-{a['comp_name']}/Table.pkl", "wb") as f: pickle.dump(Table, f) # -------------------------------------------

File no 5: /src-framework3/create_folds_again.py
import pandas as pd from sklearn import model_selection from iterstrat.ml_stratifiers import MultilabelStratifiedKFold import os import sys import pickle from collections import defaultdict from utils import * # Here we just creates folds and we don't recreate tables """ import os import sys import pickle with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() with open(f"../configs/configs-{comp_name}/locker.pkl", "rb") as f: a = pickle.load(f) """ if __name__ == "__main__": with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() with open(f"../configs/configs-{comp_name}/locker.pkl", "rb") as f: a = pickle.load(f) locker = a source= 'dummy' title = 'ver2' target_name = locker['target_name'] #"prediction" id_name = locker['id_name'] # "customer_ID" fold_list = ["fold3", "fold5", "fold10", "fold20"] # # test = pd.read_csv(f"../input/input-{comp_name}/test.csv") # test = pd.read_parquet(f"../input/input-{comp_name}/test.parquet") # # test.to_csv(f"../configs/configs-{comp_name}/test.csv",index=False) # test.to_parquet(f"../configs/configs-{comp_name}/test.parquet", index=False) # df = pd.read_csv(f"../input/input-{comp_name}/train.csv") # df = df.sample(frac=1).reset_index(drop=True) # use later as it shuffles index # do it when you don't have id column or do it before creating Id columns # because for id col 0,1,2,3 it makes it look bad 12,3,0,111,... # for string id it is ok if a["comp_type"] == "multi_label": # df = df.sample(fracc=1).reset_index(drop=True) raise Exception("Now we have fold_dict instead of no_fold so change multi_label according in create folds") mskf = MultilabelStratifiedKFold( n_splits=a["no_folds"], shuffle=True, random_state=23 ) for fold, (train_idx, val_idx) in enumerate( mskf.split(df[a["id_name"]].values, df[a["target_name"]].values) ): print(len(train_idx), len(val_idx)) df.loc[val_idx, "fold"] = fold else: try: df = pd.read_parquet(f"../input/input-{source}/train.parquet") except: try: df = pd.read_csv(f"../input/input-{source}/train.csv") except: raise Exception("train is neither parquet nor csv") if a["id_name"] in df.columns: # always create new ID column for train # keep it simple df.drop(a["id_name"], axis=1, inplace=True) # reset for all fix_random(231) # to make result reproducible df = df.sample(frac=1).reset_index(drop=True) # use later as it changes index df.index.name = a["id_name"] df = df.sort_index().reset_index() # sorting is very necessary for f,v in a['fold_dict'].items(): # 'fold5', 5 print(f,v,"=============>") df[f] = 1 kf = model_selection.StratifiedKFold( n_splits=v, shuffle=True, random_state=23 ) target_name = a["target_name"] for fold, (train_idx, val_idx) in enumerate( kf.split(X=df, y=df[target_name].values) ): print(fold+1,":",len(train_idx), len(val_idx)) df.loc[val_idx, f] = fold df[f] = df[f].astype('int8') # bydefault it is int64 print() if a["data_type"] in ["image_path", "image_df"]: raise Exception() # df.to_csv(f"../configs/configs-{comp_name}/my_folds.csv", index=False) df.to_parquet(f"../configs/configs-{comp_name}/my_folds.parquet", index=False) useful_features = [a["id_name"]] with open( f"../configs/configs-{a['comp_name']}/useful_features_l_1.pkl", "wb" ) as f: pickle.dump(useful_features, f) elif a["data_type"] == "tabular": # df.to_csv(f"../configs/configs-{comp_name}/my_folds.csv", index=False) #df.to_parquet(f"../input/input-{comp_name}/my_folds.parquet", index=False) # test = pd.read_csv(f"../input/input-{comp_name}/test.csv") # Now no need to save test in cnfigs as we are saving it in input only [heavy files in input folder] #test = pd.read_parquet(f"../input/input-{comp_name}/test.parquet") ## test.to_csv(f"../configs/configs-{comp_name}/test.csv", index=False) #test.to_parquet(f"../configs/configs-{comp_name}/test.parquet", index=False) try: test = pd.read_parquet(f"../input/input-{source}/test.parquet") except: try: # csv format data is present test = pd.read_csv(f"../input/input-{source}/test.csv") #test.to_parquet(f"../input/input-{comp_name}/test.parquet", index=False) # sample = pd.read_csv(f"../input/input-{comp_name}/sample.csv") # sample.to_parquet(f"../input/input-{comp_name}/sample.parquet", index=False) except: raise Exception("test is neither parquet nor csv") all_columns = list(test.drop(id_name, axis=1).columns) useful_features_l_1 = load_pickle(f"../configs/configs-{a['comp_name']}/useful_features_l_1.pkl") # any true if any(x in useful_features_l_1 for x in all_columns): #all_columns in useful_features_l_1): # some already present pass #raise Exception("Some features are already present in useful_features_l_1") print("These features will be added:") print(all_columns) v = input("Do you want to proceed? [y/n] : ") if v.lower() == 'n': raise Exception("Process Terminated") useful_features_l_1 += all_columns useful_features_l_1 = list(set(useful_features_l_1)) with open( f"../configs/configs-{a['comp_name']}/useful_features_l_1.pkl", "wb" ) as f: pickle.dump(useful_features_l_1, f) # train = df[all_columns].copy() test = test[all_columns].copy() train.to_parquet(f"../input/input-{comp_name}/train_{title}.parquet") test.to_parquet(f"../input/input-{comp_name}/test_{title}.parquet") input_dict = load_pickle(f"../input/input-{comp_name}/input_dict.pkl") input_dict[title] = all_columns save_pickle(f"../input/input-{comp_name}/input_dict.pkl", input_dict) #----------- print("New data Sucessfully added")

File no 6: /src-framework3/custom_classes.py
# --> to pass it to pytorch dataloader # > __init__ # > __len__ # > __getitem__ # each competition may require different # preprocessing and scaling from torch.utils.data import Dataset import numpy as np import pandas as pd import torch import os import sys import pickle import joblib import imageio import cv2 import os.path import albumentations from PIL import Image class TabularDataset: def __init__(self, data, target): self.data = data self.targets = targets def __len__(self): return self.data.shape[0] def __getitem__(self, idx): sample = self.data[idx, :] target = self.targets[idx] return { "x": torch.tensor(sample, dtype=float), "y": torch.tensor(target, dtype=long), } # classification / regression class TextDataset: def __init__(self, data, targets, tokenizer): self.data = data # list of texts self.targets = targets self.tokenizer = tokenizer def __len__(self): return len(data) def __getitem__(self, idx): # main part text = self.data[idx] # len(self.target.shape) will catch (30,) if len(self.target.shape) == 2 and self.target.shape[1] > 1: target = self.targets[idx, :] else: target = self.targets[idx] # binary: 0, 1, 1, 0 # multiclass: 1, 2, 0, 1 # regr(single col/ multicol): 0.3, 4, 5 # multilabel classification: [1, 0, 0, 1, 0], [1, 1, 0, 0, 0] # entity extraction # input_ids: text=> tokens i.e numbers input_ids = tokenizer(text) # transformers # input_ids : set of numbers [101, 42, 27, 216] # these seq can be of different length so do padding return { "text": torch.tensor(input_ids, dtype=torch.long), "target": torch.tensor( target ), # dtype= classification: torch.long, reg: torch.float) } class BengaliDataset(Dataset): def __init__(self, image_paths, targets, img_height, img_width, transform): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() with open(f"../configs/configs-{comp_name}/locker.pkl", "rb") as f: self.locker = pickle.load(f) # self.csv = csv.reset_index() self.paths = image_paths self.targets = targets self.img_ids = self.paths # csv[self.locker["id_name"]] self.img_height = img_height self.img_width = img_width self.transform = transform def __len__(self): # return len(self.csv) return len(self.paths) def __getitem__(self, idx): path = self.paths[idx] path = f"{path}.png" # print("Found", path) # img = Image.open(path) img = Image.open(r"../input/input-bengaliai/train_images/Train_10.png") img = np.array(img) # print("Found-->") # img = joblib.load( # f"../input/input-{self.locker['comp_name']}/train_images/{img_id}.pkl" # ) # reshape 137, 236 # new dataset 128,128 img = img.reshape(self.img_height, self.img_width).astype(np.uint8) img = 255 - img # # make it 3dimensional (X,Y, RGB) if not img = img[:, :, np.newaxis] # np.repeat(item, no_times, along axis) # it repeats item along an axis # duplicates whole image 3 times to create RGB channels # img = img[img, 3, 2] img = np.repeat(img, 3, 2) # or # most of the models take RGB image convert PIL grayscale to "RGB" # img = Image.fromarray(image).convert("RGB") if self.transform is not None: img = self.transform(image=img)["image"] img = torch.tensor(img, dtype=torch.float) img = torch.permute(img, (2, 1, 0)) target_list = self.locker["target_name"] # target_1 = self.csv.iloc[index][target_list[0]].values # target_2 = self.csv.iloc[index][target_list[1]].values # target_3 = self.csv.iloc[index][target_list[2]].values target_1 = self.targets[idx, 0] # .iloc[index][target_list[0]].values target_2 = self.targets[idx, 1] # .iloc[index][target_list[1]].values target_3 = self.targets[idx, 2] # .iloc[index][target_list[2]].values return { "image": torch.tensor(img, dtype=torch.float), "grapheme_root": torch.tensor(target_1, dtype=torch.long), "vowel_diacritic": torch.tensor(target_2, dtype=torch.long), "consonant_diacritic": torch.tensor(target_3, dtype=torch.long), } # return img, np.array([target_1, target_2, target_3]) # tez2 1 channel pretrained 3channel models class DigitRecognizerDataset: def __init__(self, df, augmentations, model_name): # temp adding model name with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() with open(f"../configs/configs-{comp_name}/locker.pkl", "rb") as f: self.locker = pickle.load(f) self.df = df self.targets = self.df[self.locker["target_name"]].values self.df = self.df.drop(columns=[self.locker["target_name"]]) self.augmentations = augmentations if self.locker["comp_name"] == "twistmnist": self.images = self.df.to_numpy(dtype=np.float32).reshape((-1, 28, 50)) self.images = self.images[:, :, -28:] # remove the 1's else: self.images = self.df.to_numpy(dtype=np.float32).reshape((-1, 28, 28)) self.model_name = model_name def __len__(self): return len(self.df) def __getitem__(self, item): # item: index_no target = self.targets[item] image = self.images[item] if self.model_name == "pretrained": image = 255 - image image = image[:, :, np.newaxis] image = np.repeat(image, 3, 2) image = torch.tensor(image, dtype=torch.float) image = torch.permute(image, (2, 1, 0)) # or # image = np.transpose(iamge, (2,0,1)).astype(np.float32) # pytorch expects batch size * no of channels * height * weidth\ else: image = np.expand_dims(image, axis=0) # experimenting this is just for p1 which takes 1D input # image = image.reshape((-1)) # target = target.reshape((-1)) return { "image": torch.tensor(image, dtype=torch.float), "targets": torch.tensor(target, dtype=torch.float), } # this is for keras cutmix class CutMixImageDataGenerator: def __init__(self, generator1, generator2, img_size, batch_size): self.batch_index = 0 self.samples = generator1.samples self.class_indices = generator1.class_indices self.generator1 = generator1 self.generator2 = generator2 self.img_size = img_size self.batch_size = batch_size def reset_index(self): # Ordering Reset (If Shuffle is True, Shuffle Again) self.generator1._set_index_array() self.generator2._set_index_array() def reset(self): self.batch_index = 0 self.generator1.reset() self.generator2.reset() self.reset_index() def get_steps_per_epoch(self): quotient, remainder = divmod(self.samples, self.batch_size) return (quotient + 1) if remainder else quotient def __len__(self): self.get_steps_per_epoch() def __next__(self): if self.batch_index == 0: self.reset() crt_idx = self.batch_index * self.batch_size if self.samples > crt_idx + self.batch_size: self.batch_index += 1 else: # If current index over number of samples self.batch_index = 0 reshape_size = self.batch_size last_step_start_idx = (self.get_steps_per_epoch() - 1) * self.batch_size if crt_idx == last_step_start_idx: reshape_size = self.samples - last_step_start_idx X_1, y_1 = self.generator1.next() X_2, y_2 = self.generator2.next() cut_ratio = np.random.beta(a=1, b=1, size=reshape_size) cut_ratio = np.clip(cut_ratio, 0.2, 0.8) label_ratio = cut_ratio.reshape(reshape_size, 1) cut_img = X_2 X = X_1 for i in range(reshape_size): cut_size = int((self.img_size - 1) * cut_ratio[i]) y1 = random.randint(0, (self.img_size - 1) - cut_size) x1 = random.randint(0, (self.img_size - 1) - cut_size) y2 = y1 + cut_size x2 = x1 + cut_size cut_arr = cut_img[i][y1:y2, x1:x2] cutmix_img = X_1[i] cutmix_img[y1:y2, x1:x2] = cut_arr X[i] = cutmix_img y = y_1 * (1 - (label_ratio**2)) + y_2 * (label_ratio**2) return X, y def __iter__(self): while True: yield next(self)

File no 7: /src-framework3/custom_models.py
# tez ---------------------------- import os import sys import pickle import albumentations as A import pandas as pd import numpy as np import tez from tez.datasets import ImageDataset from tez.callbacks import EarlyStopping import torch import torch.nn as nn from torch.nn import functional as F from sklearn import metrics, model_selection, preprocessing import timm from sklearn.model_selection import KFold # ignoring warnings import warnings warnings.simplefilter("ignore") import os, cv2, json from PIL import Image import random # ------------------------------ from torch.nn import ( Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout, ) from torch.optim import Adam, SGD # ----------------- import pretrainedmodels import torch.nn as nn from torch.nn import functional as F class trainer_p1: def __init__( self, model, train_loader, valid_loader, optimizer, scheduler, use_cutmix ): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() with open(f"../configs/configs-{comp_name}/locker.pkl", "rb") as f: self.locker = pickle.load(f) self.model = model self.train_loader = train_loader self.valid_loader = valid_loader self.optimizer = optimizer # self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( # self.optimizer, mode="max", verbose=True, patience=7, factor=0.5 # ) self.scheduler = scheduler self.locc_fn = nn.CrossEntropyLoss() self.use_cutmix = use_cutmix def loss_fn_multilabel(self, outputs, targets): o1, o2, o3 = outputs t1, t2, t3 = targets l1 = nn.CrossEntropyLoss()(o1, t1) l2 = nn.CrossEntropyLoss()(o2, t2) l3 = nn.CrossEntropyLoss()(o3, t3) return (l1 + l2 + l3) / 3 def loss_fn(self, targets, output): device = "cuda" targets = targets.type(torch.LongTensor) targets = targets.to(device) output = output.to(device) return nn.CrossEntropyLoss()(output, targets) # output is the prediction # targets is the true label output = torch.argmax(output, dim=1) output = output.unsqueeze(1) targets = targets.unsqueeze(1) # use it for conv make it 2D print(output.shape, targets.shape) # For nn.CrossEntropyLoss the target has to be a single number from the interval [0, #classes] return nn.CrossEntropyLoss()(output, targets) # return nn.BCEWithLogitsLoss()(output, targets) def scheduler_fn(self): pass def optimizer_fn(self): learning_rate = 0.001 pass def rand_bbox(self, size, lam): W = size[2] H = size[3] cut_rat = np.sqrt(1.0 - lam) cut_w = np.int(W * cut_rat) cut_h = np.int(H * cut_rat) # uniform cx = np.random.randint(W) cy = np.random.randint(H) bbx1 = np.clip(cx - cut_w // 2, 0, W) bby1 = np.clip(cy - cut_h // 2, 0, H) bbx2 = np.clip(cx + cut_w // 2, 0, W) bby2 = np.clip(cy + cut_h // 2, 0, H) return bbx1, bby1, bbx2, bby2 def cutmix_data(self, data): inputs = data["image"] targets = data["targets"] self.lam = np.random.beta(1.0, 1.0) rand_index = torch.randperm(inputs.size()[0]) target = data["targets"] self.shuffled_targets = target[rand_index] bbx1, bby1, bbx2, bby2 = self.rand_bbox(inputs.size(), self.lam) inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2] self.lam = 1 - ( (bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]) ) return inputs, targets def train_one_epoch(self): self.model.train() # put model in train mode total_loss = 0 for batch_index, data in enumerate(self.train_loader): loss = self.train_one_step(data) # loss = self.loss_fn(data["targets"], output) total_loss += loss return total_loss def train_one_step(self, data): self.optimizer.zero_grad() for k, v in data.items(): data[k] = v.to("cuda") # make sure forward function of model has same keys # dictinary is passed using ** if self.use_cutmix == True: inputs, targets = self.cutmix_data(data) output = self.model(inputs) # **data) loss = self.loss_fn(targets, output) * self.lam + self.loss_fn( self.shuffled_targets, output ) * (1 - self.lam) else: if self.locker["comp_name"] == "bengaliai": image = data["image"] grapheme_root = data["grapheme_root"] vowel_diacritic = data["vowel_diacritic"] consonant_diacritic = data["consonant_diacritic"] image = image.to("cuda", dtype=torch.float) grapheme_root = grapheme_root.to("cuda", dtype=torch.long) vowel_diacritic = vowel_diacritic.to("cuda", dtype=torch.long) consonant_diacritic = consonant_diacritic.to("cuda", dtype=torch.long) targets = (grapheme_root, vowel_diacritic, consonant_diacritic) output = self.model(data["image"]) loss = self.loss_fn_multilabel(output, targets) else: output = self.model(data["image"]) loss = self.loss_fn(data["targets"], output) loss.backward() self.optimizer.step() return loss def validate_one_epoch(self): self.model.eval() total_loss = 0 for batch_index, data in enumerate(self.valid_loader): with torch.no_grad(): loss = self.validate_one_step(data) total_loss += loss return total_loss def validate_one_step(self, data): for k, v in data.items(): data[k] = v.to("cuda") # added if self.locker["comp_name"] == "bengaliai": image = data["image"] grapheme_root = data["grapheme_root"] vowel_diacritic = data["vowel_diacritic"] consonant_diacritic = data["consonant_diacritic"] image = image.to("cuda", dtype=torch.float) grapheme_root = grapheme_root.to("cuda", dtype=torch.long) vowel_diacritic = vowel_diacritic.to("cuda", dtype=torch.long) consonant_diacritic = consonant_diacritic.to("cuda", dtype=torch.long) targets = (grapheme_root, vowel_diacritic, consonant_diacritic) output = self.model(data["image"]) loss = self.loss_fn_multilabel(output, targets) else: # upto here # make sure forward function of model has same keys output = self.model(data["image"]) # **data) loss = self.loss_fn(data["targets"], output) return loss def fit(self, n_iter): for epoch in range(n_iter): epoch_loss = 0 counter = 0 train_loss = self.train_one_epoch() valid_loss = self.validate_one_epoch() # scheduler self.scheduler.step(valid_loss) if epoch % 2 == 0: print( f"epoch {epoch}, train loss {train_loss}, valid loss {valid_loss}" ) # self.optimizer.swap_swa_sgd() ## Here def predict_one_step(self, data): for k, v in data.items(): data[k] = v.to("cuda") output = self.model(data["image"]) return output def predict(self, test_loader): if self.locker["comp_type"] == "multi_label": outputs = [[], [], []] preds = [[], [], []] with torch.no_grad(): for batch_index, data in enumerate(test_loader): out = self.predict_one_step(data) outputs[0].append( out[0] ) # out.argmax(1) required when the final layer gives probabilites of classes and we want hard class outputs[1].append( out[1] ) # out.argmax(1) required when the final layer gives probabilites of classes and we want hard class outputs[2].append( out[2] ) # out.argmax(1) required when the final layer gives probabilites of classes and we want hard class preds[0] = torch.cat( outputs[0] ) # .view(-1) view(-1) is needed when we want 1D array preds[1] = torch.cat( outputs[1] ) # .view(-1) view(-1) is needed when we want 1D array preds[2] = torch.cat( outputs[2] ) # .view(-1) view(-1) is needed when we want 1D array else: outputs = [] with torch.no_grad(): for batch_index, data in enumerate(test_loader): out = self.predict_one_step(data) outputs.append( out ) # out.argmax(1) required when the final layer gives probabilites of classes and we want hard class preds = torch.cat( outputs ) # .view(-1) view(-1) is needed when we want 1D array return [ preds ] # make output as list of arrays for one d output make it a list of one element def save(self, path): state_dict = self.model.cpu().state_dict() self.model = self.model.cuda() torch.save(state_dict, path) class ResNet34(nn.Module): def __init__(self, pretrained): super(ResNet34, self).__init__() if pretrained is True: self.model = pretrainedmodels.__dict__["resnet34"](pretrained="imagenet") else: self.model = pretrainedmodels.__dict__["resnet34"](pretrained=None) self.l0 = nn.Linear(512, 168) self.l1 = nn.Linear(512, 11) self.l2 = nn.Linear(512, 7) def forward(self, x): # supports all kind of image size bs, _, _, _ = x.shape x = self.model.features(x) x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1) l0 = self.l0(x) l1 = self.l1(x) l2 = self.l2(x) return l0, l1, l2 class pretrained_models(nn.Module): # basic pytorch model # conv2d(in_channels, out_channels): # in_channels:- no of channels in the input image # out_channel:- no of channels in the output image # kernel_size:- size of convolving kernel def __init__(self, no_features): super().__init__() model_name = "resnet34" self.model = pretrainedmodels.__dict__[model_name](pretrained="imagenet") # adding a head in_features = self.model.last_linear.in_features self.model.last_linear = torch.nn.Linear(in_features, 10) # self.layer0 = nn.Conv2d(in_channels = 3, out_channels = 50, kernel_size=3, padding=1) # self.layer1 = nn.Linear(50, 32) # self.layer2 = nn.Linear(32, 16) # self.layer3 = nn.Linear(16, 1) # self.cnn_layers = Sequential( # # Defining a 2D convolution layer # Conv2d(1, 4, kernel_size=3, stride=1, padding=1), # BatchNorm2d(4), # ReLU(inplace=True), # MaxPool2d(kernel_size=2, stride=2), # # Defining another 2D convolution layer # Conv2d(4, 4, kernel_size=3, stride=1, padding=1), # BatchNorm2d(4), # ReLU(inplace=True), # MaxPool2d(kernel_size=2, stride=2), # ) # self.linear_layers = Sequential(Linear(4 * 7 * 7, 10)) def forward(self, data): # batch_size, no_featrues : xtrain.shape # use this if now using 1D array in starting # xtrain = data # x = self.layer1(xtrain) # x = self.layer2(x) # x = self.layer3(x) # return x x = data return self.model(x) # x = self.cnn_layers(x) # x = x.view(x.size(0), -1) # x = self.linear_layers(x) # return x class p1_model1(nn.Module): # basic pytorch model # conv2d(in_channels, out_channels): # in_channels:- no of channels in the input image # out_channel:- no of channels in the output image # kernel_size:- size of convolving kernel def __init__(self): super().__init__() # self.layer0 = nn.Conv2d(in_channels = 3, out_channels = 50, kernel_size=3, padding=1) # self.layer1 = nn.Linear(50, 32) # self.layer2 = nn.Linear(32, 16) # self.layer3 = nn.Linear(16, 1) self.cnn_layers = Sequential( # Defining a 2D convolution layer Conv2d(3, 4, kernel_size=3, stride=1, padding=1), BatchNorm2d(4), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2), # Defining another 2D convolution layer Conv2d(4, 4, kernel_size=3, stride=1, padding=1), BatchNorm2d(4), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2), ) self.linear_layers1 = nn.Linear(100, 168) # Sequential(Linear(4 * 7 * 7, 168)) self.linear_layers2 = nn.Linear(100, 11) # Sequential(Linear(4 * 7 * 7, 11)) self.linear_layers3 = nn.Linear(100, 7) # Sequential(Linear(4 * 7 * 7, 7)) def forward(self, data): # batch_size, no_featrues : xtrain.shape # use this if now using 1D array in starting # xtrain = data # x = self.layer1(xtrain) # x = self.layer2(x) # x = self.layer3(x) # return x x = data x = self.cnn_layers(x) x = x.view(x.size(0), -1) x1 = self.linear_layers1(x) x2 = self.linear_layers2(x) x3 = self.linear_layers3(x) return x1, x2, x3 class p1_model(nn.Module): # basic pytorch model # conv2d(in_channels, out_channels): # in_channels:- no of channels in the input image # out_channel:- no of channels in the output image # kernel_size:- size of convolving kernel def __init__(self): super().__init__() self.cnn_layers = Sequential( # Defining a 2D convolution layer Conv2d(1, 4, kernel_size=3, stride=1, padding=1), BatchNorm2d(4), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2), # Defining another 2D convolution layer Conv2d(4, 4, kernel_size=3, stride=1, padding=1), BatchNorm2d(4), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2), ) self.linear_layers = Sequential(Linear(4 * 7 * 7, 10)) def forward(self, data): x = data x = self.cnn_layers(x) x = x.view(x.size(0), -1) x = self.linear_layers(x) return x # tez1 class UModel(tez.Model): # nn.Module): #tez.Model): def __init__( self, model_name, num_classes, learning_rate, n_train_steps # , warmup_ratio ): super().__init__() self.learning_rate = learning_rate self.n_train_steps = n_train_steps # self.warmup_ratio = warmup_ratio self.model = timm.create_model( model_name, pretrained=True, in_chans=3, num_classes=num_classes ) self.step_scheduler_after = "batch" def monitor_metrics(self, outputs, targets): if targets is None: return {} outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy() targets = targets.cpu().detach().numpy() accuracy = metrics.accuracy_score(targets, outputs) return {"accuracy": accuracy} def fetch_optimizer(self): opt = torch.optim.Adam(self.parameters(), lr=3e-4) return opt def fetch_scheduler(self): sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1 ) return sch def forward(self, image, targets=None): x = self.model(image) if targets is not None: loss = nn.CrossEntropyLoss()(x, targets) metrics = self.monitor_metrics(x, targets) return x, loss, metrics return x, 0, {} # tez2 class DigitRecognizerModel(nn.Module): def __init__(self, model_name, num_classes, learning_rate, n_train_steps): super().__init__() self.learning_rate = learning_rate self.n_train_steps = n_train_steps self.model = timm.create_model( model_name, pretrained=True, in_chans=1, num_classes=num_classes, ) def monitor_metrics(self, outputs, targets): device = targets.get_device() outputs = np.argmax(outputs.cpu().detach().numpy(), axis=1) targets = targets.cpu().detach().numpy() acc = metrics.accuracy_score(targets, outputs) acc = torch.tensor(acc, device=device) return {"accuracy": acc} def optimizer_scheduler(self): opt = torch.optim.SGD( self.parameters(), lr=self.learning_rate, momentum=0.9, ) sch = torch.optim.lr_scheduler.ReduceLROnPlateau( opt, factor=0.5, patience=2, verbose=True, mode="max", threshold=1e-4, ) return opt, sch def forward(self, image, targets=None): x = self.model(image) if targets is not None: targets = targets.type(torch.LongTensor) targets = targets.to("cuda") # print(targets.device, x.device, "these are devices") # very strong sanity check loss = nn.CrossEntropyLoss()(x, targets) metrics = self.monitor_metrics(x, targets) return x, loss, metrics return x, 0, {}

File no 8: /src-framework3/datasets_create.py
import os import sys import json def create_datasets(): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() # -------------------CREATE-FOLDERS----------------------------------# try: os.system(f"kaggle datasets create -p ../configs/{'configs-'+ comp_name}/") print( f"configs-{comp_name} dataset created on kaggle." ) except: print(f"configs-{comp_name} dataset already created on kaggle.") # --------------------models----------------------------------------# print("=" * 40) try: os.system(f"kaggle datasets create -p ../models/{'models-'+ comp_name}/") print(f"models-{comp_name} dataset created on kaggle.") except: print(f"models-{comp_name} dataset already created on kaggle.") # --------------------input----------------------------------------# print("=" * 40) try: os.system(f"kaggle datasets create -p ../input/{'input-'+ comp_name}/") print(f"input-{comp_name} dataset created on kaggle.") except: print(f"input-{comp_name} dataset already created on kaggle.") print("=" * 40) if __name__ == "__main__": # [ IF RUN 2nd time will throw error] # CALL IT ONLY ONCE after init_folders.py from next time just call push.py create_datasets() print("Done")

File no 9: /src-framework3/datasets_init.py
import os import sys import json def initialize_folders(): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() # -------------------CREATE-FOLDERS----------------------------------# # --------------------configs----------------------------------------# try: os.mkdir(f"../configs/configs-{comp_name}/") os.mkdir(f"../configs/configs-{comp_name}/logs/") os.mkdir(f"../configs/configs-{comp_name}/oof_preds/") os.mkdir(f"../configs/configs-{comp_name}/test_preds/") os.mkdir(f"../configs/configs-{comp_name}/test_feats/") os.mkdir(f"../configs/configs-{comp_name}/train_feats/") os.mkdir(f"../configs/configs-{comp_name}/ensemble_logs/") os.mkdir(f"../configs/configs-{comp_name}/feature_importance/") os.mkdir(f"../configs/configs-{comp_name}/auto_exp_tables/") print(f"configs-{comp_name} folder and subfolders logs/oof_preds/test_preds created.") except: print(f"configs-{comp_name} and subfolders logs/oof_preds/test_preds already exists.") # --------------------models----------------------------------------# try: os.mkdir(f"../models/models-{comp_name}/") print(f"models-{comp_name} folder created.") except: print(f"models-{comp_name} already exists.") # --------------------input----------------------------------------# try: os.mkdir(f"../input/input-{comp_name}/") print(f"input-{comp_name} folder created.") except: print(f"input-{comp_name} already exists.") # -------------------ADD-META DATA----------------------------------# # --------------------configs----------------------------------------# print("=" * 40) # --------------------meta data create os.system(f"kaggle datasets init -p ../configs/{'configs-'+ comp_name}/") os.system(f"kaggle datasets init -p ../models/{'models-'+ comp_name}/") os.system(f"kaggle datasets init -p ../input/{'input-'+ comp_name}/") # read the json file with open(f"../configs/configs-{comp_name}/dataset-metadata.json") as f: dataset_meta = json.load(f) try: dataset_meta["id"] = f"raj401/configs-{comp_name}" dataset_meta["title"] = f"configs-{comp_name}" with open( f"../configs/configs-{comp_name}/dataset-metadata.json", "w" ) as outfile: json.dump(dataset_meta, outfile) print(outfile) with open(f"../configs/configs-{comp_name}/jj.txt", "w") as x: x.write("demo") x.close() print( f"configs-{comp_name} folder meta-data added." ) except: print(f"configs-{comp_name} meta-data already exists.") # --------------------models----------------------------------------# print("=" * 40) try: dataset_meta["id"] = f"raj401/models-{comp_name}" dataset_meta["title"] = f"models-{comp_name}" with open( f"../models/models-{comp_name}/dataset-metadata.json", "w" ) as outfile: json.dump(dataset_meta, outfile) print(outfile) with open(f"../models/models-{comp_name}/jj.txt", "w") as x: x.write("demo") x.close() print(f"models-{comp_name} folder meta-data added.") except: print(f"models-{comp_name} meta-data already exists.") # --------------------input----------------------------------------# print("=" * 40) try: dataset_meta["id"] = f"raj401/input-{comp_name}" dataset_meta["title"] = f"input-{comp_name}" with open(f"../input/input-{comp_name}/dataset-metadata.json", "w") as outfile: json.dump(dataset_meta, outfile) print(outfile) with open(f"../input/input-{comp_name}/jj.txt", "w") as x: x.write("demo") x.close() print(f"input-{comp_name} folder meta-data added.") except: print(f"input-{comp_name} meta-data already exists.") print("=" * 40) if __name__ == "__main__": #[ CAN CALL IT MULTIPLE TIMES, no harm will be done in calling] # each time it will reset .json file """ It Creates 3 folders: configs-{}, models-{}, input-{} It then initializes their metadata It then modifies their meta data accordingly sets the name and rug It then puts a demo txt file jj """ initialize_folders() print("Done")

File no 10: /src-framework3/dummy_exp.py
import os ,sys import pandas as pd import numpy as np from utils import load_pickle, save_pickle class dummy: def __init__(self): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() self.comp_name = comp_name # put it here for the mkdir of lgb callback self.locker = load_pickle(f"../configs/configs-{comp_name}/locker.pkl") self.current_dict = load_pickle( f"../configs/configs-{self.locker['comp_name']}/current_dict.pkl" ) self.Table = load_pickle(f"../configs/configs-{self.locker['comp_name']}/Table.pkl") self.Table = pd.DataFrame(self.Table) # update it with the lates values self.get_exp_no() print("=" * 30) print(f"Current Exp no: {self.current_exp_no}") print("=" * 30) def get_exp_no(self): # exp_no, current_level self.current_dict = load_pickle( f"../configs/configs-{self.locker['comp_name']}/current_dict.pkl" ) self.current_exp_no = int(self.current_dict["current_exp_no"]) def get_row(self, pull_exp_no): if pull_exp_no == -1: return self.Table.loc[self.Table.shape[0]-1, :].copy() else: # since this table stores all the experiment so index no corresponds to exp_no return self.Table.loc[pull_exp_no, :].copy() def insert_row(self, changer, pull_exp_no): raw_row = self.get_row(pull_exp_no) print("Original row") print(raw_row) print() for key,value in changer.items(): if value != "--|--": # to change it raw_row[key] = value raw_row['exp_no'] = self.current_exp_no if pull_exp_no == -1: raw_row["notes"] = f"dummy_{self.Table.shape[0]-1}" else: raw_row["notes"] = f"dummy_{pull_exp_no}" print("Modified row") print(raw_row) self.Table.loc[self.Table.shape[0], :] = raw_row.values print() print(self.Table.tail(3)) m = input("Do you want to insert this row!!, Type Y/y to proceed, else type any other key.\n: ") if m.lower() == "y": self._save_models() print("Updated!!") else: print("Aborted!!") def _save_models(self): self.current_exp_no += 1 # --------------- dump experiment no self.current_dict["current_exp_no"] = self.current_exp_no save_pickle( f"../configs/configs-{self.locker['comp_name']}/current_dict.pkl", self.current_dict, ) #---------------- sanity check table self.Table.exp_no = self.Table.exp_no.astype(int) self.Table.level_no = self.Table.level_no.astype(int) self.Table.no_iterations = self.Table.no_iterations.astype(int) self.Table.random_state = self.Table.random_state.astype(int) # ---------------- dump table save_pickle(f"../configs/configs-{self.locker['comp_name']}/Table.pkl", self.Table) def show_variables(self): print() for i, (k, v) in enumerate(self.__dict__.items()): print(f"{i}. {k} :=======>", v) print() if __name__ == "__main__": d = dummy() changer= { "exp_no": "--|--", "model_name": "--|--", "bv": 100, # to keep it on top "bp": {'learning_rate': 0.010821262164314453, 'max_depth': 16, 'min_child_weight': 5, 'subsample': 0.4521783648128741, 'n_estimators': 500, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist', 'gpu_id': 0, 'predictor': 'gpu_predictor'}, "random_state": "--|--", "with_gpu": "--|--", "aug_type": "--|--", "_dataset": "--|--", "use_cutmix": "--|--", "callbacks_list": "--|--", "features_list": "--|--", "level_no": "--|--", "oof_fold_name": [], "opt_fold_name": "--|--", "no_iterations": "--|--", "prep_list": "--|--", "metrics_name": "--|--", # Below and some of the above things shold be empty when creating a new row "seed_mean": None, "seed_std": None, "fold_mean": [], "fold_std": [], "pblb_single_seed": None, "pblb_all_seed": None, "pblb_all_fold": [], "notes": "--|--", } pull_exp_no = 7 d.insert_row(changer, pull_exp_no)

File no 11: /src-framework3/ensemble.py
import numpy as np import pandas as pd import os import sys import gc from utils import * from metrics import * from collections import defaultdict class Ensembler: def __init__(self): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() self.comp_name = comp_name self.locker = load_pickle(f"../configs/configs-{self.comp_name}/locker.pkl") self.current_dict = load_pickle(f"../configs/configs-{self.comp_name}/current_dict.pkl") self.current_ens_no = self.current_dict['current_ens_no'] print("="*40) print("Current ens no:", self.current_ens_no) print("="*40) self.cv_score = [] # of fold3, all, single self.pblb_score = [] # corresponding self.Table = load_pickle(f"../configs/configs-{self.comp_name}/Table.pkl") #self.my_folds = pd.read_parquet(f"../input/input-{self.comp_name}/my_folds.parquet") self.id_folds_target = pd.read_parquet(f"../input/input-{self.comp_name}/id_folds_target.parquet") self.target = self.id_folds_target[self.locker['target_name']] #self.my_folds[self.locker["target_name"]].values self.id = self.id_folds_target[self.locker["id_name"]].values self.sample = pd.read_parquet(f"../input/input-{self.comp_name}/sample.parquet") #del self.my_folds gc.collect() def access_predictions(self, submission_list): self.submission_list = submission_list self.train_list = [] self.test_list = [] for i,j in submission_list: # access it if j.startswith("fold"): # it is a prediction self.train_list.append( load_pickle(f"../configs/configs-{self.comp_name}/oof_preds/oof_pred_e_{i}_{j}.pkl").reshape(-1,)) self.test_list.append( load_pickle(f"../configs/configs-{self.comp_name}/test_preds/test_pred_e_{i}_{j}.pkl").reshape(-1,)) # exp_no and index match for table so works names = self.Table.loc[i, "oof_fold_name"] ind = names.index(j) print() print("exp_no",i) try: self.pblb_score.append(self.Table.loc[i, 'pblb_all_fold'][ind]) print(f"pblb: {self.Table.loc[i, 'pblb_all_fold'][ind]}") except: print("pblb_score not found") try: self.cv_score.append(self.Table.loc[i, 'fold_mean'][ind]) print(f"cv: {self.Table.loc[i, 'fold_mean'][ind]}") except: print("cv_score is not found") elif j in ["all", "single"]: raise Exception("Need to work on it") elif j.startswith("feat"): # these are features raise Exception("Need to work on it") self.column_names.append(f"feat_l_1_e_{i}_feat") elif j.startswith("../working"): # picked some public kernel assert j.endswith(".csv") # must be a csv # [[0.7977, 0.799], "../working/.."] assert len(i) == 2 self.cv_score.append(i[0]) self.pblb_score.append(i[1]) print(f"cv: {i[0]} , pblb: {i[1]}") # no training self.train_list.append( np.ones((self.id_folds_target.shape[0], )) ) self.test_list.append( pd.read_csv(j)[self.locker['target_name']]) # got train and test #self.train_list = np.array() # draw correlation plot col_names = [str(i[0]) for i in submission_list] train_corr = (pd.DataFrame(np.array(self.train_list + [self.id_folds_target[self.locker['target_name']]]).T, columns= col_names + [self.locker['target_name']])).corr() test_corr = (pd.DataFrame(np.array(self.test_list).T, columns= col_names)).corr() print("-"*40) print("Train corr") print(train_corr) print("-"*40) print("Test corr") print(test_corr) print("-"*40) def save_process(self, save_dict): # save submissions_list a # technique # no need to save seperately contained in submissions_list name # update current_dict save_json(f"../configs/configs-{self.comp_name}/ensemble_logs/ens_{self.current_ens_no}.json", save_dict) self.current_ens_no += 1 self.current_dict['current_ens_no'] = self.current_ens_no save_pickle(f"../configs/configs-{self.locker['comp_name']}/current_dict.pkl", self.current_dict) def combine(self, technique): self.technique = technique if technique == "power_averaging_basic": # self.train_list is a list where each element is a list of submissions , [0] picking the first since not multi-class problem train_pred = [ np.sum([0.2 * i for i in f], axis=0) for f in [self.train_list]][0] test_pred = [ np.sum([0.2 * i for i in f], axis=0) for f in [self.test_list]][0] elif technique == "weight_averaging_by_pblb": train_pred = [ np.sum([i*j for i,j in zip(f,self.pblb_score) ], axis=0) for f in [self.train_list]][0] test_pred = [ np.sum([i*j for i,j in zip(f,self.pblb_score)], axis=0) for f in [self.test_list]][0] elif technique == "weight_average_by_cv": train_pred = [ np.sum([i*j for i,j in zip(f,self.cv_score) ], axis=0) for f in [self.train_list]][0] test_pred = [ np.sum([i*j for i,j in zip(f,self.cv_score)], axis=0) for f in [self.test_list]][0] print(test_pred) train_pred = np.array(train_pred)/sum(self.cv_score) test_pred = np.array(test_pred)/sum(self.cv_score) elif technique == "median": train_pred = [ np.median(f) for f in [self.train_list]][0] test_pred = [ np.median(f) for f in [self.test_list]][0] print(train_pred.shape, test_pred.shape) # save oof predictions #score = amex_metric(self.target, train_pred) score = getaroom_metrics(self.target, train_pred) print("train score: ",score) save_dict = defaultdict() save_dict["submission_list"] = self.submission_list save_dict["cv_score"] = score save_dict["technique"] = technique self.sample[self.locker['target_name']] = test_pred print() print(self.sample.head(3)) # Now working withing same comp because folds hold for same datset and not amex, amex2 input("Want to proceed!") self.sample.to_parquet(f"../working/{self.comp_name}_ens_{self.current_ens_no}.parquet") self.save_process(save_dict) if __name__ == "__main__": # For now works only for fold prediction # submission_list = [ # [215, "fold5"], # [1, "fold5"], # [265, "fold5"], # [49, "fold5"], # [138, "fold5"] # ] submission_list = [ # 1,8,20,18, 47, 14, 54, 43, 56, 42, 35, 3, 34, 6, 41 # [294,'fold5'], # [297,'fold5'], # [273,'fold5'], # [254,'fold5'], [[0.7977, 0.799], "../working/rr_mean_submission.csv"], [[0.7977, 0.799], "../working/rr_submission.csv"], [[0.7977, 0.799], "../working/rr_submission1.csv"] #[1, "fold5"], #[8, "fold5"], #[20, "fold5"], # [18, "fold5"], # [47, "fold5"], # [14, "fold5"], # [54, "fold5"], #[61, "fold5"], # [43, "fold5"], # [56, "fold5"], # [42, "fold5"], # [35, "fold5"], # [3, "fold5"], # [34, "fold5"], # [6, "fold5"], # [41, "fold5"], # [8, "fold5"], # [20, "fold5"], # [18, "fold5"], # [35, "fold5"], # [29, "fold5"], # [1, "fold5"], # [45, "fold5"], # [54, "fold5"], # [56, "fold5"], # [14, "fold5"], # [43, "fold5"], # [34, "fold5"], # [47, "fold5"], # [27, "fold5"], # [3, "fold5"], # [55, "fold5"], # [6, "fold5"], # [42, "fold5"], # [41, "fold5"], # [7, "fold5"], # [2, "fold5"], # [32, "fold5"], # [15, "fold5"], # [44, "fold5"], # [80, "fold5"], #[43, "fold5"], #[15, "fold5"], # [1, "fold5"], # [8, "fold5"], # [14, "fold5"], # [15, "fold5"], # [17, "fold5"], # [45, "fold5"], #[79, "fold5"], #[98, 'fold5'], #[111, 'fold5'], #[1, "fold5"], #[44, "fold5"], #[15, "fold5"], #[[0.798, 0.799], "../working/mean_submission.csv"], #[[0.7977, 0.799], "../working/test_lgbm_baseline_5fold_seed_blend.csv"], #[112, "fold5"], ] # "median", "mean" , "weighted_mean", "best", "rank", "weigh_by_cv", "weight_averaging_by_pblb", "weigh_by_cv_pblb_jump", "power_averaging_basic" technique = "weight_average_by_cv" #"weight_average_by_cv" #"power_averaging_basic" # "mean" # "weighted_mean" , "best" , "mean" e = Ensembler() e.access_predictions(submission_list) e.combine(technique) """ 0.795 215 fold5, 1 fold5, 265 fold5 , 0.796 49 fold5, 138 fold5 """ # some ideas # larger the jump b/w cv and pblb score better is the model pblb > cv not cv < pblb

File no 12: /src-framework3/exp_infos.txt
1: did feature extraction 2: used extracted features and score cv increased repeated till exp3 exp 4 to exp 10 ran xgbc

File no 13: /src-framework3/experiment.py
from optuna_search import OptunaOptimizer from feature_generator import features from feature_picker import Picker import os import sys import gc import pickle import pandas as pd import tracemalloc # from custom_models import UModel # from custom_models import * from utils import * from settings import * class Agent: def __init__( self, useful_features=[], model_name="", comp_type="2class", metrics_name="accuracy", n_trials=5, prep_list=[], fold_name = 'fold5', optimize_on=0, save_models=True, with_gpu=False, aug_type="Aug1", _dataset="ImageDataset", use_cutmix=True, callbacks_list=[], note="---", ): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() self.locker = load_pickle(f"../configs/configs-{comp_name}/locker.pkl") self.current_dict = load_pickle( f"../configs/configs-{comp_name}/current_dict.pkl" ) print("=" * 30) print(f"Current Exp no: {self.current_dict['current_exp_no']}") print("=" * 30) # ---------------------------------------------------------- self.useful_features = useful_features self.model_name = model_name self.comp_type = comp_type self.metrics_name = metrics_name self.n_trials = n_trials self.prep_list = prep_list self.fold_name = fold_name self.optimize_on = optimize_on self.save_models = True self.with_gpu = with_gpu self.aug_type = aug_type self._dataset = _dataset self.use_cutmix = use_cutmix self.callbacks_list = callbacks_list self.note = note def sanity_check(self): if "--|--" in [ self.useful_features, self.model_name, self.comp_type, self.metrics_name, self.n_trials, self.prep_list, self.fold_name, self.optimize_on, self.save_models, self.with_gpu, self.aug_type, self._dataset, self.note, ]: raise Exception("Found --|--- while sanity check!") def run( self, useful_features="--|--", model_name="--|--", comp_type="--|--", metrics_name="--|--", n_trials="--|--", prep_list="--|--", fold_name = "--|--", optimize_on="--|--", save_models="--|--", with_gpu="--|--", aug_type="--|--", _dataset="--|--", ): tracemalloc.start() check_memory_usage("Experiment started", self, 0) ###################################### # Memory uage # ###################################### if useful_features != "--|--": self.useful_features = useful_features if model_name != "--|--": self.model_name = model_name if comp_type != "--|--": self.comp_type = comp_type if metrics_name != "--|--": self.metrics_name = metrics_name if n_trials != "--|--": self.n_trials = n_trials if prep_list != "--|--": self.prep_list = prep_list if fold_name != "--|--": self.fold_name = fold_name if optimize_on != "--|--": self.optimize_on = optimize_on if save_models != "--|--": self.save_models = save_models if with_gpu != "--|--": self.with_gpu = with_gpu if aug_type != "--|--": self.aug_type = aug_type if _dataset != "--|--": self._dataset = _dataset self.sanity_check() # # my_folds = pd.read_csv(f"../configs/configs-{self.locker['comp_name']}/my_folds.csv")[self.useful_features + [self.locker["target_name"] , self.locker["id_name"], "fold"] ] # my_folds = pd.read_parquet( # f"../input/input-{self.locker['comp_name']}/my_folds.parquet", # columns=self.useful_features # + [self.locker["target_name"], self.locker["id_name"], "fold"], # ) # [self.useful_features + [self.locker["target_name"] , self.locker["id_name"], "fold"] ] # # print(my_folds.head(2)) # # taking only what is needed to reduce memory issue opt = OptunaOptimizer( model_name=self.model_name, comp_type=self.comp_type, metrics_name=self.metrics_name, n_trials=self.n_trials, prep_list=self.prep_list, fold_name = self.fold_name, optimize_on=self.optimize_on, with_gpu=self.with_gpu, save_models=self.save_models, aug_type=self.aug_type, _dataset=self._dataset, use_cutmix=self.use_cutmix, callbacks_list=self.callbacks_list, ) print(f"Total no of trials: {self.n_trials}") self.study, random_state = opt.run( self.useful_features) del opt # delete object gc.collect() if self.save_models == True: self._save_models(self.study, random_state) # Let's make perdiction on Test Set: # self._seed_it() check_memory_usage("run experiment Ends", self) tracemalloc.stop() def get_exp_no(self): # exp_no, current_level self.current_dict = load_pickle( f"../configs/configs-{self.locker['comp_name']}/current_dict.pkl" ) self.current_exp_no = int(self.current_dict["current_exp_no"]) def _save_models(self, study, random_state): Table = load_pickle(f"../configs/configs-{self.locker['comp_name']}/Table.pkl") Table = pd.DataFrame(Table) # what unifies it self.get_exp_no() # ExpNo- self.current_exp_no print("=" * 30) print(f"Current Exp no: {self.current_exp_no}") print("=" * 30) # # temp # whenever need to add new column # Table["callbacks_list"] = None # Table = Table[ # [ # "exp_no", # "model_name", # "bv", # "bp", # "random_state", # "with_gpu", # "aug_type", # "_dataset", # "use_cutmix", # "callbacks_list", # "features_list", # "level_no", # "oof_fold_name" # "opt_fold_name" # ###############"fold_no", # "no_iterations", # "prep_list", # "metrics_name", # "seed_mean", # "seed_std", # ---\ # "fold_mean", [] # "fold_std", [] # "pblb_single_seed", # "pblb_all_seed", # "pblb_all_fold", [] # "notes", # ] # ] # print(Table.columns) # input("What you see:") # # temp # Rule # initialize int/str feature with None and list with [] Table.loc[Table.shape[0], :] = [ self.current_exp_no, self.model_name, study.best_trial.value, study.best_trial.params, random_state, self.with_gpu, self.aug_type, self._dataset, self.use_cutmix, self.callbacks_list, self.useful_features, 1, #self.current_dict["current_level"], [], # oof on fold name self.fold_name, # opt on fold name self.optimize_on, self.n_trials, self.prep_list, self.metrics_name, None, None, [], [], None, None, [], self.note, ] self.current_exp_no += 1 # --------------- dump experiment no self.current_dict["current_exp_no"] = self.current_exp_no save_pickle( f"../configs/configs-{self.locker['comp_name']}/current_dict.pkl", self.current_dict, ) #---------------- sanity check table Table.exp_no = Table.exp_no.astype(int) Table.level_no = Table.level_no.astype(int) Table.no_iterations = Table.no_iterations.astype(int) # ---------------- dump table save_pickle(f"../configs/configs-{self.locker['comp_name']}/Table.pkl", Table) def show_variables(self): print() for i, (k, v) in enumerate(self.__dict__.items()): print(f"{i}. {k} :=======>", v) print() if __name__ == "__main__": # '100_165', # ['100_165', '98_166', '93_168', '91_171', '90_172', '89_173', '88_182', '87_183', '85_185', '84_188', '83_195', '82_186', '78_188', '75_191', '74_194', '72_195', '71_196', '70_201', '69_205', '68_208', '66_209', '65_216', '64_220', '63_229', '62_235', '61_242', '60_244', '59_245', '58_248', '52_234', '51_235', '50_237', '49_238', '48_239', '47_241', '44_242', '43_247', '42_258', '40_263', '39_266', '38_268', '37_270', 'filter35_165', 'filter34_168', 'filter33_186', 'filter32_265', 'filter31_256', 'filter30_234', 'filter29_273', 'filter28_313', 'filter27_275', 'filter26_317', 'filter25_319', 'filter24_324', 'filter23_336', 'filter22_340', 'filter21_347', 'filter20_355', 'filter19_368', 'filter18_375', 'filter17_377', 'filter16_386', 'filter15_430', 'filter14_448', 'filter13_481', 'filter12_527', 'filter11_537', 'filter10_580', 'filter9_594', 'filter8_569', 'filter7_1364', 'filter6.1_43', 'filter6_37', 'filter5_38', 'filter4_45', 'filter2_49', 'filter1_54']: for jd in [ '93_168', '91_171', '88_182', '83_195', '78_188', '70_201', '64_220', '52_234','filter23_336','filter7_1364', 'filter6.1_43']: #for jd in ['100_165', '98_166', '93_168', '91_171', '90_172', '89_173', '88_182', '87_183', '85_185', '84_188', '83_195', '82_186', '78_188', '75_191', '74_194', '72_195', '71_196', '70_201', '69_205', '68_208', '66_209', '65_216', '64_220', '63_229', '62_235', '61_242', '60_244', '59_245', '58_248', '52_234', '51_235', '50_237', '49_238', '48_239', '47_241', '44_242', '43_247', '42_258', '40_263', '39_266', '38_268', '37_270', 'filter35_165', 'filter34_168', 'filter33_186', 'filter32_265', 'filter31_256', 'filter30_234', 'filter29_273', 'filter28_313', 'filter27_275', 'filter26_317', 'filter25_319', 'filter24_324', 'filter23_336', 'filter22_340', 'filter21_347', 'filter20_355', 'filter19_368', 'filter18_375', 'filter17_377', 'filter16_386', 'filter15_430', 'filter14_448', 'filter13_481', 'filter12_527', 'filter11_537', 'filter10_580', 'filter9_594', 'filter8_569', 'filter7_1364', 'filter6.1_43', 'filter6_37', 'filter5_38', 'filter4_45', 'filter2_49', 'filter1_54']: # ========================================================== useful_features = [] # auto_features =['ver2_statistical'] # for f in auto_features: # useful_features += amzcomp1_settings().feature_dict[f] auto_features = [jd] for f in auto_features: useful_features += amzcomp1_settings().auto_filtered_features[f] useful_features = list(set(useful_features)) # exp 20 features added #useful_features += ['Water_Supply_Once in two days', 'Dust_and_Noise_Medium', 'Property_Area', 'Dust_and_Noise_Low', 'Crime_Rate_Well above average', 'Property_Type_Bungalow', 'Traffic_Density_Score', 'Crime_Rate_Slightly below average', 'Number_of_Windows', 'Property_Type_Single-family home', 'Power_Backup_Yes', 'Air_Quality_Index', 'Frequency_of_Powercuts', 'Crime_Rate_Well below average', 'Neighborhood_Review', 'Property_Type_Apartment', 'Water_Supply_Once in a day - Evening', 'Furnishing_Semi_Furnished', 'Water_Supply_Once in a day - Morning', 'Property_Type_Container Home', 'Property_Type_Duplex', 'Water_Supply_NOT MENTIONED', 'Number_of_Doors', 'Power_Backup_No', 'Furnishing_Unfurnished'] # ========================================================== model_name = "xgbr" # -----s---> """ [ "lgr", "lir", "xgb", "xgbc", "xgbr", "cbc", "cbr", "mlpc", "rg", "ls", "knnc", "dtc", "adbc", "gbmc" , "gbmr, "hgbc", "lgb", "lgbmc", "lgbmr", "rfc" , "rfr", # --------------->["tabnetr", "tabnetc", "k1", "k2", "k3", "tez1", "tez2", "p1" ,"pretrained"] """ comp_type = ( "2class" # -------->["regression", "2class","multi_class", "multi_label"] ) metrics_name = "amzcomp1_metrics" # --------->["getaroom_metrics", "amex_metric","amex_metric_mod", "accuracy","f1","recall","precision", "auc", "logloss","auc_tf","mae","mse","rmse","msle","rmsle","r2"] n_trials = 100 #0 #30 # ------------> no of times to run optuna prep_list = [ "Sd", ] # ------> ["SiMe", "SiMd",~ "SiMo", "Mi", "Ro", "Sd", "Lg"] <= _prep_list prep_list = [] fold_name = "fold5" # ['fold3', 'fold5', 'fold10', 'fold20'] optimize_on = [random.choice(range(5))] # [0] # fold on which optimize # 0,1,2,4 with_gpu = True aug_type = "aug2" # "aug1", "aug2", "aug3", "aug4" _dataset = "DigitRecognizerDataset" # "BengaliDataset", "ImageDataset", "DigitRecognizerDataset", "DigitRecognizerDatasetTez2" use_cutmix = False # CALLBACKS # lgbmClassifiers callback: # https://lightgbm.readthedocs.io/en/latest/Python-API.html#callbacks """ ############################ change learning rate custom schedulers :=>cosine_decay , exponential_decay, simple_decay built in scheduler :=>"ReduceLROnPlateau" # EarlyStopping, ############################# save model :=>chk_pt : whether to use checkpoint or not ############################### stop training :=>terminate_on_NaN >Callbacks monitor a particular varaibable and stops exectuion when it crosses fixed value , >early_top also monitors a particular value but it stops when it stops improving it has some patience builtin callback :=>early_stopping custom callback :=>myCallback1 """ # don't use early_stopping with cyclic decay lr because model gets good and bad periodically and it doesn't mean we should terminate. # "swa", "cosine_decay", "exponential_decay", "simple_decay", "ReduceLROnPlateau", "chk_pt", "terminate_on_NaN", "early_stopping", "myCallback1" callbacks_list = ["terminate_on_NaN"] # ["exponential_decay", "terminate_on_NaN"] # [swa,early_stop] # ----------------------------------------------------------- note = "ragnar" e = Agent( useful_features=useful_features, model_name=model_name, comp_type=comp_type, metrics_name=metrics_name, n_trials=n_trials, prep_list=prep_list, fold_name = fold_name, optimize_on=optimize_on, with_gpu=with_gpu, aug_type=aug_type, _dataset=_dataset, use_cutmix=use_cutmix, callbacks_list=callbacks_list, note=note, ) print("=" * 40) print("Useful_features:", useful_features) e.run() del e # ------------------------------------------------------------- # exp_list = ["1"] # ----------------> [1,2,3,4] # e.show(exp_list) """ {'learning_rate': 0.010821262164314453, 'max_depth': 16, 'min_child_weight': 5, 'subsample': 0.4521783648128741, 'n_estimators': 500, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist', 'gpu_id': 0, 'predictor': 'gpu_predictor'} """ # Make prediction also os.system(f"python predict.py") #break

File no 14: /src-framework3/feature_generator.py
import pandas as pd import numpy as np from scipy.stats import skew from scipy.stats import median_abs_deviation #from statsmodels import robust from sklearn import model_selection import os import sys import pickle from collections import defaultdict from utils import * #from auto_exp import * """ generates new features on top of some existing featrues. and stores this info(title: [column_names_generated, columns_name_used_to_generate]) as a dictionary. # This dictionary is used only to diplay it is not used to get access of features> # Features accessed by their name initials: pred_.. feat_... """ class features: def __init__(self): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() self.comp_name = comp_name self.locker = load_pickle(f"../configs/configs-{comp_name}/locker.pkl") # ------------------------------------- self.test_feat_path = f"../configs/configs-{comp_name}/test_feats/" self.train_feat_path = f"../configs/configs-{comp_name}/train_feats/" self.level_no = None self.current_dict = None #-------------------------------------------------- self.get_feat_no() # load level_no and current_feature_no from DISC # self.level_no and self.current_feature_no is update along with selfcurrent_dict #---------------------------------------------------- self.useful_features = load_pickle( f"../configs/configs-{self.locker['comp_name']}/useful_features_l_{self.level_no}.pkl" ) # from patsylearn import PatsyTransformer # transformer = PatsyTransformer("y ~ a + b + a^2 + b^2") # transformer.fit(data) def change_level(self, new_val="--|--"): if new_val != "--|--": self.level_no = new_val else: self.level_no += 1 self.current_dict["current_level"] = self.level_no save_pickle( f"../configs/configs-{self.locker['comp_name']}/current_dict.pkl", self.current_dict, ) def display_features_generated(self): # display all the feature engineering done so far # Key:- f"l{self.level_no}_f{feat_no}" # value:- [created, from , info] self.feat_dict = load_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl" ) for key, value in self.feat_dict.items(): print(f"Title: {key}") print("features created:") print(value[0]) # print("from:") # print(value[1]) print() def show_variables(self): print() for i, (k, v) in enumerate(self.__dict__.items()): print(f"{i}. {k} :=======>", v) print() def get_feat_no(self): # exp_no, current_level, current_feature_no self.current_dict = load_pickle( f"../configs/configs-{self.locker['comp_name']}/current_dict.pkl" ) self.level_no = int(self.current_dict["current_level"]) self.current_feature_no = int(self.current_dict["current_feature_no"]) def isRepetition(self, gen_features, old_features, feat_title): # f"create_statistical_features_l_{self.level_no}_f_{self.feat_no}" # Standard naming convention # f"feat_l_{self.level_no}_f_{self.feat_no}_csf # feat_dict[f"exp_{self.exp_no}"] = [ # [f"pred_e_{self.exp_no}_{self.fold_name}"], # self.useful_features # ] # feat_l_2_f_23_std # self.curr for key, value in self.feat_dict.items(): if key.split("_")[0]== "feat": # it is a feature entry # feat_l_{self.level_no}_f_{feat_no}_nan_count # old_features same and title same if set(value[0]) == set(old_features) and key.split("_")[-1] == feat_title: raise Exception(f"This set of feature is already there. details key name: {key}") # for key, value in self.feat_dict.items(): # f1, f2, ft = value # if f2 == 0: # # from base # pass # elif len(f1[0].split("_")[0]) < 5 or ( # f1[0].split("_")[0][0] == "l" and f1[0].split("_")[0][2] == "f" # ): # # originate from base so f2 can't be split # f1 = ["_".join(f.split("_")[2:]) for f in f1] # gen_features = ["_".join(f.split("_")[2:]) for f in gen_features] # else: # f2 = ["_".join(f.split("_")[2:]) for f in f2] # old_features = ["_".join(f.split("_")[2:]) for f in old_features] # f1 = ["_".join(f.split("_")[2:]) for f in f1] # gen_features = ["_".join(f.split("_")[2:]) for f in gen_features] # if f1 == gen_features and f2 == old_features and ft == feat_title: # raise Exception("This feature is already present!") def create_statistical_features(self, useful_features="--|--"): fill_na_with = -100 if useful_features == "--|--": useful_features = self.useful_features else: self.useful_features = useful_features # Get train, test from bottleneck: #------------------------------------------------------------------------ # BOTTLENECK return_type = "numpy_array" self.optimize_on = None # just to make sure it is not called fold_name = "fold_check" #self._state = "seed" state = "seed" self.val_idx, self.my_folds, self.xvalid, self.ytrain, self.yvalid, ordered_list_test = bottleneck(self.locker['comp_name'],self.useful_features, fold_name, self.optimize_on, state, return_type) self.xvalid = None self.yvalid = None self.val_idx = None self.test,ordered_list_train = bottleneck_test(self.locker['comp_name'], self.useful_features, return_type) # sanity check: for i,j in zip(ordered_list_test, ordered_list_train): if i != j: raise Exception(f"Features don't correspond in test - train {i},{j}") ordered_list_test = None ordered_list_train = None # self.test, self.my_folds #------------------------------------------------------------------------ self.get_feat_no() # --updated self.current_feature_no to the latest feat no # self.level_no, self.current_feature_no self.feat_dict = load_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl" ) # new set of feature is created so increase feat no feat_no = self.current_feature_no + 1 #feat_no = 10 feat_title = f"feat_l_{self.level_no}_f_{feat_no}_csf{fill_na_with}" # to mke it unique in the dictionary # ------------------------------------------ new_features = [ f"feat_l_{self.level_no}_f_{feat_no}_nan_count", f"feat_l_{self.level_no}_f_{feat_no}_num_missing_std", f"feat_l_{self.level_no}_f_{feat_no}_abs_sum", f"feat_l_{self.level_no}_f_{feat_no}_sem", f"feat_l_{self.level_no}_f_{feat_no}_std", f"feat_l_{self.level_no}_f_{feat_no}_medianad", f"feat_l_{self.level_no}_f_{feat_no}_meanad", f"feat_l_{self.level_no}_f_{feat_no}_avg", f"feat_l_{self.level_no}_f_{feat_no}_median", f"feat_l_{self.level_no}_f_{feat_no}_max", f"feat_l_{self.level_no}_f_{feat_no}_min", f"feat_l_{self.level_no}_f_{feat_no}_skew", ] # --------------------------Duplicacy check self.isRepetition( new_features, useful_features, feat_title ) # check for duplicate process print(self.train_feat_path) print(self.test_feat_path) print() #------------------------------------------------- # nan_count try: # for dataframe: self.test.isnull().sum(axis=1) val1= np.isnan(self.test).sum(axis=1) save_pickle(self.test_feat_path+ f"test_feat_l_{self.level_no}_f_{feat_no}_nan_count.pkl", val1) val2= np.isnan(self.my_folds).sum(axis=1) save_pickle(self.train_feat_path+ f"train_feat_l_{self.level_no}_f_{feat_no}_nan_count.pkl", val2) print(val2) print(val1) print() except: raise Exception(f"Couldn't create feat_l_{self.level_no}_f_{feat_no}_nan_count") #-------------------------------------------------------------- # num_missing_std try: # for dataframe: self.test.isnull().sum(axis=1) val1= np.isnan(self.test).std(axis=1).astype("float") save_pickle(self.test_feat_path+ f"test_feat_l_{self.level_no}_f_{feat_no}_num_missing_std.pkl", val1) val2= np.isnan(self.my_folds).std(axis=1).astype("float") save_pickle(self.train_feat_path+ f"train_feat_l_{self.level_no}_f_{feat_no}_num_missing_std.pkl", val2) print(val2) print(val1) print() except: raise Exception(f"Couldn't create feat_l_{self.level_no}_f_{feat_no}_num_missing_std") # below all are affected by nan # ------------------------------------------------- # So first fill nan self.my_folds[np.isnan(self.my_folds)] = fill_na_with self.test[np.isnan(self.test)] = fill_na_with # sanity check assert np.isnan(self.my_folds).sum() ==0 assert np.isnan(self.test).sum() ==0 # quite different value is added #----------------------------------------------------- # abs_sum : nan affected try: # for dataframe: self.test.isnull().sum(axis=1) val1= np.abs(self.test).sum(axis=1) print(val1.shape, "abs_sum") save_pickle(self.test_feat_path+ f"test_feat_l_{self.level_no}_f_{feat_no}_abs_sum.pkl", val1) val2= np.abs(self.my_folds).sum(axis=1) save_pickle(self.train_feat_path+ f"train_feat_l_{self.level_no}_f_{feat_no}_abs_sum.pkl", val2) print(val2) print(val1) print() except: raise Exception(f"Couldn't create feat_l_{self.level_no}_f_{feat_no}_abs_sum") # sem : nan affected try: # for dataframe: self.test.isnull().sum(axis=1) val1= np.std(self.test, axis=1)/np.sqrt(self.test.shape[1]) print(val1.shape, "val1") save_pickle(self.test_feat_path+ f"test_feat_l_{self.level_no}_f_{feat_no}_sem.pkl", val1) val2= np.std(self.my_folds, axis=1)/np.sqrt(self.my_folds.shape[1]) save_pickle(self.train_feat_path+ f"train_feat_l_{self.level_no}_f_{feat_no}_sem.pkl", val2) print(val2) print(val1) print() except: raise Exception(f"Couldn't create feat_l_{self.level_no}_f_{feat_no}_sem") # std : nan affected try: # for dataframe: self.test.isnull().sum(axis=1) val1= np.std(self.test, axis=1) save_pickle(self.test_feat_path+ f"test_feat_l_{self.level_no}_f_{feat_no}_std.pkl", val1) val2= np.std(self.my_folds, axis=1) save_pickle(self.train_feat_path+ f"train_feat_l_{self.level_no}_f_{feat_no}_std.pkl", val2) print(val2) print(val1) print() except: raise Exception(f"Couldn't create feat_l_{self.level_no}_f_{feat_no}_std") # medianad : nan affected try: # for dataframe: self.test.isnull().sum(axis=1) # need to reshpae since medina creates 1D array and thus can't be broadcasted to 2D #val1= np.median(np.absolute(self.test - np.median(self.test, axis=1)), axis=1).reshape(-1) #val1 = self.test.mad(axis=1) #robust.mad(self.test, axis=1) val1 = median_abs_deviation(self.test, axis=1) save_pickle(self.test_feat_path+ f"test_feat_l_{self.level_no}_f_{feat_no}_medianad.pkl", val1) #val2= np.median(np.absolute(self.my_folds - np.median(self.my_folds, axis=1)), axis=1).reshape(-1) #val2 = self.my_folds.mad(axis=1) #robust.mad(self.my_folds, axis=1) val2 = median_abs_deviation(self.my_folds, axis=1) save_pickle(self.train_feat_path+ f"train_feat_l_{self.level_no}_f_{feat_no}_medianad.pkl", val2) print(val2) print(val1) print() except: raise Exception(f"Couldn't create feat_l_{self.level_no}_f_{feat_no}_medianad") # meanad : nan affected try: # for dataframe: self.test.isnull().sum(axis=1) # need to reshpae since medina creates 1D array and thus can't be broadcasted to 2D #val1= np.median(np.absolute(self.test - np.median(self.test, axis=1)), axis=1).reshape(-1) val1 = np.mean(np.abs(self.test - np.mean(self.test, axis=1).reshape(-1,1)), axis=1) save_pickle(self.test_feat_path+ f"test_feat_l_{self.level_no}_f_{feat_no}_meanad.pkl", val1) #val2= np.median(np.absolute(self.my_folds - np.median(self.my_folds, axis=1)), axis=1).reshape(-1) val2 = np.mean(np.abs(self.my_folds - np.mean(self.my_folds, axis=1).reshape(-1,1)), axis=1) save_pickle(self.train_feat_path+ f"train_feat_l_{self.level_no}_f_{feat_no}_meanad.pkl", val2) print(val2) print(val1) print() except: raise Exception(f"Couldn't create feat_l_{self.level_no}_f_{feat_no}_meanad") # avg : nan affected try: # for dataframe: self.test.isnull().sum(axis=1) # need to reshpae since medina creates 1D array and thus can't be broadcasted to 2D val1= np.mean(self.test, axis=1) save_pickle(self.test_feat_path+ f"test_feat_l_{self.level_no}_f_{feat_no}_avg.pkl", val1) val2= np.mean(self.my_folds, axis=1) save_pickle(self.train_feat_path+ f"train_feat_l_{self.level_no}_f_{feat_no}_avg.pkl", val2) print(val2) print(val1) print() except: raise Exception(f"Couldn't create feat_l_{self.level_no}_f_{feat_no}_avg") # median : nan affected try: # for dataframe: self.test.isnull().sum(axis=1) # need to reshpae since medina creates 1D array and thus can't be broadcasted to 2D val1= np.median(self.test, axis=1) save_pickle(self.test_feat_path+ f"test_feat_l_{self.level_no}_f_{feat_no}_median.pkl", val1) val2= np.median(self.my_folds, axis=1) save_pickle(self.train_feat_path+ f"train_feat_l_{self.level_no}_f_{feat_no}_median.pkl", val2) print(val2) print(val1) print() except: raise Exception(f"Couldn't create feat_l_{self.level_no}_f_{feat_no}_median") # max : nan affected try: # for dataframe: self.test.isnull().sum(axis=1) # need to reshpae since medina creates 1D array and thus can't be broadcasted to 2D val1= np.max(self.test, axis=1) save_pickle(self.test_feat_path+ f"test_feat_l_{self.level_no}_f_{feat_no}_max.pkl", val1) val2= np.max(self.my_folds, axis=1) save_pickle(self.train_feat_path+ f"train_feat_l_{self.level_no}_f_{feat_no}_max.pkl", val2) print(val2) print(val1) print() except: raise Exception(f"Couldn't create feat_l_{self.level_no}_f_{feat_no}_max") # min : nan affected try: # for dataframe: self.test.isnull().sum(axis=1) # need to reshpae since medina creates 1D array and thus can't be broadcasted to 2D val1= np.min(self.test, axis=1) save_pickle(self.test_feat_path+ f"test_feat_l_{self.level_no}_f_{feat_no}_min.pkl", val1) val2= np.min(self.my_folds, axis=1) save_pickle(self.train_feat_path+ f"train_feat_l_{self.level_no}_f_{feat_no}_min.pkl", val2) print(val2) print(val1) print() except: raise Exception(f"Couldn't create feat_l_{self.level_no}_f_{feat_no}_min") # skew : nan affected try: # for dataframe: self.test.isnull().sum(axis=1) # need to reshpae since medina creates 1D array and thus can't be broadcasted to 2D val1= np.min(self.test, axis=1) save_pickle(self.test_feat_path+ f"test_feat_l_{self.level_no}_f_{feat_no}_skew.pkl", val1) val2= np.min(self.my_folds, axis=1) save_pickle(self.train_feat_path+ f"train_feat_l_{self.level_no}_f_{feat_no}_skew.pkl", val2) print(val2) print(val1) print() except: raise Exception(f"Couldn't create feat_l_{self.level_no}_f_{feat_no}_skew") del val1, val2 gc.collect() print(new_features) print() v = input("Are you sure you want to add these features?: [y/n]") if v.lower() == 'n': raise Exception("Process terminated.") # -----------------------------dump current dict self.current_feature_no = feat_no self.current_dict["current_level"] = self.level_no self.current_dict["current_feature_no"] = self.current_feature_no save_pickle( f"../configs/configs-{self.locker['comp_name']}/current_dict.pkl", self.current_dict, ) # -----------------------------dump feature dictionary feat_dict = load_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl" ) feat_dict[feat_title] = [new_features, useful_features] # feat_dict[f"l_{self.level_no}_f_{feat_no}"] = [ # new_features, # useful_features, # feat_title, # ] save_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl", feat_dict, ) print("New features created:- ") print(new_features) def create_unique_characters(self, useful_features="--|--"): raise Exception("Don't enter") feat_title = "unique_characters" self.my_folds = pd.read_csv( f"../configs/configs-{self.locker['comp_name']}/my_folds.csv" ) self.test = pd.read_csv( f"../configs/configs-{self.locker['comp_name']}/test.csv" ) if useful_features == "--|--": useful_features = self.useful_features self.get_feat_no() # --updated self.current_feature_no to the latest feat no self.feat_dict = load_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl" ) feat_no = self.current_feature_no + 1 # ------------------------------------------ # From https://www.kaggle.com/ambrosm/tpsmay22-eda-which-makes-sense new_features = [] for i in range(10): new_features.append(f"ch{i}") new_features.append(f"unique_characters") # ------------------------------------------------- self.isRepetition( new_features, useful_features, feat_title ) # check for duplicate process # ------------------------------------------------- for df in [self.test, self.my_folds]: for i in range(10): df[f"ch{i}"] = df.f_27.str.get(i).apply(ord) - ord("A") df["unique_characters"] = df.f_27.apply(lambda s: len(set(s))) # -----------------------------dump data self.my_folds.to_csv( f"../configs/configs-{self.locker['comp_name']}/my_folds.csv", index=False ) self.test.to_csv( f"../configs/configs-{self.locker['comp_name']}/test.csv", index=False ) # -----------------------------dump current dict self.current_feature_no = feat_no self.current_dict["current_level"] = self.level_no self.current_dict["current_feature_no"] = self.current_feature_no save_pickle( f"../configs/configs-{self.locker['comp_name']}/current_dict.pkl", self.current_dict, ) # -----------------------------dump feature dictionary feat_dict = load_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl" ) feat_dict[f"l_{self.level_no}_f_{feat_no}"] = [ new_features, useful_features, feat_title, ] save_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl", feat_dict, ) print("New features create:- ") print(new_features) def create_interaction_features(self, useful_features="--|--"): raise Exception("Don't enter") feat_title = "interaction_features" self.my_folds = pd.read_csv( f"../configs/configs-{self.locker['comp_name']}/my_folds.csv" ) self.test = pd.read_csv( f"../configs/configs-{self.locker['comp_name']}/test.csv" ) if useful_features == "--|--": useful_features = self.useful_features else: self.useful_features = useful_features self.get_feat_no() # --updated self.current_feature_no to the latest feat no self.feat_dict = load_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl" ) feat_no = self.current_feature_no + 1 # ------------------------------------------ # From https://www.kaggle.com/ambrosm/tpsmay22-eda-which-makes-sense new_features = ["i_02_21", "i_05_22", "i_00_01_26"] # ------------------------------------------------- self.isRepetition( new_features, useful_features, feat_title ) # check for duplicate process # ------------------------------------------------- for df in [self.test, self.my_folds]: df["i_02_21"] = (df.f_21 + df.f_02 > 5.2).astype(int) - ( df.f_21 + df.f_02 < -5.3 ).astype(int) df["i_05_22"] = (df.f_22 + df.f_05 > 5.1).astype(int) - ( df.f_22 + df.f_05 < -5.4 ).astype(int) i_00_01_26 = df.f_00 + df.f_01 + df.f_26 df["i_00_01_26"] = (i_00_01_26 > 5.0).astype(int) - ( i_00_01_26 < -5.0 ).astype(int) # -----------------------------dump data self.my_folds.to_csv( f"../configs/configs-{self.locker['comp_name']}/my_folds.csv", index=False ) self.test.to_csv( f"../configs/configs-{self.locker['comp_name']}/test.csv", index=False ) # -----------------------------dump current dict self.current_feature_no = feat_no self.current_dict["current_level"] = self.level_no self.current_dict["current_feature_no"] = self.current_feature_no save_pickle( f"../configs/configs-{self.locker['comp_name']}/current_dict.pkl", self.current_dict, ) # -----------------------------dump feature dictionary feat_dict = load_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl" ) feat_dict[f"l_{self.level_no}_f_{feat_no}"] = [ new_features, useful_features, feat_title, ] save_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl", feat_dict, ) print("New features create:- ") print(new_features) def create_polynomial_features(self,title, useful_features="--|--"): if useful_features == "--|--": useful_features = self.useful_features else: self.useful_features = useful_features # Get train, test from bottleneck: #------------------------------------------------------------------------ # BOTTLENECK return_type = "numpy_array" self.optimize_on = None # just to make sure it is not called fold_name = "fold_check" #self._state = "seed" state = "seed" self.val_idx, self.xtrain, self.xvalid, self.ytrain, self.yvalid, ordered_list_train = bottleneck(self.locker['comp_name'],self.useful_features, fold_name, self.optimize_on, state, return_type) self.xvalid = None self.yvalid = None self.val_idx = None print(self.xtrain.shape) self.xtrain = pd.DataFrame(self.xtrain, columns = useful_features) print(self.xtrain.iloc[:10,:5]) self.test,ordered_list_test = bottleneck_test(self.locker['comp_name'], self.useful_features, return_type) print(self.test.shape) self.test = pd.DataFrame(self.test, columns = useful_features) # sanity check: for i,j in zip(ordered_list_test, ordered_list_train): if i != j: raise Exception(f"Features don't correspond in test - train {i},{j}") useful_features = ordered_list_test # just to make sure order ordered_list_test = None ordered_list_train = None # self.test, self.my_folds #------------------------------------------------------------------------ # This updated input folder #---------------------------------------------- # places where feature are updated #1> useful_features_l_1 #2> feature_dict (here base is never used so can skip) #3> input_dict self.input_dict = load_pickle( f"../input/input-{self.comp_name}/input_dict.pkl" ) print("input dict before") print(self.input_dict.keys()) useful_features_l_1 = load_pickle(f"../configs/configs-{self.comp_name}/useful_features_l_1.pkl") print("Total features before", len(useful_features_l_1)) # ------------------------------------------ self.xtrain = self.xtrain.values self.test = self.test.values train_dummy = np.array([], dtype=np.int8).reshape(self.xtrain.shape[0],0) test_dummy = np.array([], dtype=np.int8).reshape(self.test.shape[0],0) no_features = len(useful_features) generated_features = [] for i in range(no_features): f = useful_features[i] generated_features += [f"{f}*{useful_features[i]}" for i in range(no_features)] train_dummy = np.concatenate((train_dummy, self.xtrain* self.xtrain[:,i].reshape(-1,1)), axis=1) test_dummy = np.concatenate((test_dummy, self.test* self.test[:,i].reshape(-1,1)), axis=1) print("After") print(train_dummy.shape) print(test_dummy.shape) print(len(generated_features)) print(generated_features[:6]) # poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False) # t= poly.fit_transform(self.xtrain) # print(t.shape) # print(t.iloc[:10,:5]) # # PUSH FEATURES self.input_dict[f"{title}_interact"] = generated_features print("input dict after ") print(self.input_dict.keys()) useful_features_l_1 += generated_features # sanity check remove duplicate if called twice #assert len(useful_features_l_1) == len(list(set(useful_features_l_1))) useful_features_l_1 = list(set(useful_features_l_1)) print("Total features after", len(useful_features_l_1)) print(useful_features_l_1[:4]) v = input("Do you want to create these features? y/Y or else: ") if v.lower() == "y": self.xtrain = pd.DataFrame(train_dummy , columns = generated_features) self.test = pd.DataFrame(test_dummy , columns = generated_features) self.xtrain.to_parquet(f"../input/input-{self.comp_name}/train_{title}_interact.parquet") self.test.to_parquet(f"../input/input-{self.comp_name}/test_{title}_interact.parquet") save_pickle(f"../input/input-{self.comp_name}/input_dict.pkl", self.input_dict) save_pickle(f"../configs/configs-{self.comp_name}/useful_features_l_1.pkl",useful_features_l_1) print("Updated!") print() print(f"Title: {title}_interact") print() print(generated_features) else: print("Aborted!") def pull_input(self, source_comp_name, source_feat_name): """ source_comp_name = "amex4" source_feat_name = "last_mean_diff" """ train_dummy = pd.read_parquet(f"../input/input-{source_comp_name}/train_{source_feat_name}.parquet") test_dummy = pd.read_parquet(f"../input/input-{source_comp_name}/test_{source_feat_name}.parquet") generated_features = list(train_dummy.columns) #---------------------------------------------- # places where feature are updated #1> useful_features_l_1 #2> feature_dict (here base is never used so can skip) #3> input_dict self.input_dict = load_pickle( f"../input/input-{self.comp_name}/input_dict.pkl" ) print("input dict before") print(self.input_dict.keys()) useful_features_l_1 = load_pickle(f"../configs/configs-{self.comp_name}/useful_features_l_1.pkl") print("Total features before", len(useful_features_l_1)) print("Total Generated features", len(generated_features)) # ------------------------------------------ # PUSH FEATURES self.input_dict[source_feat_name] = generated_features print("input dict after ") print(self.input_dict.keys()) useful_features_l_1 += generated_features # sanity check remove duplicate if called twice assert len(useful_features_l_1) == len(list(set(useful_features_l_1))) useful_features_l_1 = list(set(useful_features_l_1)) print("Total features after", len(useful_features_l_1)) print(useful_features_l_1[:4]) v = input("Do you want to create these features? y/Y or else: ") if v.lower() == "y": self.xtrain = pd.DataFrame(train_dummy , columns = generated_features) self.test = pd.DataFrame(test_dummy , columns = generated_features) self.xtrain.to_parquet(f"../input/input-{self.comp_name}/train_{source_feat_name}.parquet") self.test.to_parquet(f"../input/input-{self.comp_name}/test_{source_feat_name}.parquet") save_pickle(f"../input/input-{self.comp_name}/input_dict.pkl", self.input_dict) save_pickle(f"../configs/configs-{self.comp_name}/useful_features_l_1.pkl",useful_features_l_1) print("Updated!") print() print(generated_features) else: print("Aborted!") from settings import * if __name__ == "__main__": with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() with open(f"../configs/configs-{comp_name}/locker.pkl", "rb") as f: a = pickle.load(f) # ---------------------------------------------------------- # ----------------------------------------------------------- ft = features() ## Statistical features useful_features = amzcomp1_settings.feature_dict['ver2'] #ft.create_statistical_features( useful_features) # ------------ # ft.create_unique_characters() # ft.create_interaction_features() # Interaction/polynomial features #useful_features = amzcomp1_settings().filtered_features['filter6'] title = 'ver2' print("Feature to interact") print(useful_features) #input() ft.create_polynomial_features(title, useful_features) #ft.create_statistical_features(useful_features) #ft.display_features_generated() # title = '--' # amex = amex4_settings() # useful_features = amex.feature_dict2[title] # print(useful_features) # ft.create_polynomial_features(title,useful_features) # source_comp_name = "amexdummy" # source_feat_name = 'date' # ft.pull_input(source_comp_name=source_comp_name, source_feat_name=source_feat_name) # print("===================") # # ft.show_variables() # useful_features = getaroom_settings().feature_dict['base'] # title = "base" # print(useful_features) # ft.create_polynomial_features(title,useful_features)

File no 15: /src-framework3/feature_picker.py
import pandas as pd from sklearn import model_selection import os import sys import pickle from collections import defaultdict """ picks features for training a model params: list of levels, list of features, title. """ class Picker: def __init__(self): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() self.locker = self.load_pickle(f"../configs/configs-{comp_name}/locker.pkl") # ---------------------------------------------------------- self.list_levels = [] self.list_features = [] self.list_feat_title = [] self.feat_dict = self.load_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl" ) def show_variables(self): print() for i, (k, v) in enumerate(self.__dict__.items()): print(f"{i}. {k} :=======>", v) print() def save_pickle(self, path, to_dump): with open(path, "wb") as f: pickle.dump(to_dump, f) def load_pickle(self, path): with open(path, "rb") as f: o = pickle.load(f) return o def find_keys( self, list_levels="--|--", list_features="--|--", list_feat_title="--|--" ): # -----------------------------dump feature dictionary if list_levels != "--|--": self.list_levels = list_levels if list_features != "--|--": self.list_features = list_features if list_feat_title != "--|--": self.list_feat_title = list_feat_title all_keys = list(self.feat_dict.keys()) valid_keys1 = list(self.feat_dict.keys()) valid_keys2 = list(self.feat_dict.keys()) valid_keys3 = list(self.feat_dict.keys()) if self.list_levels != []: valid_keys1 = [] for key in list(self.feat_dict.keys()): t = str(key).split("_")[1] if t in str(self.list_levels): valid_keys1.append(key) if self.list_features != []: valid_keys2 = [] for key in list(self.feat_dict.keys()): t = str(key).split("_")[3] if t in str(self.list_features): valid_keys2.append(key) if self.list_feat_title != []: valid_keys3 = [] for key, val in list(self.feat_dict.items()): if str(val[2]) in str(self.list_feat_title): valid_keys3.append(key) valid_keys = set(valid_keys1).intersection(set(valid_keys2)) valid_keys = list(valid_keys.intersection(set(valid_keys3))) return valid_keys def find_features( self, list_levels="--|--", list_features="--|--", list_feat_title="--|--" ): # -----------------------------dump feature dictionary if list_levels != "--|--": self.list_levels = list_levels if list_features != "--|--": self.list_features = list_features if list_feat_title != "--|--": self.list_feat_title = list_feat_title valid_keys = self.find_keys( self.list_levels, self.list_features, self.list_feat_title ) valid_features = [] for key in valid_keys: valid_features += self.feat_dict[key][0] return valid_features def help(self): # display all the feature engineering done so far # Key:- f"l{self.level_no}_f{feat_no}" # value:- [created, from , info] for key, value in self.feat_dict.items(): print(key, f"{value[-1]} :-") print("features created:") print(value[0]) print("from:") print(value[1]) print("=" * 40) if __name__ == "__main__": p = Picker() # p.list_levels = ["1"] # p.list_features = ["1", "2", "0"] p.list_feat_title = ["unique_characters"] print(p.find_keys()) print() print(p.find_features())

File no 16: /src-framework3/find_roots.py
import os import sys from utils import * def roots(exp_no): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() Table = load_pickle(f"../configs/configs-{comp_name}/Table.pkl") base = load_pickle(f"../configs/configs-{comp_name}/useful_features_l_1.pkl") Features = [] Exp = [] exp_list = [exp_no] while True: current_features = [] temp = [] for e in exp_list: if e == 'base': temp.append((e, 'NA')) continue else: e = int(e) current_features += Table.loc[Table.exp_no == e, 'features_list'][e] temp.append((e, Table.loc[Table.exp_no == e, 'model_name'][e])) Exp.append(temp) # print(exp_list) # print() Features.append(current_features) entered = False exp_list = [] for feat in current_features: # if feat in base: exp_list.append('base') pass else: entered = True exp_list.append(feat.split("_")[2]) exp_list= list(sorted(set(exp_list))) if entered is False: # all base features # so break break # for f in Features: # print(f) # print() for i,j in enumerate(Exp[::-1]): print(f"LEVEL {i}:", j) print() if __name__ == '__main__': exp_no = 240 roots(exp_no)

File no 17: /src-framework3/global_variables.py
name = "temp" lr_start = 0 lr_end = 0 epochs = 0 # for the animation part of logs done = False # dart lgb max_score = 0.75 fold = 0 exp_no = None counter = 0 # mkdir exp_no = 0

File no 18: /src-framework3/grab.py
import os import pandas as pd import pickle import sys """ used to show stored variables: """ class Storage: def __init__(self): # read all stored files: # ----------------------------Keys and store it in [locker] with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() with open(f"../configs/configs-{comp_name}/locker.pkl", "rb") as f: self.locker = pickle.load(f) # ----------------------------current dict self.current_dict = self.load_pickle( f"../configs/configs-{self.locker['comp_name']}/current_dict.pkl" ) # -----------------------------features dict self.features_dict = self.load_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl" ) # ----------------------------base features self.useful_features_l1 = self.load_pickle( f"../configs/configs-{self.locker['comp_name']}/useful_features_l_1.pkl" ) # -----------------------------Table self.Table = self.load_pickle( f"../configs/configs-{self.locker['comp_name']}/Table.pkl" ) # ------------------------------my folds # self.my_folds = pd.read_csv( # f"../configs/configs-{self.locker['comp_name']}/my_folds.csv" # ) self.my_folds = pd.read_parquet( f"../input/input-{self.locker['comp_name']}/my_folds.parquet" ) # ------------------------------ test self.test = None if self.locker["data_type"] == "tabular": # self.test = pd.read_csv(f"../configs/configs-{self.locker['comp_name']}/test.csv") self.test = pd.read_parquet( f"../input/input-{self.locker['comp_name']}/test.parquet" ) # ---------------------------------container self.names = [ "locker", "current_dict", "features_dict", "useful_features_l1", "Table", "my_folds", "test", ] self.obj = [ self.locker, self.current_dict, self.features_dict, self.useful_features_l1, self.Table, self.my_folds, self.test, ] def show(self, list_keys): for k in list_keys: if int(k) < len(self.names) and int(k) >= 0: # valid if self.names[k] == "Table": print(f"{k}. {self.names[k]} :=======>") print(self.obj[k]) elif k == 2: # asked for feture dict # for f1,f2,ft in self.obj[k]: # print(ft) for i,(l,val) in enumerate(self.features_dict.items()): print(l) print(val[0],"-->",val[1]) print() print() #print(self.features_dict) #print(self.features_dict.keys()) else: print(f"{k}. {self.names[k]} :=======>", self.obj[k]) print() else: print(f"{k} is not a valid key!") def get(self, key_no): if int(key_no) < len(self.names) and int(key_no) >= 0: # valid return self.obj[key_no] else: raise Exception(f"{key_no} is not a valid key!") def get_log_table(self, exp_no): # -------------------------------log table log_table = self.load_pickle( f"../configs/configs-{self.locker['comp_name']}/log_exp_{exp_no}.pkl" ) return log_table def show_log_table(self, exp_no): print(self.get_log_table(exp_no)) def help(self): print("functions: show() get() show_log_table() get_log_table()") print() for i, n in enumerate(self.names): print(f"{i} :=======>", n) print() def save_pickle(self, path, to_dump): with open(path, "wb") as f: pickle.dump(to_dump, f) def load_pickle(self, path): with open(path, "rb") as f: o = pickle.load(f) return o if __name__ == "__main__": s = Storage() s.help() s.show([2]) # s.show([0, 1, 2, 3, 4])

File no 19: /src-framework3/info.txt
pip install -r requirements.txt conda install scikit-learn [TABULAR STEPS] STEP-->1 : First set ref STEP-->2 : run init_folders.py --> create_datasets.py STEP-->3 : put train.parquet, test.parquet, sample.parquet in input folder [requirement] train:- id_col, features, target ( train may or may not have id column but test must have id column) [if not in parquet then convert using csv_to_parquet.py] [Now see once input content train, sample, test using show_input.py] test:- id_col, features sample:- id_col, target STEP-->4 : run keys.py STEP-->5 : run create_folds.py [ Create New id columns for train] [ No need to sort test as test will always have submission with id column just never RESHUFFLE] [ Sort train by id column, if not create one by reshuffling since when we get folds we will also sort them] [No need to sort test, we just predict on them, no training] train------------------->my_folds # After this what should we have these files in input folder # what we did is train------------------->my_folds my_folds:- id_col, features, target , fold_cols test:- id_col, features sample:- id_col, target_col [You may now remove train folder] check once everything is as it should be using show_input.py STEP-->6 : run experiment.py / auto_exp.py # we should pass list of optimize_on in run() We won't be predicting for all the experiments we do. So keep log_exp_22.pkl file in a subfolder also make seperate folder for preds: oof_preds, test_preds # After 5-6 hr do plot the auto table to see which set performs well and you can limit the search in that direction # Like "f_base", "f_max" performs quite well STEP-->7 : run predict.py or run seed_it.py # calls opt() function but not run() so keep it that way as obj() don't require optimize_on Note : run() takes list optimize_on : obj() takes single integer optimize_on for each fold : both takes fold_name STEP-->8 : run output.py after running predict.py STEP-->9 : make submission submit.py kaggle competitions list kaggle competitions leaderboard amex-default-prediction --show | --download kaggle competitions submisssions amex-default-prediction kaggle competitions submit ventilator-pressure-prediction -f submission.csv -m "exp_{}_fold/single/all" #submit your submission.csv Note: all parquet file contains id and target , all pkl files contain 1d prediction STEP-->10 : auto_exp # initialize datasets # configs: should contain only those file which is used to reproduce the submission. So Table, and the log files[very small size file 1kb so keep it as it is used later for visualization] , my_folds, test, locker, current_dict # no need to store seed_all and seed_single file as they are not used in creating ensemble of models, # seed_all/seed_single since trained on full datasets are end points and can be only used to make submission and achieve highest possible. # tabular df: # image_df: image pixels are stored as dataframe STEPS: 1> move train.csv ,test.csv, sample.csv to models_ [make name train,test,sample] 2> decide: id_name : as that of train id columns target_name : as that of sample target column -----------Make format like below exactly ---------------------- train: ImageId, Label, pixel0, pixel1, pixel2, ... , pixel200, test: ImageId, pixel0, pixel1, pixel2, ... , pixel200 sample: ImageId, Label # sample may be huge in size and it don't change over time unline my_folds and test, so it is better to keep # fixed things in input 3> run keys.py after setting appropriate name of variables 4> run create_folds.py to create [my_folds.csv] # image_path: there is train.csv and sample.csv folder which contains image name and there are image folders initially> (before putting image ID column do sample(frac=1)) train.csv: image_id, target sample.csv: image_id, fake_target target_name >> sample target name id_name >> sample id name STEPS: 1> move train.csv to models_ by first rename id_name to image1.jpeg 2> move sample.csv to models_ as test.csv by first renaming id_name to image2.jpeg 3> move sample to models_ [image_id, target] 4> run keys.py after setting appropriate name of variables 5> run create_folds.py to create [my_folds.csv] # image folder # Note:- keep [self.valid_preds] and [spyelf.test_preds] ############################################################ # CREATE DATASETS # ############################################################ op 1> Create empty datasets on kaggle manually op 2> init datasets from cmd and create it after changing json names pickle is best but works only inside python while Feather is more portable across languages than pickle /home/pramit_mazumdar/anaconda3/envs/AKR_env2/bin/pip show pandas Need to be compatible in case of pickle #AKR_env2: 1.4.2 #AKR_env: 1.3.5 #kaggle: 1.3.5 # This is stable !pip install --upgrade pandas==1.3.5 For saving pandas dataframe parquet is best For saving 1D numpy array pkl is best # NOT THAT GREAT BENEFIT # Regarding improving speed in Case of exp:- save the optimize on fold in disc and call it everytime directly no need to preprocess and all Case of auto_exp:- Can't help here Case of seed:- save full dataset preprocessed Case of predict: Case of folds:- save each fold preprocessed ## To Do: implement First sort my_folds then save back [DONE] # then takeout predictions and keep separately [DONE] # save auto_logs separately for different model. reason:- different model has different sets of settings and same for all experiment with same model name Also saving different table is good because as no of exp increases then loading whole dataset will take up quit a space. # delete Table where ever possible # make oof/test preds contain fold10, fold5 do same for features_dict and while finding features in bottleneck and sanity_check [DONE] # remove reading test file each time inside for loop of predict # just after fitting model we can actually deleted xtrain, ytrain # idea source:- https://www.kaggle.com/code/ambrosm/amex-lightgbm-quickstart #--> Big Step do later # implement fillna_with parameter but in next comp # If we make sepearte Table for each model will that not help create multi-processing [ SPLIT EVERYTHING MODEL WISE ] ( so at one time only work on one experiment in a given model, but will allow to work on separate model simultaneously also loading time of table will decrease) # current dict of each model will be also different : Actually no need to maintain it just load from the table last row [Just store level no, and when we jump to level 2 we will not run any code for the moment, that we can do] In that way we don't have to deal with the issue of current dict overwritten by unwanted experiment 2 LEVEL REF comp_name >>> model_name ######################################################## # DATASETS # ######################################################## # amex: Some previous version of ambrosm """ has no nan so can train NN """ # amex2: V9: Better hyperparameters , ambrosm """ has nans so don't train NN """ fold3 3 =============> 1 : 305942 152971 2 : 305942 152971 3 : 305942 152971 fold5 5 =============> 1 : 367130 91783 2 : 367130 91783 3 : 367130 91783 4 : 367131 91782 5 : 367131 91782 fold10 10 =============> 1 : 413021 45892 2 : 413021 45892 3 : 413021 45892 4 : 413022 45891 5 : 413022 45891 6 : 413022 45891 7 : 413022 45891 8 : 413022 45891 9 : 413022 45891 10 : 413022 45891 fold20 20 =============> 1 : 435967 22946 2 : 435967 22946 3 : 435967 22946 4 : 435967 22946 5 : 435967 22946 6 : 435967 22946 7 : 435967 22946 8 : 435967 22946 9 : 435967 22946 10 : 435967 22946 11 : 435967 22946 12 : 435967 22946 13 : 435967 22946 14 : 435968 22945 15 : 435968 22945 16 : 435968 22945 17 : 435968 22945 18 : 435968 22945 19 : 435968 22945 20 : 435968 22945 # amex3: Devastator: train shape (458913, 2635), test shape (924621, 2634) #1815) fold3 3 =============> 1 : 305942 152971 2 : 305942 152971 3 : 305942 152971 fold5 5 =============> 1 : 367130 91783 2 : 367130 91783 3 : 367130 91783 4 : 367131 91782 5 : 367131 91782 fold10 10 =============> 1 : 413021 45892 2 : 413021 45892 3 : 413021 45892 4 : 413022 45891 5 : 413022 45891 6 : 413022 45891 7 : 413022 45891 8 : 413022 45891 9 : 413022 45891 10 : 413022 45891 fold20 20 =============> 1 : 435967 22946 2 : 435967 22946 3 : 435967 22946 4 : 435967 22946 5 : 435967 22946 6 : 435967 22946 7 : 435967 22946 8 : 435967 22946 9 : 435967 22946 10 : 435967 22946 11 : 435967 22946 12 : 435967 22946 13 : 435967 22946 14 : 435968 22945 15 : 435968 22945 16 : 435968 22945 17 : 435968 22945 18 : 435968 22945 19 : 435968 22945 20 : 435968 22945 # amex4: ragnar latest my_folds (458913, 3358), test (924621, 3353) # useful_features 3352 fold3 3 =============> 1 : 305942 152971 2 : 305942 152971 3 : 305942 152971 fold5 5 =============> 1 : 367130 91783 2 : 367130 91783 3 : 367130 91783 4 : 367131 91782 5 : 367131 91782 fold10 10 =============> 1 : 413021 45892 2 : 413021 45892 3 : 413021 45892 4 : 413022 45891 5 : 413022 45891 6 : 413022 45891 7 : 413022 45891 8 : 413022 45891 9 : 413022 45891 10 : 413022 45891 fold20 20 =============> 1 : 435967 22946 2 : 435967 22946 3 : 435967 22946 4 : 435967 22946 5 : 435967 22946 6 : 435967 22946 7 : 435967 22946 8 : 435967 22946 9 : 435967 22946 10 : 435967 22946 11 : 435967 22946 12 : 435967 22946 13 : 435967 22946 14 : 435968 22945 15 : 435968 22945 16 : 435968 22945 17 : 435968 22945 18 : 435968 22945 19 : 435968 22945 20 : 435968 22945

File no 20: /src-framework3/keys.py
from collections import defaultdict import pickle import os import sys """ generates keys and stores it in models-ultramnist """ class KeyMaker: def __init__( self, random_state=21, target_name="Survived", id_name="PassengerId", comp_type="2class", metrics_name="accuracy", fold_dict={'fold5':5}, data_type="image", # ["image", "tabular", "text"] ): # with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() self._comp_name = comp_name self.data_type = data_type self.metrics_list = [ "accuracy", "f1", "recall", "precision", "auc", "logloss", "auc_tf", "mae", "mse", "rmse", "msle", "rmsle", "r2", "amex_metric", ] self.data_list = ["tabular", "image", "text"] self.comp_list = ["regression", "2class", "multi_class", "multi_label"] self.random_state = random_state self.target_name = target_name self.id_name = id_name self.comp_type = comp_type self.metrics_name = metrics_name self.fold_dict = fold_dict self.locker = defaultdict() self.sanity_check() # --> sanity check self.update() # dumps files as pickel def sanity_check(self): if self.comp_type not in self.comp_list: raise Exception(f"{self.comp_type} not in the list {self.comp_list}") if self.metrics_name not in self.metrics_list: raise Exception(f"{self.metrics_name} not in the list {self.metrics_name}") if self.data_type not in self.data_list: raise Exception(f"{self.data_type} not in the list {self.data_type}") def help(self): print("comp_type:=> ", [comp for i, comp in enumerate(self.comp_list)]) print("metrics_index:=>", [mt for i, mt in enumerate(self.metrics_list)]) def __call__( self, random_state="--|--", target_name="--|--", id_name="--|--", comp_type="--|--", metrics_name="--|--", fold_dict="--|--", data_type="--|--", ): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() with open(f"../configs/configs-{comp_name}/locker.pkl", "rb") as f: a = pickle.load(f) self.random_state = a["random_state"] self.target_name = a["target_name"] self.id_name = a["id_name"] self.comp_type = a["comp_type"] self.metrics_name = a["metrics_name"] self.fold_dict = a["fold_dict"] self.data_type = a["data_type"] if random_state != "--|--": # updated self.random_state = random_state if target_name != "--|--": # updated self.target_name = target_name if id_name != "--|--": # updated self.id_name = id_name if comp_type != "--|--": self.comp_type = comp_type if metrics_name != "--|--": self.metrics_name = metrics_name if fold_dict != "--|--": self.fold_dict = fold_dict if data_type != "--|--": self.data_type = data_type self.sanity_check() self.update() # dump files to pickel def update(self): # updates the locker a = self.locker self.locker["comp_name"] = self._comp_name self.locker["random_state"] = self.random_state self.locker["target_name"] = self.target_name self.locker["id_name"] = self.id_name self.locker["comp_type"] = self.comp_type self.locker["fold_dict"] = self.fold_dict self.locker["data_type"] = self.data_type with open(f"../configs/configs-{a['comp_name']}/locker.pkl", "wb") as f: pickle.dump(self.locker, f) def show_stored_keys(self): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() with open(f"../configs/configs-{comp_name}/locker.pkl", "rb") as f: a = pickle.load(f) for k, v in a.items(): print(f"{k}:", v) def show_variables(self): print() for i, (k, v) in enumerate(self.__dict__.items()): print(f"{i}. {k} :=======>", v) print() if __name__ == "__main__": x = KeyMaker() # # ultramnist # x.id_name = "id" # x.target_name = "digit_sum" # x.comp_type = "multi_class" # x.no_folds = 5 # x.data_type = "image_path" ## mnist # x.id_name = "ImageId" # x.target_name = "Label" # x.comp_type = "multi_class" # x.no_folds = 5 # x.data_type = "image_df" # image_path, image_df, image_folder # twistmnist # x.id_name = "image_id" # x.target_name = "label" # x.comp_type = "multi_class" # x.no_folds = 5 # x.data_type = "image_df" # bengaliai # x.id_name = "image_id" # x.target_name = ['grapheme_root','vowel_diacritic','consonant_diacritic'] # x.comp_type = "multi_label" # x.no_folds = 5 # x.data_type = "image_path" # # tmay # x.id_name = "id" # x.target_name = "target" # x.comp_type = "2class" # x.no_folds = 5 # x.data_type = "tabular" # # # amex # x.id_name = "customer_ID" # x.target_name = "prediction" # x.comp_type = "2class" # x.fold_dict = { # no_folds : replace it # "fold3": 3, # "fold5": 5, # "fold10": 10, # "fold20": 20 # } # 10 # x.data_type = "tabular" # amex5 # x.id_name = "customer_ID" # x.target_name = "prediction" # x.comp_type = "2class" # x.no_folds = 5 # x.data_type = "tabular" # # getaroom # x.id_name = "Property_ID" # x.target_name = "Habitability_score" # x.comp_type = "regression" # x.fold_dict = { # no_folds : replace it # "fold3": 3, # "fold5": 5, # "fold10": 10, # "fold20": 20 # } # 10 # x.data_type = "tabular" x.id_name = "ID" x.target_name = "Time_taken" x.comp_type = "regression" x.fold_dict = { # no_folds : replace it "fold3": 3, "fold5": 5, "fold10": 10, "fold20": 20 } # 10 x.data_type = "tabular" x.update() # # x.show_stored_keys()

File no 21: /src-framework3/metrics.py
from sklearn import metrics as skmetrics import tensorflow as tf import numpy as np import pandas as pd # https://stackoverflow.com/questions/47152610/what-is-the-difference-between-xgb-train-and-xgb-xgbregressor-or-xgb-xgbclassif import xgboost as xgb # when calling the low level api """ Regression:=> use .predict() Classification:=> use.predict() except auc/log_loss auc/log_loss:= binary problem: (n_samples,) .predict_proba()[:,1] multiclass problem: (n_samples, n_classes) .predict_proba() true: [0,2,1,4,2] 1D array pred: [ [0.1, 0.7, 0.2], [0.2, 0.3, 0.5], ... ] """ def log_return(list_stock_prices): return np.log(list_stock_prices).diff() def realized_volatility(series_log_return): return np.sqrt(np.sum(series_log_return**2)) def rmspe(y_true, y_pred): return (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))) def feval_RMSPE(preds, train_data): labels = train_data.get_label() return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False def getaroom_metrics(y_true, y_pred): return max( 0, 100*(skmetrics.r2_score(y_true , y_pred))) def amzcomp1_metrics(y_true, y_pred): return max( 0, 100*(skmetrics.r2_score(y_true , y_pred))) class RegressionMetrics: def __init__(self): self.metrics = { "mae": self._mae, "mse": self._mse, "rmse": self._rmse, "msle": self._msle, "rmsle": self._rmsle, "r2": self._r2, } def __call__(self, metric, y_true, y_pred): if metric not in self.metrics: raise Exception(f"{metrics}: Metric not implemented") if metric == "mae": return self._mae(y_true=y_true, y_pred=y_pred) if metric == "mse": return self._mse(y_true=y_true, y_pred=y_pred) if metric == "rmse": return self._rmse(y_true=y_true, y_pred=y_pred) if metric == "msle": return self._msle(y_true=y_true, y_pred=y_pred) if metric == "rmsle": return self._rmsle(y_true=y_true, y_pred=y_pred) if metric == "r2": return self._r2(y_true=y_true, y_pred=y_pred) @staticmethod def _mae(y_true, y_pred): return skmetrics.mean_absolute_error(y_true=y_true, y_pred=y_pred) @staticmethod def _mse(y_true, y_pred): return skmetrics.mean_squared_error(y_true=y_true, y_pred=y_pred) def _rmse(self, y_true, y_pred): return np.sqrt(self._mse(y_true, y_pred)) @staticmethod def _msle(y_true, y_pred): return skmetrics.mean_squared_log_error(y_true=y_true, y_pred=y_pred) def _rmsle(self, y_true, y_pred): return np.sqrt(self._msle(y_true, y_pred)) @staticmethod def _r2(y_true, y_pred): return skmetrics.r2_score(y_true=y_true, y_pred=y_pred) class ClassificationMetrics: def __init__(self): self.metrics = { "accuracy": self._accuracy, "f1": self._f1, "recall": self._recall, "precision": self._precision, "auc": self._auc, "logloss": self._logloss, "auc_tf": self._auc_tf, "amex_metric": self.amex_metric, } # it allows to use an instance of this class as a function # a= Class..ics() then a("auc",y_true,y_pred) # y_pred is HARD CLASS 1,2,0,.. def __call__(self, metric, y_true, y_pred, y_proba=None): if metric not in self.metrics: raise Exception(f"{metric}: Metric not implemented") if metric == "auc": if y_proba is None: raise Exception(f"y_proba can't be None for {metric}") return self._auc(y_true=y_true, y_pred=y_proba) if metric == "logloss": if y_proba is None: raise Exception(f"y_proba can't be None for {metric}") return self._auc(y_true=y_true, y_pred=y_proba) if metric == "auc_tf": if y_proba is None: raise Exception(f"y_proba can't be None for {metric}") return self._auc_tf(y_true=y_true, y_pred=y_proba) if metric == "amex_metric": if y_proba is None: raise Exception(f"y_proba can't be None for {metric}") return self.amex_metric(y_true=y_true, y_pred=y_proba) else: return self.metrics[metric](y_true=y_true, y_pred=y_pred) @staticmethod def _accuracy(y_true, y_pred): return skmetrics.accuracy_score(y_true=y_true, y_pred=y_pred) @staticmethod def _f1(y_true, y_pred): return skmetrics.f1_score(y_true=y_true, y_pred=y_pred) @staticmethod def _recall(y_true, y_pred): return skmetrics.recall_score(y_true=y_true, y_pred=y_pred) @staticmethod def _precision(y_true, y_pred): return skmetrics.precision_score(y_true=y_true, y_pred=y_pred) @staticmethod def _auc(y_true, y_pred): # auc expects probability so we need y_proba return skmetrics.roc_auc_score(y_true=y_true, y_score=y_pred) @staticmethod def _logloss(y_true, y_pred): return skmetrics.log_loss(y_true=y_true, y_pred=y_pred) @staticmethod def _auc_tf(y_true, y_pred): # should have cuda enabled def fallback_auc(y_true, y_pred): try: return metrics.roc_auc_score(y_true, y_pred) except: return 0.5 return tf.py_function(fallback_auc, (y_true, y_pred), tf.double) @staticmethod def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float: def top_four_percent_captured( y_true: pd.DataFrame, y_pred: pd.DataFrame ) -> float: df = pd.concat([y_true, y_pred], axis="columns").sort_values( "prediction", ascending=False ) df["weight"] = df["target"].apply(lambda x: 20 if x == 0 else 1) four_pct_cutoff = int(0.04 * df["weight"].sum()) df["weight_cumsum"] = df["weight"].cumsum() df_cutoff = df.loc[df["weight_cumsum"] <= four_pct_cutoff] return (df_cutoff["target"] == 1).sum() / (df["target"] == 1).sum() def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float: df = pd.concat([y_true, y_pred], axis="columns").sort_values( "prediction", ascending=False ) df["weight"] = df["target"].apply(lambda x: 20 if x == 0 else 1) df["random"] = (df["weight"] / df["weight"].sum()).cumsum() total_pos = (df["target"] * df["weight"]).sum() df["cum_pos_found"] = (df["target"] * df["weight"]).cumsum() df["lorentz"] = df["cum_pos_found"] / total_pos df["gini"] = (df["lorentz"] - df["random"]) * df["weight"] return df["gini"].sum() def normalized_weighted_gini( y_true: pd.DataFrame, y_pred: pd.DataFrame ) -> float: y_true_pred = y_true.rename(columns={"target": "prediction"}) return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred) # sanity check y_true = pd.DataFrame(y_true, columns=["target"]) y_pred = pd.DataFrame(y_pred, columns=["prediction"]) # g = normalized_weighted_gini(y_true, y_pred) d = top_four_percent_captured(y_true, y_pred) return 0.5 * (g + d) # @yunchonggan's fast metric implementation # From https://www.kaggle.com/competitions/amex-default-prediction/discussion/328020 def amex_metric(y_true: np.array, y_pred: np.array) -> float: # count of positives and negatives n_pos = y_true.sum() n_neg = y_true.shape[0] - n_pos # sorting by descring prediction values indices = np.argsort(y_pred)[::-1] preds, target = y_pred[indices], y_true[indices] # filter the top 4% by cumulative row weights weight = 20.0 - target * 19.0 cum_norm_weight = (weight / weight.sum()).cumsum() four_pct_filter = cum_norm_weight <= 0.04 # default rate captured at 4% d = target[four_pct_filter].sum() / n_pos # weighted gini coefficient lorentz = (target / n_pos).cumsum() gini = ((lorentz - cum_norm_weight) * weight).sum() # max weighted gini coefficient gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg)) # normalized weighted gini coefficient g = gini / gini_max return 0.5 * (g + d) # ==================================================== # lgbmc amex metric # ==================================================== # custom callback: https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html def lgbmc_amex_metric(y_true, y_pred): # M1 # Classification # cl = ClassificationMetrics() # return ("amex", cl("amex_metric",y_true,"y_pred_dummy",y_pred), True) # M2 # https://www.kaggle.com/code/ambrosm/amex-lightgbm-quickstart """The competition metric with lightgbm's calling convention""" return ('amex', amex_metric(y_true, y_pred), True) # https://www.kaggle.com/kyakovlev # https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534 def amex_metric_mod(y_true, y_pred): labels = np.transpose(np.array([y_true, y_pred])) labels = labels[labels[:, 1].argsort()[::-1]] weights = np.where(labels[:,0]==0, 20, 1) cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))] top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0]) gini = [0,0] for i in [1,0]: labels = np.transpose(np.array([y_true, y_pred])) labels = labels[labels[:, i].argsort()[::-1]] weight = np.where(labels[:,0]==0, 20, 1) weight_random = np.cumsum(weight / np.sum(weight)) total_pos = np.sum(labels[:, 0] * weight) cum_pos_found = np.cumsum(labels[:, 0] * weight) lorentz = cum_pos_found / total_pos gini[i] = np.sum((lorentz - weight_random) * weight) return 0.5 * (gini[1]/gini[0] + top_four) # ==================================================== # XGBOOST amex metric # ==================================================== def xgboost_amex_metric_mod(predt: np.ndarray, dtrain: xgb.DMatrix): y = dtrain.get_label() return 'AMEXcustom', 1 - amex_metric_mod(y, predt) def xgboost_amex_metric_mod1(predt: np.ndarray, dtrain: xgb.DMatrix): y = dtrain.get_label() return 'AMEXcustom', 1 - amzcomp1_metrics(y, predt) # ==================================================== # Amex metric # ==================================================== def amex_metric_lgb_base(y_true, y_pred): labels = np.transpose(np.array([y_true, y_pred])) labels = labels[labels[:, 1].argsort()[::-1]] weights = np.where(labels[:,0]==0, 20, 1) cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))] top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0]) gini = [0,0] for i in [1,0]: labels = np.transpose(np.array([y_true, y_pred])) labels = labels[labels[:, i].argsort()[::-1]] weight = np.where(labels[:,0]==0, 20, 1) weight_random = np.cumsum(weight / np.sum(weight)) total_pos = np.sum(labels[:, 0] * weight) cum_pos_found = np.cumsum(labels[:, 0] * weight) lorentz = cum_pos_found / total_pos gini[i] = np.sum((lorentz - weight_random) * weight) return 0.5 * (gini[1]/gini[0] + top_four) # ==================================================== # LGB amex metric # ==================================================== # https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7963 def lgb_amex_metric(y_pred, y_true): y_true = y_true.get_label() return 'amex_metric', amex_metric_lgb_base(y_true, y_pred), True # ==================================================== # amex custom metrics for keras # ==================================================== from keras import backend as K import tensorflow as tf def amex_metric_tensorflow(y_true: tf.Tensor, y_pred: tf.Tensor) -> float: # convert dtypes to float64 y_true = tf.cast(y_true, dtype=tf.float64) y_pred = tf.cast(y_pred, dtype=tf.float64) # count of positives and negatives n_pos = tf.math.reduce_sum(y_true) n_neg = tf.cast(tf.shape(y_true)[0], dtype=tf.float64) - n_pos # sorting by descring prediction values indices = tf.argsort(y_pred, axis=0, direction='DESCENDING') preds, target = tf.gather(y_pred, indices), tf.gather(y_true, indices) # filter the top 4% by cumulative row weights weight = 20.0 - target * 19.0 cum_norm_weight = tf.cumsum(weight / tf.reduce_sum(weight)) four_pct_filter = cum_norm_weight <= 0.04 # default rate captured at 4% d = tf.reduce_sum(target[four_pct_filter]) / n_pos # weighted gini coefficient lorentz = tf.cumsum(target / n_pos) gini = tf.reduce_sum((lorentz - cum_norm_weight) * weight) # max weighted gini coefficient gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg)) # normalized weighted gini coefficient g = gini / gini_max return 0.5 * (g + d) ########################################################### # cbc custom metrics ############################################################## # https://stackoverflow.com/questions/65462220/how-to-create-custom-eval-metric-for-catboost # https://www.kaggle.com/code/thedevastator/ensemble-lightgbm-catboost-xgboost def amex_metric_cbc(y_true, y_pred): labels = np.transpose(np.array([y_true, y_pred])) labels = labels[labels[:, 1].argsort()[::-1]] weights = np.where(labels[:,0]==0, 20, 1) cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))] top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0]) gini = [0,0] for i in [1,0]: labels = np.transpose(np.array([y_true, y_pred])) labels = labels[labels[:, i].argsort()[::-1]] weight = np.where(labels[:,0]==0, 20, 1) weight_random = np.cumsum(weight / np.sum(weight)) total_pos = np.sum(labels[:, 0] * weight) cum_pos_found = np.cumsum(labels[:, 0] * weight) lorentz = cum_pos_found / total_pos gini[i] = np.sum((lorentz - weight_random) * weight) return 0.5 * (gini[1]/gini[0] + top_four) class CustomMetric_cbc(object): def get_final_error(self, error, weight): return error def is_max_optimal(self): return True def evaluate(self, approxes, target, weight): return amex_metric_cbc(np.array(target), approxes[0]), 1.0

File no 22: /src-framework3/model_dispatcher.py
import custom_models MODEL_DISPATCHER = {"resnet34": custom_models.ResNet34} if __name__ == "__main__": pass

File no 23: /src-framework3/optuna_search.py
from metrics import ClassificationMetrics from metrics import RegressionMetrics from metrics import * from collections import defaultdict import pickle import sys from sklearn.model_selection import KFold from sklearn.metrics import roc_auc_score, accuracy_score, f1_score from sklearn.metrics import roc_auc_score from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor, ExtraTreesClassifier from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor from scipy import stats import gc import psutil import seaborn as sns import tracemalloc import pathlib from settings import * """ ################ """ # import albumentations import numpy as np import pandas as pd # import timm import torch import torch.nn as nn from sklearn import metrics, model_selection from torch.utils.data import Dataset, DataLoader ######### #Tabnet from pytorch_tabnet.metrics import Metric from pytorch_tabnet.tab_model import TabNetRegressor, TabNetClassifier from pytorch_tabnet.pretraining import TabNetPretrainer ######################## # use it only when using tez2 i.e latest version # from tez import Tez, TezConfig # from tez.callbacks import EarlyStopping # from tez.utils import seed_everything # use this will pip install tez # from tez import Tez, TezConfig # from tez.callbacks import EarlyStopping # from tez.utils import seed_everything import global_variables """ """ sns.set() from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler from sklearn.preprocessing import RobustScaler from sklearn.model_selection import KFold, StratifiedKFold from sklearn.linear_model import LogisticRegression, LinearRegression from sklearn.metrics import roc_auc_score, accuracy_score from xgboost import XGBClassifier, XGBRegressor # https://stackoverflow.com/questions/47152610/what-is-the-difference-between-xgb-train-and-xgb-xgbregressor-or-xgb-xgbclassif import xgboost as xgb # when calling the low level api from sklearn.preprocessing import PowerTransformer from sklearn.preprocessing import PolynomialFeatures from optuna.integration import LightGBMPruningCallback # get skewed features to impute median instead of mean from scipy.stats import skew from sklearn.ensemble import AdaBoostClassifier from sklearn.tree import DecisionTreeClassifier from imblearn.over_sampling import SMOTE from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor from imblearn.pipeline import make_pipeline, Pipeline from sklearn import linear_model from sklearn.linear_model import Ridge, Lasso from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.decomposition import PCA from sklearn.metrics import roc_auc_score, f1_score from xgboost import XGBRegressor, XGBRFRegressor import itertools import optuna from lightgbm import LGBMClassifier, LGBMRegressor, log_evaluation import lightgbm as lgb from sklearn.neural_network import MLPClassifier, MLPRegressor from sklearn.ensemble import GradientBoostingClassifier from catboost import CatBoostRegressor, CatBoostClassifier # from sklearn.experimental import enable_hist_gradient_boosting from sklearn.ensemble import HistGradientBoostingClassifier # import the necessary packages from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.applications import VGG16 from tensorflow.keras.layers import AveragePooling2D from tensorflow.keras.layers import Dropout from tensorflow.keras.layers import Flatten from tensorflow.keras.layers import Dense from tensorflow.keras.layers import Input from tensorflow.keras.models import Model from tensorflow.keras.optimizers import Adam from tensorflow.keras.utils import to_categorical from sklearn.preprocessing import LabelBinarizer from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from imutils import paths import matplotlib.pyplot as plt import pandas as pd import numpy as np import random import shutil import cv2 import os from keras.utils import np_utils from sklearn.preprocessing import LabelEncoder from keras.utils.np_utils import to_categorical from sklearn.utils import shuffle import keras import tensorflow as tf from keras.models import Model from keras.layers import ( Dense, Dropout, LSTM, Input, Activation, concatenate, Bidirectional, ) from keras import optimizers from keras.models import Sequential from keras.layers import ( Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization, LSTM, ) from keras import regularizers from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax from keras.callbacks import ( EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TerminateOnNaN, LearningRateScheduler, ) import warnings # Filter up and down np.random.seed(1337) # for reproducibility warnings.filterwarnings("ignore") from metrics import ClassificationMetrics, RegressionMetrics # tez ---------------------------- import os import albumentations as A import pandas as pd import numpy as np import tez from tez.datasets import ImageDataset from tez.callbacks import EarlyStopping import torch import torch.nn as nn from torch.nn import functional as F from sklearn import metrics, model_selection, preprocessing import timm from sklearn.model_selection import KFold # ignoring warnings import warnings warnings.simplefilter("ignore") import os, cv2, json from PIL import Image import random import tez from tez.datasets import ImageDataset from tez.callbacks import EarlyStopping from custom_models import * from custom_classes import * from utils import * # ------------------------------ # keras image from tensorflow.keras.preprocessing.image import ImageDataGenerator # ------------- """ self._state = fold, opt, seed """ ##---------------- import torch from torch.optim import Adam, SGD from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts from torchcontrib.optim import SWA import torch.nn as nn from model_dispatcher import MODEL_DISPATCHER class OptunaOptimizer: def __init__( self, model_name="lgr", comp_type="2class", metrics_name="accuracy", n_trials=2, # 50, fold_name = "fold3", optimize_on=[0], prep_list=[], with_gpu=False, save_models=True, aug_type="aug2", _dataset="ImageDataset", use_cutmix=True, callbacks_list=[], ): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() self.comp_name = comp_name # put it here for the mkdir of lgb callback self.locker = load_pickle(f"../configs/configs-{comp_name}/locker.pkl") self.current_dict = load_pickle( f"../configs/configs-{self.locker['comp_name']}/current_dict.pkl" ) self.exp_no = self.current_dict["current_exp_no"] # set it as default and will be changed by predict.py and seed_it.py self.calculate_feature_importance = False # later integrate it (No need just keep saving that's all or maybe) self.calculate_permutation_feature_importance = False self.save_models = save_models self._trial_score = None self._history = None self.use_cutmix = use_cutmix self.callbacks_list = callbacks_list self.all_callbacks = [ "cosine_decay", "exponential_decay", "simple_decay", "ReduceLROnPlateau", "chk_pt", "terminate_on_NaN", "early_stopping", "myCallback1", ] self.comp_list = ["regression", "2class", "multi_class", "multi_label"] self.metrics_list = [ "accuracy", "f1", "recall", "precision", "auc", "logloss", "auc_tf", "mae", "mse", "rmse", "msle", "rmsle", "r2", "amex_metric", "amex_metric_mod" "amex_metric_lgb_base", "getaroom_metrics", "amzcomp1_metrics", ] self.model_list = [ "lgr", "lir", "xgb", "xgbc", "xgbr", "cbc", "cbr", "mlpc", "mlpr", "rg", "ls", "knnc", "dtc", "adbc", "gbmc", "gbmr", "hgbc", "lgb", "lgbmc", "lgbmr", "rfc", "rfr", "tabnetc", "tabnetr", "k1", "k2", "k3", "k4", "tez1", "tez2", "p1", "pretrained", ] self._prep_list = ["SiMe", "SiMd", "SiMo", "Mi", "Ro", "Sd", "Lg"] self.metric_list = ["amzcomp1_metrics","getaroom_metrics", "amex_metric","amex_metric_mod", "amex_metric_lgb_base","accuracy","f1","recall","precision", "auc", "logloss","auc_tf","mae","mse","rmse","msle","rmsle","r2"] self.prep_list = prep_list self.comp_type = comp_type self.metrics_name = metrics_name self.with_gpu = with_gpu self.aug_type = aug_type self._dataset = _dataset self._log_table = None # will track experiments self._state = "opt" # ["opt","fold", "seed"] # in start we want to find best params then we will loop if self.metrics_name in [ "accuracy", "f1", "recall", "precision", "auc", "auc_tf", "r2", "amex_metric", "amex_metric_mod", "amex_metric_lgb_base", "getaroom_metrics", "amzcomp1_metrics", ]: self._aim = "maximize" else: self._aim = "minimize" self.n_trials = n_trials self.best_params = None self.best_value = None self.model_name = model_name self.fold_name = fold_name self.optimize_on = optimize_on self.sanity_check() def show_variables(self): print() for i, (k, v) in enumerate(self.__dict__.items()): print(f"{i}. {k} :=======>", v) gc.collect() print() def sanity_check(self): if self.comp_type not in self.comp_list: raise Exception(f"{self.comp_type} not in the list {self.comp_list}") if self.metrics_name not in self.metrics_list: raise Exception(f"{self.metrics_name} not in the list {self.metrics_list}") if self.model_name not in self.model_list: raise Exception(f"{self.model_name} not in the list {self.model_list}") if self.fold_name not in list(self.locker['fold_dict'].keys()): raise Exception( f"{self.fold_name} not in {list(self.locker['fold_dict'].keys())}" ) for f in self.optimize_on: if f >= self.locker['fold_dict'][self.fold_name]: # self.locker["no_folds"]: raise Exception( f"{self.optimize_on} out of range {self.locker['fold_dict'][self.fold_name]}" ) for p in self.prep_list: if p not in list(self._prep_list): raise Exception(f"{p} is invalid preprocessing type!") gc.collect() for c in self.callbacks_list: if c not in self.all_callbacks: raise Exception(f"{c} is invalid callback type!") gc.collect() def help(self): print("comp_type:=> ", [comp for i, comp in enumerate(self.comp_list)]) print("metrics_name:=>", [mt for i, mt in enumerate(self.metrics_list)]) print() models = [ "LogisticRegression", "LinearRegression", "XGBClassifier", "XGBRegressor", ] print("model_name:=>") for a, b in list( zip(self.model_list, models) ): # ,[mt for i,mt in enumerate(self.model_list)]) print(f"{a}:=> {b}") gc.collect() print() preps = [ "SimpleImputer_mean", "SimpleImputer_median", "SimpleImputer_mode", "RobustScaler", "StandardScaler", "LogarithmicScaler", ] print("preprocess_names:=>") for a, b in list( zip(self._prep_list, preps) ): # ,[mt for i,mt in enumerate(self.model_list)]) print(f"{a}:=> {b}") print("preprocess_names:=>", [p for i, p in enumerate(self._prep_list)]) ## preprocess # Si: SimpleImputer SiMe SiMd SiMo # Mi: MinMaxScaler # Ro: RobustScaler # Sd: StandardScaler # Lg: Logarithmic Scaler def generate_random_no(self): if self._state == "opt": comp_random_state = self.locker["random_state"] total_no_folds = self.locker['fold_dict'][self.fold_name] #self.locker["no_folds"] # fold_on = self.optimize_on metric_no = self.metrics_list.index(self.metrics_name) comp_type_no = self.comp_list.index(self.comp_type) model_no = self.model_list.index(self.model_name) prep_no = 0 for p in self._prep_list: if p == "Lg": prep_no += 10 else: prep_no += self._prep_list.index(p) # round_on # level_on # seed = ( comp_random_state + total_no_folds * 2 + metric_no * 3 #+ self.optimize_on * 4 ) for i,o in enumerate(self.optimize_on): seed = + o * (4+i) # seed = ( # comp_random_state # + total_no_folds * 2 # + metric_no * 3 # + self.optimize_on * 4 # ) seed += int( comp_type_no * 5 + model_no * 6 + prep_no * 7 + self.current_dict["current_level"] ) # + round_on * 4 + level_on * 5 if self.callbacks_list != []: for c in self.callbacks_list: seed += self.all_callbacks.index(c) seed = int(seed) else: seed = int(self._random_state) # it should be int type os.environ["PYTHONHASHSEED"] = str(seed) np.random.seed(seed) random.seed(seed) tf.random.set_seed( seed ) # f"The truth value of a {type(self).__name__} is ambiguous. " return seed # np.random.randint(3, 1000) # it should return 5 def get_params(self, trial): model_name = self.model_name if model_name == "lgr": params = { "class_weight": trial.suggest_categorical( "class_weight", [ "balanced", None, { 1: 1, 0: ( sum(list(self.ytrain == 0)) / sum(list(self.ytrain == 1)) ), }, ], ), "penalty": trial.suggest_categorical( "penalty", ["l2"] ), # ['l1','l2']), "C": trial.suggest_float("c", 0.01, 1000), } return params if model_name == "lir": params = { "max_depth": trial.suggest_int("max_depth", 2, 15), "subsample": trial.suggest_discrete_uniform( "subsample", 0.6, 1.0, 0.05 ), "n_estimators": trial.suggest_int("n_estimators", 1000, 10000, 100), "learning_rate": trial.suggest_discrete_uniform("learning_rate", 0.01, 0.1, 0.01), "reg_alpha": trial.suggest_int("reg_alpha", 1, 50), "reg_lambda": trial.suggest_int("reg_lambda", 5, 100), "min_child_weight": trial.suggest_int("min_child_weight", 2, 20), "colsample_bytree": trial.suggest_float("colsample_bytree", 0.1, 1.0), } return params if model_name == "xgb": # for regression # https://xgboost.readthedocs.io/en/stable/python/python_intro.html#setting-parameters # https://www.kaggle.com/code/sudalairajkumar/xgb-starter-in-python # https://www.kaggle.com/general/197091 params = { "objective": trial.suggest_categorical("objective", ['reg:squarederror']), #"eta": trial.suggest_categorical("eta", ['train']), "learning_rate": trial.suggest_float("learning_rate", 0,0.2), "max_depth": trial.suggest_categorical("max_depth", [6]), "silent": trial.suggest_categorical("silent", [1]), #"num_class": trial.suggest_categorical("num_class", [3]), "eval_metric": trial.suggest_categorical("eval_metric", ['rmse']), "min_child_weight": trial.suggest_categorical("min_child_weight", [1]), "subsample": trial.suggest_categorical("subsample", [0.7]), "colsample_bytree": trial.suggest_categorical("colsample_bytree", [0.7]), "booster": trial.suggest_categorical("booster", ['gbtree']), } # diff b/w .train() and .fit() # https://stackoverflow.com/questions/47152610/what-is-the-difference-between-xgb-train-and-xgb-xgbregressor-or-xgb-xgbclassif # .fit() is a wrapper over .train() # use evaluation metrics # https://stackoverflow.com/questions/60231559/how-to-set-eval-metrics-for-xgboost-train # xgb_parms = { # 'max_depth':4, # 'learning_rate':0.05, # 'subsample':0.8, # 'colsample_bytree':0.6, # 'eval_metric':'logloss', # 'objective':'binary:logistic', # 'tree_method':'gpu_hist', # 'predictor':'gpu_predictor', # 'random_state':SEED # } ## https://www.kaggle.com/code/karandora/xgboost-optuna Implement Features also from this notebook # param = { # 'booster':'gbtree', # 'tree_method':'gpu_hist', # "objective": "binary:logistic", # 'lambda': trial.suggest_loguniform( # 'lambda', 1e-3, 10.0 # ), # 'alpha': trial.suggest_loguniform( # 'alpha', 1e-3, 10.0 # ), # 'colsample_bytree': trial.suggest_float( # 'colsample_bytree', 0.5,1,step=0.1 # ), # 'subsample': trial.suggest_float( # 'subsample', 0.5,1,step=0.1 # ), # 'learning_rate': trial.suggest_float( # 'learning_rate', 0.001,0.05,step=0.001 # ), # 'n_estimators': trial.suggest_int( # "n_estimators", 80,1000,10 # ), # 'max_depth': trial.suggest_int( # 'max_depth', 2,10,1 # ), # 'random_state': 99, # 'min_child_weight': trial.suggest_int( # 'min_child_weight', 1,256,1 # ), # } # # https://www.kaggle.com/code/sietseschrder/xgboost-starter-0-793 # params = { # "max_depth": trial.suggest_int("max_depth", 2, 10), # 6 --> 10 # "subsample": trial.suggest_discrete_uniform( # "subsample", 0.6, 1.0, 0.05 # ), # #"n_estimators": trial.suggest_int("n_estimators", 10, 100, 10), # "learning_rate": trial.suggest_discrete_uniform("learning_rate", 0.001, 0.1, 0.005), # "eta" # #"reg_alpha": trial.suggest_int("reg_alpha", 1, 10), # #"reg_lambda": trial.suggest_int("reg_lambda", 5, 20), # #"min_child_weight": trial.suggest_int("min_child_weight", 2, 20), # "colsample_bytree": trial.suggest_float("colsample_bytree", 0.1, 1.0), # "objective": trial.suggest_categorical( # "objective", ["binary:logistic"] # ) # } # big file # params = { # "max_depth": trial.suggest_int("max_depth", 2, 4), # "subsample": trial.suggest_discrete_uniform( # "subsample", 0.6, 1.0, 0.05 # ), # "n_estimators": trial.suggest_int("n_estimators", 10, 100, 10), # "eta": trial.suggest_discrete_uniform("eta", 0.01, 0.1, 0.01), # "reg_alpha": trial.suggest_int("reg_alpha", 1, 10), # "reg_lambda": trial.suggest_int("reg_lambda", 5, 20), # "min_child_weight": trial.suggest_int("min_child_weight", 2, 20), # "colsample_bytree": trial.suggest_float("colsample_bytree", 0.1, 1.0), # } # --> base Good for small dataset # params = { # "max_depth": trial.suggest_int("max_depth", 2, 15), # "subsample": trial.suggest_discrete_uniform( # "subsample", 0.6, 1.0, 0.05 # ), # "n_estimators": trial.suggest_int("n_estimators", 1000, 10000, 100), # "eta": trial.suggest_discrete_uniform("eta", 0.01, 0.1, 0.01), # "reg_alpha": trial.suggest_int("reg_alpha", 1, 50), # "reg_lambda": trial.suggest_int("reg_lambda", 5, 100), # "min_child_weight": trial.suggest_int("min_child_weight", 2, 20), # "colsample_bytree": trial.suggest_float("colsample_bytree", 0.1, 1.0), # } if self.with_gpu == True: params.update( { "tree_method": trial.suggest_categorical( "tree_method", ["gpu_hist"] ), "gpu_id": trial.suggest_categorical("gpu_id", [0]), "predictor": trial.suggest_categorical( "predictor", ["gpu_predictor"] ), } ) return params if model_name == "xgbc": # diff b/w .train() and .fit() # https://stackoverflow.com/questions/47152610/what-is-the-difference-between-xgb-train-and-xgb-xgbregressor-or-xgb-xgbclassif # .fit() is a wrapper over .train() # use evaluation metrics # https://stackoverflow.com/questions/60231559/how-to-set-eval-metrics-for-xgboost-train # xgb_parms = { # 'max_depth':4, # 'learning_rate':0.05, # 'subsample':0.8, # 'colsample_bytree':0.6, # 'eval_metric':'logloss', # 'objective':'binary:logistic', # 'tree_method':'gpu_hist', # 'predictor':'gpu_predictor', # 'random_state':SEED # } ## https://www.kaggle.com/code/karandora/xgboost-optuna Implement Features also from this notebook # param = { # 'booster':'gbtree', # 'tree_method':'gpu_hist', # "objective": "binary:logistic", # 'lambda': trial.suggest_loguniform( # 'lambda', 1e-3, 10.0 # ), # 'alpha': trial.suggest_loguniform( # 'alpha', 1e-3, 10.0 # ), # 'colsample_bytree': trial.suggest_float( # 'colsample_bytree', 0.5,1,step=0.1 # ), # 'subsample': trial.suggest_float( # 'subsample', 0.5,1,step=0.1 # ), # 'learning_rate': trial.suggest_float( # 'learning_rate', 0.001,0.05,step=0.001 # ), # 'n_estimators': trial.suggest_int( # "n_estimators", 80,1000,10 # ), # 'max_depth': trial.suggest_int( # 'max_depth', 2,10,1 # ), # 'random_state': 99, # 'min_child_weight': trial.suggest_int( # 'min_child_weight', 1,256,1 # ), # } # # https://www.kaggle.com/code/sietseschrder/xgboost-starter-0-793 # params = { # "max_depth": trial.suggest_int("max_depth", 2, 6), # "subsample": trial.suggest_discrete_uniform( # "subsample", 0.6, 1.0, 0.05 # ), # #"n_estimators": trial.suggest_int("n_estimators", 10, 100, 10), # "learning_rate": trial.suggest_discrete_uniform("learning_rate", 0.01, 0.1, 0.01), # #"reg_alpha": trial.suggest_int("reg_alpha", 1, 10), # #"reg_lambda": trial.suggest_int("reg_lambda", 5, 20), # #"min_child_weight": trial.suggest_int("min_child_weight", 2, 20), # "colsample_bytree": trial.suggest_float("colsample_bytree", 0.1, 1.0), # "objective": trial.suggest_categorical( # "objective", ["binary:logistic"] # ), # # booster implemented from below discussion chris's comment # # https://www.kaggle.com/competitions/amex-default-prediction/discussion/333953 # # Which booster to use. Can be gbtree, gblinear or dart; gbtree and dart use tree based models while gblinear uses linear functions. # #"booster": trial.suggest_categorical("booster", ["dart", "gbtree", "gblinear"]) # "booster": trial.suggest_categorical("booster", [ "gbtree", "gblinear"]) # } # big file # params = { # "max_depth": trial.suggest_int("max_depth", 2, 4), # "subsample": trial.suggest_discrete_uniform( # "subsample", 0.6, 1.0, 0.05 # ), # "n_estimators": trial.suggest_int("n_estimators", 10, 100, 10), # "eta": trial.suggest_discrete_uniform("eta", 0.01, 0.1, 0.01), # "reg_alpha": trial.suggest_int("reg_alpha", 1, 10), # "reg_lambda": trial.suggest_int("reg_lambda", 5, 20), # "min_child_weight": trial.suggest_int("min_child_weight", 2, 20), # "colsample_bytree": trial.suggest_float("colsample_bytree", 0.1, 1.0), # } params = { "max_depth": trial.suggest_int("max_depth", 2, 15), "subsample": trial.suggest_discrete_uniform( "subsample", 0.6, 1.0, 0.05 ), "n_estimators": trial.suggest_int("n_estimators", 1000, 10000, 100), "learning_rate": trial.suggest_discrete_uniform("learning_rate", 0.01, 0.1, 0.01), "reg_alpha": trial.suggest_int("reg_alpha", 1, 50), "reg_lambda": trial.suggest_int("reg_lambda", 5, 100), "min_child_weight": trial.suggest_int("min_child_weight", 2, 20), "colsample_bytree": trial.suggest_float("colsample_bytree", 0.1, 1.0), "objective": trial.suggest_categorical( "objective", ["binary:logistic"] ), # booster implemented from below discussion chris's comment # https://www.kaggle.com/competitions/amex-default-prediction/discussion/333953 # Which booster to use. Can be gbtree, gblinear or dart; gbtree and dart use tree based models while gblinear uses linear functions. #"booster": trial.suggest_categorical("booster", ["dart", "gbtree", "gblinear"]) "booster": trial.suggest_categorical("booster", ["dart", "gbtree", "gblinear"]) } # --> base Good for small dataset # params = { # "max_depth": trial.suggest_int("max_depth", 2, 15), # "subsample": trial.suggest_discrete_uniform( # "subsample", 0.6, 1.0, 0.05 # ), # "n_estimators": trial.suggest_int("n_estimators", 1000, 10000, 100), # "eta": trial.suggest_discrete_uniform("eta", 0.01, 0.1, 0.01), # "reg_alpha": trial.suggest_int("reg_alpha", 1, 50), # "reg_lambda": trial.suggest_int("reg_lambda", 5, 100), # "min_child_weight": trial.suggest_int("min_child_weight", 2, 20), # "colsample_bytree": trial.suggest_float("colsample_bytree", 0.1, 1.0), # } if self.with_gpu == True: params.update( { "tree_method": trial.suggest_categorical( "tree_method", ["gpu_hist"] ), "gpu_id": trial.suggest_categorical("gpu_id", [0]), "predictor": trial.suggest_categorical( "predictor", ["gpu_predictor"] ), } ) return params if model_name == "xgbr": params = { "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.5), "max_depth": trial.suggest_int("max_depth", 3,20), #[3, 5, 7, 10] "min_child_weight": trial.suggest_categorical( "min_child_weight", [1, 3, 5] ), "subsample": trial.suggest_float("subsample", 0.01, 0.5), "n_estimators": trial.suggest_categorical( "n_estimators", [100, 200, 300, 400, 500, 1000, 1200, 1500] ), "objective": trial.suggest_categorical( "objective", ["reg:squarederror"] ), } if self.with_gpu == True: params.update( { "tree_method": trial.suggest_categorical( "tree_method", ["gpu_hist"] ), "gpu_id": trial.suggest_categorical("gpu_id", [0]), "predictor": trial.suggest_categorical( "predictor", ["gpu_predictor"] ), } ) return params if model_name == "cbc": # CatBoostClassifier(iterations=1000, random_state=22, nan_mode='Min') # https://www.kaggle.com/code/bhavikardeshna/catboost-gradient-boosting-ensemble-learning/notebook # params = { # "iterations": trial.suggest_int("iterations", 300, 1200), # 'nan_mode' : trial.suggest_categorical('nan_mode', ["Min"]) # } # main params = { "iterations": trial.suggest_int("iterations", 300, 1200), "objective": trial.suggest_categorical( "objective", ["Logloss", "CrossEntropy"] ), "bootstrap_type": trial.suggest_categorical( "bootstrap_type", ["Bernoulli"] #["Bayesian", "Bernoulli", "MVS"] ), # 'MVS' for CPU and only can be used in CPU. For GPU it is 'Bernoulli' # https://www.kaggle.com/competitions/amex-default-prediction/discussion/328606#1809347 "od_wait": trial.suggest_int("od_wait", 500, 2000), "learning_rate": trial.suggest_uniform("learning_rate", 0.02, 1), "reg_lambda": trial.suggest_uniform("reg_lambda", 1e-5, 100), "random_strength": trial.suggest_uniform("random_strength", 10, 50), "depth": trial.suggest_int("depth", 1, 15), "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 1, 30), "leaf_estimation_iterations": trial.suggest_int( "leaf_estimation_iterations", 1, 15 ), "verbose": False, } if self.with_gpu == True: params.update( { "task_type": trial.suggest_categorical("task_type", ["GPU"]), "devices": trial.suggest_categorical("devices", ["0"]), } ) return params if model_name == "cbr": # main params = { "iterations": trial.suggest_int("iterations", 300, 1000), "objective": trial.suggest_categorical( "objective", ["RMSE"] #, "MultiRMSE", "SurvivalAft", "MAE", "Quantile", "LogLinQuantile", "Poisson", "MAPE", "Lq"] ), "bootstrap_type": trial.suggest_categorical( "bootstrap_type", ["Bernoulli"] #["Bayesian", "Bernoulli", "MVS"] ), # 'MVS' for CPU and only can be used in CPU. For GPU it is 'Bernoulli' # https://www.kaggle.com/competitions/amex-default-prediction/discussion/328606#1809347 "od_wait": trial.suggest_int("od_wait", 500, 600), #900), # 2000 "learning_rate": trial.suggest_uniform("learning_rate", 0.02, 1), "reg_lambda": trial.suggest_uniform("reg_lambda", 1e-5, 30), #50), "random_strength": trial.suggest_uniform("random_strength", 10, 30), "depth": trial.suggest_int("depth", 1, 10), #15), "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 1, 20), "leaf_estimation_iterations": trial.suggest_int( "leaf_estimation_iterations", 1, 15 ), "verbose": False, } if self.with_gpu == True: params.update( { "task_type": trial.suggest_categorical("task_type", ["GPU"]), "devices": trial.suggest_categorical("devices", ["0"]), } ) return params if model_name == "mlpc": params = { "learning_rate": trial.suggest_categorical( "learning_rate", ["constant", "invscaling", "adaptive"] ), "hidden_layer_sizes": trial.suggest_categorical( "hidden_layer_sizes", [(5, 10, 5), (20, 10), (10, 20), (50, 50), (100, 100)], ), "alpha": trial.suggest_categorical( "alpha", [0.3, 0.1, 0.01, 0.001, 0.0001] ), "activation": trial.suggest_categorical( "activation", ["logistic", "relu", "tanh"] ), # "solver": trial.suggest_categorical("solver",['lbfgs']) } return params if model_name == "mlpr": params = { "learning_rate": trial.suggest_categorical( "learning_rate", ["constant", "invscaling", "adaptive"] ), "hidden_layer_sizes": trial.suggest_categorical( "hidden_layer_sizes", [(100, 100), (200,100,80),(800,40)], ), "alpha": trial.suggest_categorical( "alpha", [0.3, 0.1, 0.01, 0.001, 0.0001] ), "activation": trial.suggest_categorical( "activation", ["relu", "tanh", "identity"] ), # "solver": trial.suggest_categorical("solver",['lbfgs']) } # params = { # "learning_rate": trial.suggest_categorical( # "learning_rate", ["constant", "invscaling", "adaptive"] # ), # "hidden_layer_sizes": trial.suggest_categorical( # "hidden_layer_sizes", # [(5, 10, 5), (20, 10), (10, 20), (50, 50), (100, 100)], # ), # "alpha": trial.suggest_categorical( # "alpha", [0.3, 0.1, 0.01, 0.001, 0.0001] # ), # "activation": trial.suggest_categorical( # "activation", ["relu", "tanh", "identity"] # ), # # "solver": trial.suggest_categorical("solver",['lbfgs']) # } return params if model_name == "rg": params = { "alpha": trial.suggest_categorical( "alpha", list(np.linspace(1, 100, 100)) ), "solver": trial.suggest_categorical( "solver", ["auto", "svd", "cholesky", "lsqr", "sparse_cg", "sag", "saga"], ), "fit_intercept": trial.suggest_categorical("fit_intercept", [True]), } return params if model_name == "ls": params = { "alpha": trial.suggest_categorical( "alpha", [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] ), "selection": trial.suggest_categorical( "selection", ["cyclic", "random"] ), "fit_intercept": trial.suggest_categorical("fit_intercept", [True]), } return params if model_name == "knnc": params = { "leaf_size": trial.suggest_categorical( "leaf_size", [5, 10, 15, 20, 25, 30, 35, 40, 45] ), "n_neighbors": trial.suggest_categorical( "n_neighbors", [3, 4, 5, 6, 7, 8, 9, 10] ), "algorithm": trial.suggest_categorical( "algorithm", ["auto", "ball_tree", "kd_tree", "brute"] ), "weights": trial.suggest_categorical( "weights", ["uniform", "distance"] ), } return params if model_name == "dtc": params = { "class_weight": trial.suggest_categorical( "class_weight", [ "balanced", None, {1: 1, 0: (sum(list(ytrain == 0)) / sum(list(ytrain == 1)))}, ], ), "criterion": trial.suggest_categorical( "criterion", ["entropy", "gini"] ), "max_depth": trial.suggest_categorical("max_depth", [None, 5, 20, 70]), "min_samples_leaf": trial.suggest_categorical( "min_samples_leaf", [5, 10, 15, 20, 25] ), "min_samples_split": trial.suggest_categorical( "min_samples_split", [2, 10, 20] ), } return params if model_name == "adbc": params = { # "device_type": trial.suggest_categorical("device_type", ['gpu']), "n_estimators": trial.suggest_categorical( "n_estimators", [10, 100, 200, 500] ), # ,1000,10000 "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3), "algorithm": trial.suggest_categorical( "algorithm", ["SAMME", "SAMME.R"] ), } return params if model_name == "gbmc": params = { # "device_type": trial.suggest_categorical("device_type", ['gpu']), "n_estimators": trial.suggest_categorical( "n_estimators", [10, 100, 200, 500] ), # ,1000,10000 "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3), "max_depth": trial.suggest_int("max_depth", 3, 12), "loss": trial.suggest_categorical("loss", ["deviance", "exponential"]), "criterion": trial.suggest_categorical( "criterion", ["friedman_mse", "mse", "mae"] ), "max_features": trial.suggest_categorical( "max_features", ["auto", "sqrt", "log2"] ), "min_samples_split": trial.suggest_float("min_samples_split", 0.1, 0.5), "min_samples_leaf": trial.suggest_float("min_samples_split", 0.1, 0.5), "subsample": trial.suggest_categorical( "subsample", [0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0] ), } return params if model_name == "gbmr": params = { # "device_type": trial.suggest_categorical("device_type", ['gpu']), "n_estimators": trial.suggest_categorical( "n_estimators", [10, 100, 200, 500] ), # ,1000,10000 "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3), "max_depth": trial.suggest_int("max_depth", 3, 12), "loss": trial.suggest_categorical("loss", ['squared_error']), #, 'absolute_error', 'huber', 'quantile']), "criterion": trial.suggest_categorical( "criterion", ["friedman_mse", "squared_error", "mse"] ), "max_features": trial.suggest_categorical( "max_features", ["auto", "sqrt", "log2"] ), "min_samples_split": trial.suggest_float("min_samples_split", 0.1, 0.5), "min_samples_leaf": trial.suggest_float("min_samples_split", 0.1, 0.5), "subsample": trial.suggest_categorical( "subsample", [0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0] ), } return params if model_name == "hgbc": params = { "l2_regularization": trial.suggest_loguniform( "l2_regularization", 1e-10, 10.0 ), "early_stopping": trial.suggest_categorical( "early_stopping", ["False"] ), "learning_rate": trial.suggest_loguniform("learning_rate", 0.001, 0.1), "max_iter": trial.suggest_categorical( "max_iter", [i for i in range(1000, 12000, 100)] ), "max_depth": trial.suggest_int("max_depth", 2, 30), "max_bins": trial.suggest_int("max_bins", 100, 255), "min_samples_leaf": trial.suggest_int("min_samples_leaf", 20, 100000), "max_leaf_nodes": trial.suggest_int("max_leaf_nodes", 20, 80), } return params if model_name == "xgbdd": params = { "objective": trial.suggest_categorical("objective", ['regression']), #"eta": trial.suggest_categorical("eta", ['train']), "learning_rate": trial.suggest_float("learning_rate", 0,0.2), "max_depth": trial.suggest_categorical("max_depth", [6]), "silent": trial.suggest_categorical("silent", [1]), "num_class": trial.suggest_categorical("num_class", [3]), "eval_metric": trial.suggest_categorical("eval_metric", ['rmse']), "min_child_weight": trial.suggest_categorical("min_child_weight", [1]), "subsample": trial.suggest_categorical("subsample", [0.7]), "colsample_bytree": trial.suggest_categorical("colsample_bytree", [0.7]), } # param = {} # param['objective'] = 'multi:softprob' # param['eta'] = 0.1 # param['max_depth'] = 6 # param['silent'] = 1 # param['num_class'] = 3 # param['eval_metric'] = "mlogloss" # param['min_child_weight'] = 1 # param['subsample'] = 0.7 # param['colsample_bytree'] = 0.7 # param['seed'] = seed_val return params if model_name == "lgb": # dart: https://www.kaggle.com/code/ragnar123/amex-lgbm-dart-cv-0-7963 # params = { # 'objective': 'binary', # 'metric': "binary_logloss", # 'boosting': 'dart', # 'seed': CFG.seed, # 'num_leaves': 100, # 'learning_rate': 0.01, # 'feature_fraction': 0.20, # 'bagging_freq': 10, # 'bagging_fraction': 0.50, # 'n_jobs': -1, # 'lambda_l2': 2, # 'min_data_in_leaf': 40 # } """ params = { "objective": trial.suggest_categorical("objective", ["binary"]), "metric": trial.suggest_categorical("metric", ["binary_log_loss"]), "boosting": trial.suggest_categorical("boosting", ["dart"]), "num_leaves": trial.suggest_int("num_leaves", 80, 100, step=5), "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3), "feature_fraction": trial.suggest_float("feature_fraction", 0.2, 0.95, step=0.1), "bagging_freq": trial.suggest_categorical("bagging_freq", [1,10, 20]), "feature_fraction": trial.suggest_float("feature_fraction", 0.2, 0.95, step=0.1), "lambda_l2": trial.suggest_int("lambda_l2", 0, 10, step=2), "min_data_in_leaf": trial.suggest_int( "min_data_in_leaf", 20, 100, step=10), } """ # https://www.kaggle.com/competitions/amex-default-prediction/discussion/332575 # params = { # 'objective': 'binary', # 'metric': "amex_metric", # } # set metric as same as we set in feval: "amex_metric" # params = { # "objective": trial.suggest_categorical("objective", ["binary"]), # "metric": trial.suggest_categorical("metric", ["amex_metric"]), # "boosting": trial.suggest_categorical("boosting", ["dart"]), # "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3) # } """ params_lgbm = { 'task': 'train', 'boosting_type': 'gbdt', 'learning_rate': 0.01, 'objective': 'regression', 'metric': 'None', 'max_depth': -1, 'n_jobs': -1, 'feature_fraction': 0.7, 'bagging_fraction': 0.7, 'lambda_l2': 1, 'verbose': -1 #'bagging_freq': 5 } """ params = { "task": trial.suggest_categorical("task", ['train']), "boosting_type": trial.suggest_categorical("boosting_type", ['gbdt', 'dart']), "learning_rate": trial.suggest_float("learning_rate", 0,0.2), "objective": trial.suggest_categorical("objective", ['regression']), "metric": trial.suggest_categorical("metric", ['None']), "max_depth": trial.suggest_categorical("max_depth", [-1]), "n_jobs": trial.suggest_categorical("n_jobs", [-1]), #"feature_fraction": trial.suggest_categorical("feature_fraction", [0.7]), "feature_fraction": trial.suggest_categorical("feature_fraction", [0.7]), #"bagging_fraction": trial.suggest_categorical("bagging_fraction", [0.7]), "bagging_fraction": trial.suggest_categorical("bagging_fraction", [0.7]), "lambda_l2": trial.suggest_categorical("lambda_l2", [1]), "verbose": trial.suggest_categorical("verbose", [-1]), #"bagging_freq": trial.suggest_categorical("bagging_freq", [5]), # "objective": trial.suggest_categorical("objective", ["regression"]),#["binary"]), # "metric": trial.suggest_categorical("metric", [None]),#["binary_logloss"]), # "boosting": trial.suggest_categorical("boosting", ["dart"]), # "learning_rate": trial.suggest_float("learning_rate", 0.009, 0.011), # "seed": trial.suggest_categorical("seed", [241]), # "num_leaves": trial.suggest_categorical("num_leaves", [95,101,105]), # "feature_fraction": trial.suggest_float("feature_fraction", 0.15,0.22), # "bagging_freq": trial.suggest_categorical("bagging_freq" , [9,11,12]), # "bagging_fraction": trial.suggest_float("bagging_fraction", 0.45,0.52), # "n_jobs": trial.suggest_categorical("n_jobs" , [-1]), # "lambda_l2": trial.suggest_categorical("lambda_l2" , [1,2,3]), # "min_data_in_leaf": trial.suggest_categorical("min_data_in_leaf" , [35,41,45]), } # params = { # "objective": trial.suggest_categorical("objective", ["regression"]),#["binary"]), # "metric": trial.suggest_categorical("metric", [None]),#["binary_logloss"]), # "boosting": trial.suggest_categorical("boosting", ["dart"]), # "learning_rate": trial.suggest_float("learning_rate", 0.009, 0.011), # "seed": trial.suggest_categorical("seed", [241]), # "num_leaves": trial.suggest_categorical("num_leaves", [95,101,105]), # "feature_fraction": trial.suggest_float("feature_fraction", 0.15,0.22), # "bagging_freq": trial.suggest_categorical("bagging_freq" , [9,11,12]), # "bagging_fraction": trial.suggest_float("bagging_fraction", 0.45,0.52), # "n_jobs": trial.suggest_categorical("n_jobs" , [-1]), # "lambda_l2": trial.suggest_categorical("lambda_l2" , [1,2,3]), # "min_data_in_leaf": trial.suggest_categorical("min_data_in_leaf" , [35,41,45]), # } if self.with_gpu == True: params.update( { "device_type": trial.suggest_categorical( "device_type", ["gpu"] ), } ) return params if model_name == "lgbmc": # amex amrosm # link: https://www.kaggle.com/code/ambrosm/amex-lightgbm-quickstart params = { "n_estimators": trial.suggest_categorical( "n_estimators", [i for i in range(1100, 1300, 100)] ), # 10000 "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3), "num_leaves": trial.suggest_int("num_leaves", 80, 100, step=5), "colsample_bytree": trial.suggest_float("colsample_bytree", 0.1,0.2), "max_bins": trial.suggest_int("max_bins", 500,520, step=5), # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html "boosting_type": trial.suggest_categorical("boosting_type", ['dart']) #"max_depth": trial.suggest_int("max_depth", 3, 12), # "min_data_in_leaf": trial.suggest_int( # "min_data_in_leaf", 200, 10000, step=100 # ), #"lambda_l1": trial.suggest_int("lambda_l1", 0, 100, step=5), #"lambda_l2": trial.suggest_int("lambda_l2", 0, 100, step=5), #"min_gain_to_split": trial.suggest_float("min_gain_to_split", 0, 15), # "bagging_fraction": trial.suggest_float( # "bagging_fraction"lgbmc, 0.2, 0.95, step=0.1 # ), #"bagging_freq": trial.suggest_categorical("bagging_freq", [1]), # "feature_fraction": trial.suggest_float( # "feature_fraction", 0.2, 0.95, step=0.1 # ), } # main # params = { # "n_estimators": trial.suggest_categorical( # "n_estimators", [i for i in range(1000, 10000, 100)] # ), # 10000 # "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3), # "num_leaves": trial.suggest_int("num_leaves", 20, 3000, step=20), # "max_depth": trial.suggest_int("max_depth", 3, 12), # "min_data_in_leaf": trial.suggest_int( # "min_data_in_leaf", 200, 10000, step=100 # ), # "lambda_l1": trial.suggest_int("lambda_l1", 0, 100, step=5), # "lambda_l2": trial.suggest_int("lambda_l2", 0, 100, step=5), # "min_gain_to_split": trial.suggest_float("min_gain_to_split", 0, 15), # "bagging_fraction": trial.suggest_float( # "bagging_fraction", 0.2, 0.95, step=0.1 # ), # "bagging_freq": trial.suggest_categorical("bagging_freq", [1]), # "feature_fraction": trial.suggest_float( # "feature_fraction", 0.2, 0.95, step=0.1 # ), # } ## gpu build if self.with_gpu == True: params.update( { "device_type": trial.suggest_categorical( "device_type", ["gpu"] ), } ) return params if model_name == "lgbmr": params = { "n_estimators": trial.suggest_categorical("n_estimators", [10000]), "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3), "num_leaves": trial.suggest_int("num_leaves", 20, 3000, step=20), "max_depth": trial.suggest_int("max_depth", 3, 12), "min_data_in_leaf": trial.suggest_int( "min_data_in_leaf", 200, 10000, step=100 ), "lambda_l1": trial.suggest_int("lambda_l1", 0, 100, step=5), "lambda_l2": trial.suggest_int("lambda_l2", 0, 100, step=5), "min_gain_to_split": trial.suggest_float("min_gain_to_split", 0, 15), "bagging_fraction": trial.suggest_float( "bagging_fraction", 0.2, 0.95, step=0.1 ), "bagging_freq": trial.suggest_categorical("bagging_freq", [1]), "feature_fraction": trial.suggest_float( "feature_fraction", 0.2, 0.95, step=0.1 ), "objective": trial.suggest_categorical("objective", ["regression"]), } if self.with_gpu == True: params.update( { "device_type": trial.suggest_categorical( "device_type", ["gpu"] ), } ) return params if model_name == "rfc": params = { "n_estimators": trial.suggest_int("n_estimators", 50, 1000), "max_depth": trial.suggest_int("max_depth", 4, 50), "min_samples_split": trial.suggest_int("min_samples_split", 2, 150), "min_samples_leaf": trial.suggest_int("min_samples_leaf", 2, 60), } return params if model_name == "rfr": params = { "n_estimators": trial.suggest_int("n_estimators", 5, 50), "max_depth": trial.suggest_int("max_depth", 4, 20), "min_samples_split": trial.suggest_int("min_samples_split", 2, 150), "min_samples_leaf": trial.suggest_int("min_samples_leaf", 2, 60), # "n_estimators": trial.suggest_int("n_estimators", 50, 1000), # "max_depth": trial.suggest_int("max_depth", 4, 50), # "min_samples_split": trial.suggest_int("min_samples_split", 2, 150), # "min_samples_leaf": trial.suggest_int("min_samples_leaf", 2, 60), } return params if model_name == "tabnetc": # https://www.kaggle.com/code/wangqihanginthesky/baseline-tabnet # params = { # "gamma": trial.suggest_uniform("gamma", 0, 3) # } params = { # cat_idxs=cat_idxs, # cat_emb_dim=1, "n_d": trial.suggest_categorical("n_d", [16]), "n_a": trial.suggest_categorical("n_a", [16]), "n_steps": trial.suggest_categorical("n_steps", [2]), "gamma": trial.suggest_categorical("gamma", [1.4690246460970766]), "n_independent": trial.suggest_categorical("n_independent", [9]), "n_shared": trial.suggest_categorical("n_shared", [4]), "lambda_sparse": trial.suggest_categorical("lambda_sparse", [0]), "optimizer_fn": trial.suggest_categorical("optimizer_fn", [Adam]), "optimizer_params": trial.suggest_categorical("optimizer_params", [dict(lr = (0.024907164557092944))]), "mask_type": trial.suggest_categorical("mask_type", ["entmax"]), "scheduler_params": trial.suggest_categorical("scheduler_params", [dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False)]), "scheduler_fn": trial.suggest_categorical("scheduler_fn", [CosineAnnealingWarmRestarts]), "seed": trial.suggest_categorical("seed", [42]), "verbose": trial.suggest_categorical("verbose", [10]), # n_d = 16, # n_a = 16, # n_steps = 2, # gamma = 1.4690246460970766, # n_independent = 9, # n_shared = 4, # lambda_sparse = 0, # optimizer_fn = Adam, # optimizer_params = dict(lr = (0.024907164557092944)), # mask_type = "entmax", # scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False), # scheduler_fn = CosineAnnealingWarmRestarts, # seed = 42, # verbose = 10, } return params if model_name == "tabnetr": # https://www.kaggle.com/code/wangqihanginthesky/baseline-tabnet # params = { # "gamma": trial.suggest_uniform("gamma", 0, 3) # } params = { # cat_idxs=cat_idxs, # cat_emb_dim=1, "n_d": trial.suggest_categorical("n_d", [16]), "n_a": trial.suggest_categorical("n_a", [16]), "n_steps": trial.suggest_categorical("n_steps", [2]), #"n_steps": trial.suggest_categorical("n_steps", [2, 4, 7]), "gamma": trial.suggest_categorical("gamma", [1.4690246460970766]), #"gamma": trial.suggest_uniform("gamma", 0.1, 2), "n_independent": trial.suggest_categorical("n_independent", [9]), "n_shared": trial.suggest_categorical("n_shared", [4]), "lambda_sparse": trial.suggest_categorical("lambda_sparse", [0]), "optimizer_fn": trial.suggest_categorical("optimizer_fn", [Adam]), "optimizer_params": trial.suggest_categorical("optimizer_params", [dict(lr = (0.024907164557092944))]), "mask_type": trial.suggest_categorical("mask_type", ["entmax"]), "scheduler_params": trial.suggest_categorical("scheduler_params", [dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False)]), "scheduler_fn": trial.suggest_categorical("scheduler_fn", [CosineAnnealingWarmRestarts]), "seed": trial.suggest_categorical("seed", [42]), "verbose": trial.suggest_categorical("verbose", [10]), # n_d = 16, # n_a = 16, # n_steps = 2, # gamma = 1.4690246460970766, # n_independent = 9, # n_shared = 4, # lambda_sparse = 0, # optimizer_fn = Adam, # optimizer_params = dict(lr = (0.024907164557092944)), # mask_type = "entmax", # scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False), # scheduler_fn = CosineAnnealingWarmRestarts, # seed = 42, # verbose = 10, } return params if model_name == "k1": # params = { # "epochs": trial.suggest_int("epochs", 10, 55, step=5, log=False), # 5,55 # "batchsize": trial.suggest_int("batchsize", 8, 40, step=16, log=False), # "learning_rate": trial.suggest_uniform("learning_rate", 0, 3), # "o": trial.suggest_categorical("o", [True, False]), # } params = { "epochs": trial.suggest_int("epochs", 2, 55, step=5, log=False), # 5,55 "batchsize": trial.suggest_int("batchsize", 8, 40, step=16, log=False), "learning_rate": trial.suggest_uniform("learning_rate", 0, 3), "o": trial.suggest_categorical("o", [True, False]), } return params if model_name == "k2": params = { "epochs": trial.suggest_int("epochs", 5, 55, step=5, log=False), # 5,55 "batchsize": trial.suggest_int("batchsize", 8, 40, step=16, log=False), "learning_rate": trial.suggest_uniform("learning_rate", 0, 3), "prime": trial.suggest_categorical( "prime", [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31] ), "drop_val": trial.suggest_uniform("drop", 0.01, 0.9), "o": trial.suggest_categorical("o", [True, False]), } return params if model_name == "k3": no_hidden_layers = trial.suggest_int("no_hidden_layers", 1, 30, step=1) dropout_placeholder = [ trial.suggest_uniform(f"drop_rate_{i+1}", 0, 0.99) for i in range(no_hidden_layers) ] units_placeholder = [ trial.suggest_int( f"units_val_{j+1}", int((self.xtrain.shape[1] + 1) ** (1 / 3)), int((self.xtrain.shape[1] + 1) ** 3), step=1, log=False, ) for j in range(no_hidden_layers) ] batch_norm_placeholder = [ trial.suggest_categorical(f"batch_norm_val_{i+1}", [0, 1]) for i in range(no_hidden_layers) ] activation_placeholder = [ trial.suggest_categorical( f"activation_string_{i+1}", ["relu", "sigmoid", "tanh"] ) for i in range(no_hidden_layers) ] epochs = trial.suggest_int("epochs", 2, 3, step=5, log=False) # 5,55 batchsize = trial.suggest_int("batchsize", 8, 40, step=16, log=False) learning_rate = trial.suggest_uniform("learning_rate", 0, 3) # epochs = trial.suggest_int("epochs", 5, 100, step=5, log=False) # 5,55 # batchsize = trial.suggest_int("batchsize", 8, 40, step=16, log=False) # learning_rate = trial.suggest_uniform("learning_rate", 0, 3) o = trial.suggest_categorical("o", [True, False]) params = { "no_hidden_layers": no_hidden_layers, "dropout_placeholder": dropout_placeholder, "units_placeholder": units_placeholder, "batch_norm_placeholder": batch_norm_placeholder, "activation_placeholder": activation_placeholder, "epochs": epochs, "batchsize": batchsize, "learning_rate": learning_rate, "o": o, } return params if model_name == "tez1": # -batch_size = 16 # -epochs = 5 # =====seed = 42 # ===target_size = 28 # -learning_rate = 0.002 params = { "batch_size": trial.suggest_categorical( "batch_size", [16] ), # ,32,64, 128,256, 512]), # "epochs": trial.suggest_int( # "epochs", 1, 2, step=1, log=False # ), # 55, step=5, log=False), # 5,55 "epochs": trial.suggest_categorical("epochs", [5]), "learning_rate": trial.suggest_uniform("learning_rate", 2, 8), "patience": trial.suggest_categorical("patience", [5]), } return params if model_name == "tez2": # -batch_size = 16 # -epochs = 5 # =====seed = 42 # ===target_size = 28 # -learning_rate = 0.002 params = { "batch_size": trial.suggest_categorical( "batch_size", [16] ), # ,32,64, 128,256, 512]), # "epochs": trial.suggest_int( # "epochs", 1, 2, step=1, log=False # ), # 55, step=5, log=False), # 5,55 "epochs": trial.suggest_categorical("epochs", [1]), "learning_rate": trial.suggest_uniform("learning_rate", 2, 8), "patience": trial.suggest_categorical("patience", [5]), } return params if model_name == "p1": # -batch_size = 16 # -epochs = 5 # =====seed = 42 # ===target_size = 28 # -learning_rate = 0.002 params = { "batch_size": trial.suggest_categorical( "batch_size", [16, 32, 128, 512] ), # ,32,64, 128,256, 512]), "epochs": trial.suggest_int( "epochs", 20, 55, step=10, log=False ), # 55, step=5, log=False), # 5,55 # "epochs": trial.suggest_categorical("epochs", [1]), "learning_rate": trial.suggest_uniform("learning_rate", 1, 8), "patience": trial.suggest_categorical("patience", [3, 5]), "momentum": trial.suggest_uniform("momentum", 0.2, 0.9), } # Demo # params = { # "batch_size": trial.suggest_categorical( # "batch_size", [16, 32, 128, 512] # ), # ,32,64, 128,256, 512]), # "epochs": trial.suggest_int( # "epochs", 1,3, step=1, log=False # ), # 55, step=5, log=False), # 5,55 # # "epochs": trial.suggest_categorical("epochs", [1]), # "learning_rate": trial.suggest_uniform("learning_rate", 1, 8), # "patience": trial.suggest_categorical("patience", [3, 5]), # "momentum": trial.suggest_uniform("momentum", 0.2, 0.9), # } return params if model_name == "pretrained": # -batch_size = 16 # -epochs = 5 # =====seed = 42 # ===target_size = 28 # -learning_rate = 0.002 # params = { # "batch_size": trial.suggest_categorical( # "batch_size", [16, 32, 128, 512] # ), # ,32,64, 128,256, 512]), # "epochs": trial.suggest_int( # "epochs", 12,25, step=5, log=False # ), # 55, step=5, log=False), # 5,55 # #"epochs": trial.suggest_categorical("epochs", [1]), # "learning_rate": trial.suggest_uniform("learning_rate", 1, 8), # "patience": trial.suggest_categorical("patience", [3,5]), # "momentum": trial.suggest_uniform("momentum", 0.2, 0.9) # } # Demo params = { "batch_size": trial.suggest_categorical( "batch_size", [16, 32, 128, 512] ), # ,32,64, 128,256, 512]), "epochs": trial.suggest_int( "epochs", 1, 2, step=1, log=False ), # 55, step=5, log=False), # 5,55 # "epochs": trial.suggest_categorical("epochs", [1]), "learning_rate": trial.suggest_uniform("learning_rate", 1, 8), "patience": trial.suggest_categorical("patience", [3, 5]), "momentum": trial.suggest_uniform("momentum", 0.2, 0.9), } print("finding params") print(params) return params if model_name == "keras": # demo self.Table = pd.DataFrame( columns=[ "val_score", "lr_modified", "learning_rate", "epochs", "batch_size", "no_hidden_layers", "dropout_placeholder", "units_placeholder", "batch_norm_placeholder", " activation_placeholder", ] ) return params def get_model(self, params): # ["lgr","lir","xgb","xgbc","xgbr"] model_name = self.model_name self._random_state = self.generate_random_no() if model_name == "lgr": return LogisticRegression(**params, random_state=self._random_state) if model_name == "lir": return LinearRegression() #**params, random_state=self._random_state) if model_name == "xgb": return "No Model Yet" if model_name == "xgbc": return XGBClassifier(**params, random_state=self._random_state) if model_name == "xgbr": return XGBRegressor(**params, random_state=self._random_state) if model_name == "mlpc": return MLPClassifier(**params, random_state=self._random_state) if model_name == "mlpr": return MLPRegressor(**params, random_state=self._random_state) if model_name == "rg": return Ridge(**params, random_state=self._random_state) if model_name == "ls": return Lasso(**params, random_state=self._random_state) if model_name == "knnc": return KNeighborsClassifier(**params) if model_name == "cbc": # User defined loss functions, metrics and callbacks are not supported for GPU return CatBoostClassifier(**params, random_state=self._random_state) #, eval_metric = CustomMetric_cbc()) if model_name == "cbr": # User defined loss functions, metrics and callbacks are not supported for GPU return CatBoostRegressor(**params, random_state=self._random_state) #, eval_metric = CustomMetric_cbc()) if model_name == "dtc": return DecisionTreeClassifier(**params, random_state=self._random_state) if model_name == "adbc": return AdaBoostClassifier(**params, random_state=self._random_state) if model_name == "gbmc": #return HistGradientBoostingClassifier(**params, random_state=self._random_state) return GradientBoostingClassifier(**params, random_state=self._random_state) if model_name == "gbmr": #return HistGradientBoostingRegressor(**params, random_state=self._random_state) return GradientBoostingRegressor(**params, random_state=self._random_state) if model_name == "hgbc": return HistGradientBoostingClassifier( **params, random_state=self._random_state ) if model_name == "lgb": return "No model yet" if model_name == "lgbmc": return LGBMClassifier(**params, random_state=self._random_state) if model_name == "lgbmr": return LGBMRegressor(**params, random_state=self._random_state) if model_name == "rfc": return RandomForestClassifier(**params, random_state=self._random_state) if model_name == "rfr": return RandomForestRegressor(**params, random_state=self._random_state) if model_name == "tabnetc": return TabNetClassifier(**params) if model_name == "tabnetr": return TabNetRegressor(**params) if model_name == "k1": return self._k1(params, random_state=self._random_state) if model_name == "k2": return self._k2(params, random_state=self._random_state) if model_name == "k3": return self._k3(params, random_state=self._random_state) if model_name == "k4": return self._k4(params, random_state=self._random_state) if model_name == "tez1": return self._tez1(params, random_state=self._random_state) if model_name == "tez2": return self._tez2(params, random_state=self._random_state) if model_name == "p1": # pytorch1 # basic pytorch model return self._p1(params=params) if model_name == "pretrained": # pytorch return self._pretrained(params=params) else: raise Exception(f"{model_name} is invalid!") def _pretrained(self, params): self.learning_rate = 10 ** (-1 * params["learning_rate"]) # model = pretrained_models(len(self.filtered_features)) model = MODEL_DISPATCHER["resnet34"]( pretrained=False ) # supports all image size model.to("cuda") # train_loader self.train_loader = DataLoader( self.train_dataset, shuffle=True, num_workers=4, batch_size=params["batch_size"], ) self.valid_loader = DataLoader( self.valid_dataset, shuffle=False, num_workers=4, batch_size=params["batch_size"], ) self.test_loader = DataLoader( self.test_dataset, shuffle=False, num_workers=4, batch_size=params["batch_size"], ) optimizer = torch.optim.SGD( model.parameters(), lr=10 ** (-1 * params["learning_rate"]), momentum=params["momentum"], # 0.9, ) # base_opt = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer = SWA(base_opt, swa_start=10, swa_freq=2, swa_lr=0.0005) # in torch some scheduler step after every epoch, # some after every batch scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, factor=0.5, patience=params["patience"], verbose=True, mode="max", threshold=1e-4, ) if torch.cuda.device_count() > 1: model = nn.DataParallel(model) return trainer_p1( model, self.train_loader, self.valid_loader, optimizer, scheduler, self.use_cutmix, ) def _p1(self, params=0, random_state=0): self.learning_rate = 10 ** (-1 * params["learning_rate"]) model = p1_model() model.to("cuda") # train_loader self.train_loader = DataLoader( self.train_dataset, shuffle=True, num_workers=4, batch_size=params["batch_size"], ) self.valid_loader = DataLoader( self.valid_dataset, shuffle=False, num_workers=4, batch_size=params["batch_size"], ) self.test_loader = DataLoader( self.test_dataset, shuffle=False, num_workers=4, batch_size=params["batch_size"], ) optimizer = torch.optim.SGD( model.parameters(), lr=10 ** (-1 * params["learning_rate"]), momentum=params["momentum"], # 0.9, ) # base_opt = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer = SWA(base_opt, swa_start=10, swa_freq=2, swa_lr=0.0005) scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, factor=0.5, patience=params["patience"], verbose=True, mode="max", threshold=1e-4, ) return trainer_p1( model, self.train_loader, self.valid_loader, optimizer, scheduler, self.use_cutmix, ) def _tez1(self, params, random_state): """ self.train_image_paths self.valid_image_paths self.train_dataset selt.valid_dataset """ print("params of tez1") print(params) print("=" * 40) batch_size = params["batch_size"] epochs = params["epochs"] learning_rate = 10 ** ( -1 * params["learning_rate"] ) # params["learning_rate"] # img_size = 256 # nothing to do with model used for naming output files model_name = "resnet50" seed = random_state target_size = len(set(self.ytrain)) n_train_steps = int(len(self.train_dataset) / batch_size * epochs) model = UModel( model_name=model_name, num_classes=target_size, learning_rate=learning_rate, n_train_steps=n_train_steps, ) return model def _tez2(self, params, random_state): """ self.train_image_paths self.valid_image_paths self.train_dataset selt.valid_dataset """ print("params of tez2") print(params) print("=" * 40) batch_size = params["batch_size"] epochs = params["epochs"] learning_rate = 10 ** ( -1 * params["learning_rate"] ) # params["learning_rate"] # img_size = 256 # nothing to do with model used for naming output files model_name = "resnet50" seed = random_state target_size = len(set(self.ytrain)) # n_train_steps = int(len(self.train_image_paths) / batch_size * epochs) n_train_steps = int(len(self.train_dataset) / batch_size / 1 * epochs) model = DigitRecognizerModel( model_name="resnet50", num_classes=target_size, learning_rate=learning_rate, n_train_steps=n_train_steps, ) model = Tez(model) return model def _k1(self, params, random_state): # simple model model = keras.Sequential() model.add(BatchNormalization()) model.add(Dense(132, activation="relu")) model.add(BatchNormalization()) model.add(Dense(32, activation="relu")) model.add(BatchNormalization()) model.add(Dense(32, activation="relu")) model.add(BatchNormalization()) model.add(Dense(32, activation="relu")) model.add(BatchNormalization()) model.add(Dense(32, activation="relu")) # model.add(Dense(32, activation="relu")) # #model.add(Dropout(p=0.2)) # model.add(Dense(32, activation="relu")) # model.add(BatchNormalization()) # model.add(Dense(32, activation="relu")) # #model.add(Dropout(p=0.2)) # model.add(BatchNormalization()) # model.add(Dense(32, activation="relu")) # model.add(BatchNormalization()) # model.add(Dense(16, activation="relu")) model.add(BatchNormalization()) """ model = keras.Sequential() model.add(BatchNormalization()) model.add(Dense(32, activation="relu")) model.add(Dense(32, activation="relu")) model.add(Dense(32, activation="relu")) model.add(BatchNormalization()) model.add(Dense(16, activation="relu")) model.add(BatchNormalization()) """ # adam is used when we don't have custom lr_scheduler # any(x in ['b', 'd', 'foo', 'bar'] for x in ['a', 'b']): True # all(x in ['b', 'd', 'foo', 'bar'] for x in ['a', 'b']): False if any( x in self.callbacks_list for x in ["simple_decay", "cosine_decay", "exponential_decay"] ): # time for custom lr lr_start = 1e-2 # 0.01 lr_start = 10 ** ( -1 * params["learning_rate"] ) # optimize starting point using optuna global_variables.lr_start = lr_start opt = Adamax(learning_rate=lr_start) else: adam = tf.keras.optimizers.Adam( learning_rate=10 ** (-1 * params["learning_rate"]) ) opt = adam # PREDICT: gives probability always so in case of metrics which takes hard class do (argmax) # For #class more than 2 output label has multiple node: # confusion remains with 2class problem as it can have both one node or 2 node in end if self.comp_type == "regression": model.add(Dense(1, activation="linear")) # /None linear tanh # model.compile( # loss=self.metrics_name, # optimizer=Adamax(learning_rate=lr_start), # adam, # metrics=[tf.keras.metrics.MeanSquaredError()], # ) model.compile(optimizer=opt, loss = 'mae', metrics=['mean_squared_error']) elif self.comp_type == "2class": if len(self.ytrain.shape) == 1: model.add(Dense(1, activation="sigmoid")) #: binary_crossentropy #--> https://www.kaggle.com/code/cdeotte/tensorflow-gru-starter-0-790 #opt = tf.keras.optimizers.Adam(learning_rate=0.001) loss = tf.keras.losses.BinaryCrossentropy() model.compile(loss=loss, optimizer = opt) # model.compile( # optimizer=opt, metrics= [amex_metric_tensorflow], #["accuracy"],["accuracy"] # loss=tf.keras.losses.BinaryCrossentropy(), # ) #loss="binary_crossentropy", print("New metrics implemented1") else: # binary with one hot y no more binary problem so it is like multi class := don't use this case use above one instead model.add(Dense(self.ytrain.shape[1], activation="softmax")) model.compile( loss="categorical_crossentropy", optimizer=opt, metrics= [amex_metric], #["accuracy"], ) print("New metrics implemented") elif self.comp_type == "multi_class": # https://medium.com/deep-learning-with-keras/which-activation-loss-functions-in-multi-class-clasification-4cd599e4e61f if len(self.ytrain.shape) == 1: # sparse since true prediction is 1D # op1> # act:None # loss:keras.losses.SparseCategoricalCrossentropy(from_logits=True) # metrics:metrics=[keras.metrics.SparseCategoricalAccuracy()] # op2> # act: softmax # loss: sparse_categorical_crossentropy # metrics: keras.metrics.SparseCategoricalAccuracy() if params["o"]: loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True) metrics = [keras.metrics.SparseCategoricalAccuracy()] act = None else: loss = keras.losses.SparseCategoricalCrossentropy() metrics = [keras.metrics.SparseCategoricalAccuracy()] act = "softmax" model.add(Dense(self.ytrain.shape[1], activation=act)) model.compile(loss=loss, optimizer=opt, metrics=metrics) else: # ytrain is not 1D # op1> # act:None # loss:keras.losses.CategoricalCrossentropy(from_logits=True) # metrics:metrics=[keras.metrics.CategoricalAccuracy()] # op2> # act: softmax # loss: categorical_crossentropy # metrics: keras.metrics.CategoricalAccuracy() if params["o"]: loss = keras.losses.CategoricalCrossentropy(from_logits=True) metrics = [keras.metrics.CategoricalAccuracy()] act = None else: loss = keras.losses.CategoricalCrossentropy() metrics = [keras.metrics.CategoricalAccuracy()] act = "softmax" model.add(Dense(self.ytrain.shape[1], activation="softmax")) model.compile(loss=loss, optimizer=opt, metrics=metrics) elif self.comp_type == "multi_label": model.add(Dense(self.ytrain.shape[1], activation="sigmoid")) model.compile( loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"] ) return model def _k2(self, params, random_state): # cone model model = Sequential() model.add(BatchNormalization()) no_cols = self.xtrain.shape[1] model.add(Dense(2 * no_cols, activation="relu")) while no_cols > 2 * 10: model.add(Dropout(params["drop_val"])) model.add(Dense(no_cols, activation="relu")) model.add(Dense(no_cols, activation="relu")) model.add(BatchNormalization()) no_cols = int(no_cols // params["prime"]) adam = tf.keras.optimizers.Adam( learning_rate=10 ** (-1 * params["learning_rate"]) ) # PREDICT: gives probability always so in case of metrics which takes hard class do (argmax) # For #class more than 2 output label has multiple node: # confusion remains with 2class problem as it can have both one node or 2 node in end if self.comp_type == "regression": model.add(Dense(1, activation="relu")) # /None linear tanh model.compile( loss=self.metrics_name, optimizer=adam, metrics=[tf.keras.metrics.MeanSquaredError()], ) elif self.comp_type == "2class": if len(self.ytrain.shape) == 1: model.add(Dense(1, activation="sigmoid")) #: binary_crossentropy #--> https://www.kaggle.com/code/cdeotte/tensorflow-gru-starter-0-790 #opt = tf.keras.optimizers.Adam(learning_rate=0.001) loss = tf.keras.losses.BinaryCrossentropy() model.compile(loss=loss, optimizer = opt) # model.compile( # optimizer=opt, metrics= [amex_metric_tensorflow], #["accuracy"],["accuracy"] # loss=tf.keras.losses.BinaryCrossentropy(), # ) #loss="binary_crossentropy", print("New metrics implemented1") else: # binary with one hot y no more binary problem so it is like multi class := don't use this case use above one instead model.add(Dense(self.ytrain.shape[1], activation="softmax")) model.compile( loss="categorical_crossentropy", optimizer=opt, metrics= [amex_metric], #["accuracy"], ) print("New metrics implemented") elif self.comp_type == "multi_class": # https://medium.com/deep-learning-with-keras/which-activation-loss-functions-in-multi-class-clasification-4cd599e4e61f if len(self.ytrain.shape) == 1: # sparse since true prediction is 1D # op1> # act:None # loss:keras.losses.SparseCategoricalCrossentropy(from_logits=True) # metrics:metrics=[keras.metrics.SparseCategoricalAccuracy()] # op2> # act: softmax # loss: sparse_categorical_crossentropy # metrics: keras.metrics.SparseCategoricalAccuracy() if params["o"]: loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True) metrics = [keras.metrics.SparseCategoricalAccuracy()] act = None else: loss = keras.losses.SparseCategoricalCrossentropy() metrics = [keras.metrics.SparseCategoricalAccuracy()] act = "softmax" model.add(Dense(self.ytrain.shape[1], activation=act)) model.compile(loss=loss, optimizer=opt, metrics=metrics) else: # ytrain is not 1D # op1> # act:None # loss:keras.losses.CategoricalCrossentropy(from_logits=True) # metrics:metrics=[keras.metrics.CategoricalAccuracy()] # op2> # act: softmax # loss: categorical_crossentropy # metrics: keras.metrics.CategoricalAccuracy() if params["o"]: loss = keras.losses.CategoricalCrossentropy(from_logits=True) metrics = [keras.metrics.CategoricalAccuracy()] act = None else: loss = keras.losses.CategoricalCrossentropy() metrics = [keras.metrics.CategoricalAccuracy()] act = "softmax" model.add(Dense(self.ytrain.shape[1], activation="softmax")) model.compile(loss=loss, optimizer=opt, metrics=metrics) elif self.comp_type == "multi_label": model.add(Dense(self.ytrain.shape[1], activation="sigmoid")) model.compile( loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"] ) return model def _k3(self, params, random_state): # cone model model = Sequential() model.add(BatchNormalization()) for i in range(params["no_hidden_layers"]): if params["batch_norm_placeholder"][i] == 1: model.add(BatchNormalization()) model.add( Dense( units=params["units_placeholder"][i], activation=params["activation_placeholder"][i], ) ) model.add(Dropout(params["dropout_placeholder"][i])) gc.collect() adam = tf.keras.optimizers.Adam( learning_rate=10 ** (-1 * params["learning_rate"]) ) # PREDICT: gives probability always so in case of metrics which takes hard class do (argmax) # For #class more than 2 output label has multiple node: # confusion remains with 2class problem as it can have both one node or 2 node in end if self.comp_type == "regression": model.add(Dense(1, activation="relu")) # /None linear tanh model.compile( loss=self.metrics_name, optimizer=adam, metrics=[tf.keras.metrics.MeanSquaredError()], ) elif self.comp_type == "2class": if len(self.ytrain.shape) == 1: model.add(Dense(1, activation="sigmoid")) #: binary_crossentropy #--> https://www.kaggle.com/code/cdeotte/tensorflow-gru-starter-0-790 #opt = tf.keras.optimizers.Adam(learning_rate=0.001) loss = tf.keras.losses.BinaryCrossentropy() model.compile(loss=loss, optimizer = opt) # model.compile( # optimizer=opt, metrics= [amex_metric_tensorflow], #["accuracy"],["accuracy"] # loss=tf.keras.losses.BinaryCrossentropy(), # ) #loss="binary_crossentropy", print("New metrics implemented1") else: # binary with one hot y no more binary problem so it is like multi class := don't use this case use above one instead model.add(Dense(self.ytrain.shape[1], activation="softmax")) model.compile( loss="categorical_crossentropy", optimizer=opt, metrics= [amex_metric], #["accuracy"], ) print("New metrics implemented") elif self.comp_type == "multi_class": # https://medium.com/deep-learning-with-keras/which-activation-loss-functions-in-multi-class-clasification-4cd599e4e61f if len(self.ytrain.shape) == 1: # sparse since true prediction is 1D # op1> # act:None # loss:keras.losses.SparseCategoricalCrossentropy(from_logits=True) # metrics:metrics=[keras.metrics.SparseCategoricalAccuracy()] # op2> # act: softmax # loss: sparse_categorical_crossentropy # metrics: keras.metrics.SparseCategoricalAccuracy() if params["o"]: loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True) metrics = [keras.metrics.SparseCategoricalAccuracy()] act = None else: loss = keras.losses.SparseCategoricalCrossentropy() metrics = [keras.metrics.SparseCategoricalAccuracy()] act = "softmax" model.add(Dense(self.ytrain.shape[1], activation=act)) model.compile(loss=loss, optimizer=opt, metrics=metrics) else: # ytrain is not 1D # op1> # act:None # loss:keras.losses.CategoricalCrossentropy(from_logits=True) # metrics:metrics=[keras.metrics.CategoricalAccuracy()] # op2> # act: softmax # loss: categorical_crossentropy # metrics: keras.metrics.CategoricalAccuracy() if params["o"]: loss = keras.losses.CategoricalCrossentropy(from_logits=True) metrics = [keras.metrics.CategoricalAccuracy()] act = None else: loss = keras.losses.CategoricalCrossentropy() metrics = [keras.metrics.CategoricalAccuracy()] act = "softmax" model.add(Dense(self.ytrain.shape[1], activation="softmax")) model.compile(loss=loss, optimizer=opt, metrics=metrics) elif self.comp_type == "multi_label": model.add(Dense(self.ytrain.shape[1], activation="sigmoid")) model.compile( loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"] ) return model # DLastStark # https://www.kaggle.com/code/dlaststark/tps-may22-what-tf-again def _k4(self, params, random_state): x_input = Input(shape=(len(self.useful_features),)) xi = Dense(units=384, activation="swish", kernel_initializer="lecun_normal")( x_input ) xi = BatchNormalization()(xi) xi = Dropout(rate=0.25)(xi) x = Reshape((16, 24))(xi) x = Conv1D( filters=48, activation="swish", kernel_size=3, strides=2, padding="same", kernel_initializer="lecun_normal", )(x) x = BatchNormalization()(x) x1 = Conv1D( filters=96, activation="swish", kernel_size=3, strides=1, padding="same", kernel_initializer="lecun_normal", )(x) x1 = BatchNormalization()(x1) x2 = Conv1D( filters=96, activation="swish", kernel_size=3, strides=1, padding="same", kernel_initializer="lecun_normal", )(x1) x2 = BatchNormalization()(x2) x2 = Conv1D( filters=96, activation="swish", kernel_size=3, strides=1, padding="same", kernel_initializer="lecun_normal", )(x2) x2 = BatchNormalization()(x2) x = Add()([x1, x2]) x = Conv1D( filters=96, activation="swish", kernel_size=3, strides=2, padding="same", kernel_initializer="lecun_normal", )(x) x = BatchNormalization()(x) x = Flatten()(x) x = Add()([x, xi]) x = Dense(units=192, activation="swish", kernel_initializer="lecun_normal")(x) x = BatchNormalization()(x) x = Dropout(rate=0.3)(x) x = Dense(units=96, activation="swish", kernel_initializer="lecun_normal")(x) x = BatchNormalization()(x) x = Dropout(rate=0.2)(x) x_output = Dense( units=1, activation="sigmoid", kernel_initializer="lecun_normal" )(x) model = Model(inputs=x_input, outputs=x_output, name="TPS_May22_TF_Model") model.compile( optimizer=Adamax(learning_rate=lr_start), loss="binary_crossentropy", metrics=["AUC"], ) return model def save_logs(self, params): print("Saving Log Table") if self._log_table is None: # not initialized self._log_table = pd.DataFrame( columns=["trial_score"] + list(params.keys()) + ["keras_history"] ) self._log_table.loc[self._log_table.shape[0], :] = ( [self._trial_score] + list(params.values()) + [self._history] ) def get_callbacks(self, params, verbose): ############################################################################### # Tree based models ############################################################################ # lgb/lgbmc # https://lightgbm.readthedocs.io/en/latest/Python-API.html#callbacks """ early_stopping(stopping_rounds[, ...]) Create a callback that activates early stopping. log_evaluation([period, show_stdv]) Create a callback that logs the evaluation results. record_evaluation(eval_result) Create a callback that records the evaluation history into eval_result. reset_parameter(**kwargs) Create a callback that resets the parameter after the first iteration. """ ############################################################################### # KERAS ############################################################################ # DLastStark # https://www.kaggle.com/code/dlaststark/tps-may22-what-tf-again # ambrosm # https://www.kaggle.com/code/ambrosm/amex-keras-quickstart-1-training/notebook # lr_start = 1e-2 set equal to lr # It is set while defining model K1 global_variables.lr_end = 1e-5 # 1e-4 global_variables.epochs = params["epochs"] callbacks = [TerminateOnNaN()] # always keep TerminateOnNan() if "chk_pt" in self.callbacks_list: # https://keras.io/api/callbacks/model_checkpoint/ # chk_point = ModelCheckpoint( # filepath=f"../models/models-{self.locker['comp_name']}/", # "./", # to work on this part # save_weights_only=True, # monitor="val_accuracy", # mode="max", # save_best_only=True, # ) # https://www.kaggle.com/code/dlaststark/tps-may22-what-tf-again chk_point = ModelCheckpoint( f"../models/models-{self.locker['comp_name']}/model_exp_{self.current_dict['current_exp_no']}.h5", monitor="val_auc", verbose=verbose, save_best_only=True, mode="max", ) callbacks.append(chk_pt) if "ReduceLROnPlateau" in self.callbacks_list: reduce_lr = ReduceLROnPlateau( monitor="val_accuracy", factor=0.5, patience=5, min_lr=0.00001, verbose=verbose, ) callbacks.append(reduce_lr) if "early_stopping" in self.callbacks_list: stop = EarlyStopping( monitor="accuracy", mode="max", patience=50 ) # , verbose=1) callbacks.append(stop) if "cosine_decay" in self.callbacks_list: lr = LearningRateScheduler(cosine_decay, verbose=verbose) callbacks.append(lr) if "exponential_decay" in self.callbacks_list: print("entered exponential_decay") lr = LearningRateScheduler(exponential_decay, verbose=verbose) callbacks.append(lr) if "simple_decay" in self.callbacks_list: lr = LearningRateScheduler(simple_decay, verbose=verbose) callbacks.append(lr) if "swa" in self.callbacks_list: # https://www.kaggle.com/competitions/google-quest-challenge/discussion/119371 # https://pypi.org/project/keras-swa/ """ # 'manual' , 'constant' or 'cyclic' The default schedule is 'manual', allowing the learning rate to be controlled by an external learning rate scheduler or the optimizer. start_epoch - Starting epoch for SWA. lr_schedule - Learning rate schedule. 'manual' , 'constant' or 'cyclic'. swa_lr - Learning rate used when averaging weights. swa_lr2 - Upper bound of learning rate for the cyclic schedule. swa_freq - Frequency of weight averagining. Used with cyclic schedules. batch_size - Batch size model is being trained with (only when using batch normalization). verbose - Verbosity mode, 0 or 1. """ from swa.keras import SWA #Define when to start SWA start_epoch = 5 # define swa callback swa = SWA(start_epoch=start_epoch, lr_schedule='constant', #cyclic 'manual' , 'constant' or 'cyclic' swa_lr=1e-5, #batch_size = 256 , #swa_lr2=9e-5, # swa_freq = 10, verbose=1) return callbacks def obj(self, trial): if self._state == "seed" or self._state == "fold": params = self.params else: params = self.get_params(trial) print("Current params:") print(params) model = self.get_model(params) if self._state == "seed": # There must be some other better ways self.xvalid = self.xtrain self.yvalid = self.ytrain # fit xtrain # ---------------------------------------------------------------------------- # ---------------------------------------------------------------------------- # lgb/lgbmc # https://lightgbm.readthedocs.io/en/latest/Python-API.html#callbacks """ early_stopping(stopping_rounds[, ...]) Create a callback that activates early stopping. log_evaluation([period, show_stdv]) Create a callback that logs the evaluation results. record_evaluation(eval_result) Create a callback that records the evaluation history into eval_result. reset_parameter(**kwargs) Create a callback that resets the parameter after the first iteration. """ if self.model_name == "lgb": # done but add callback # dart lgb_train = lgb.Dataset(self.xtrain, self.ytrain ) #, categorical_feature = cat_features) lgb_valid = lgb.Dataset(self.xvalid, self.yvalid ) #, categorical_feature = cat_features) """ model = lgb.train( params = params, train_set = lgb_train, num_boost_round = 10500, valid_sets = [lgb_train, lgb_valid], early_stopping_rounds = 100, verbose_eval = 500, feval = lgb_amex_metric ) # Save best model joblib.dump(model, f"../models/models-{self.locker['comp_name']}/model_exp_{self.current_dict['current_exp_no']}.pkl") """ # https://www.kaggle.com/competitions/amex-default-prediction/discussion/332575 global_variables.fold = self.optimize_on global_variables._state = self._state # sometimes we may want to predict an old experiment so don't use current exp no, set self.exp_no # counter stores no of times objective function is run path = f"../models/models-{self.comp_name}/callback_logs/lgb_models_e_{self.exp_no}_f_{global_variables.counter}_{global_variables._state}/" mkdir_from_path(path) n_rounds = 5000#5000 model = lgb.train(params, train_set = lgb_train, num_boost_round= n_rounds, valid_sets = [lgb_train,lgb_valid], feval=feval_RMSPE, verbose_eval= 250, early_stopping_rounds=500 ) # model = lgb.train( # params = params, # train_set = lgb_train, # num_boost_round = 105, #10500, # 0, #10500 # valid_sets = [lgb_train,lgb_valid], # #early_stopping_rounds = 1500, #100 # verbose_eval = 50, # #feval = amzcomp1_metrics, # #callbacks=[save_model1()], # #callbacks=[save_model2(models_folder=pathlib.Path(path), fold_id=global_variables.counter, min_score_to_save=0.78, every_k=50)] # ) elif self.model_name in ["lgbmr"]: # , "lgbmc"]: model.fit( self.xtrain, self.ytrain, eval_set=[(self.xvalid, self.yvalid)], eval_metric="auc", early_stopping_rounds=1000, callbacks=[ LightGBMPruningCallback(trial, "auc") ], # there is trial which creates issue when called from seed_it verbose=0, ) elif self.model_name in ["lgbmc"]: # done # https://www.kaggle.com/code/ambrosm/amex-lightgbm-quickstart model.fit( self.xtrain, self.ytrain, eval_set = [(self.xvalid, self.yvalid)], # custom metrics: https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html eval_metric=[lgbmc_amex_metric], callbacks= [log_evaluation(100)] # save_model() can't be called from lgbmc but can be from lgb ) elif self.model_name == "xgb": # not working # https://www.kaggle.com/competitions/amex-default-prediction/discussion/332575 # global_variables.fold = self.optimize_on # global_variables._state = self._state # global_variables.exp_no = self.current_dict['current_exp_no'] # path = f"../models/models-{self.comp_name}/lgb_models_e_{global_variables.exp_no}_f_{global_variables.counter}_{global_variables._state}/" # mkdir_from_path(path) # https://www.kaggle.com/code/cdeotte/xgboost-starter-0-793 # https://www.kaggle.com/code/sietseschrder/xgboost-starter-0-793 # of chris deotte and one other person but not worked yet # it is a low level xgboost # TRAIN, VALID, TEST FOR FOLD K # pass xtrain, ytrain as df # Xy_train = IterLoadForDMatrix(np.concatenate((self.xtrain, self.ytrain.reshape(-1,1)), axis=1)) # dtrain = xgb.DeviceQuantileDMatrix(Xy_train, max_bin=256) # dvalid = xgb.DMatrix(data=self.xvalid, label=self.yvalid) # if we are using feature importance it is better to convert it to dataframe if self.calculate_feature_importance: self.xtrain = pd.DataFrame(self.xtrain, columns = self.useful_features) self.xvalid = pd.DataFrame(self.xvalid, columns = self.useful_features) self.ytrain = pd.DataFrame(self.ytrain, columns = [self.locker["target_name"]]) self.yvalid = pd.DataFrame(self.yvalid, columns = [self.locker["target_name"]]) # https://www.kaggle.com/code/thedevastator/ensemble-lightgbm-catboost-xgboost dtrain = xgb.DMatrix(data = self.xtrain, label = self.ytrain) dvalid = xgb.DMatrix(data = self.xvalid, label = self.yvalid) # TRAIN MODEL FOLD K watchlist = [(dtrain, 'train'), (dvalid, 'eval')] model = xgb.train(params, dtrain=dtrain, evals= watchlist, # it is same [(dtrain,'train'),(dvalid,'valid')], custom_metric= xgboost_amex_metric_mod1, #getaroom_metrics maximize = True, # be very careful of this maximizes amex_metric # num_boost_round= 9999, #3000, #9999, #9999, # early_stopping_rounds= 5000, # 1000, #5000, #1000, # verbose_eval= 500, #100, num_boost_round= 6000, #3000, #9999, #9999, early_stopping_rounds= 1000, # 1000, #5000, #1000, verbose_eval= 500, #100, #callbacks=[save_model2(models_folder=pathlib.Path(path), fold_id=global_variables.counter, min_score_to_save=0.78, every_k=50)] #callbacks = [save_model()] ) #model.save_model(f"../models/models-{self.locker['comp_name']}/model_exp_{self.current_dict['current_exp_no']}.xgb") # https://stackoverflow.com/questions/37627923/how-to-get-feature-importance-in-xgboost # print('best ntree_limit:', model.best_ntree_limit) # print('best score:', model.best_score) # self.best_ntree_limit_value = model.best_ntree_limit # GET FEATURE IMPORTANCE FOR FOLD K # will work only when we pass a dataframe if self.calculate_feature_importance: dd = model.get_score(importance_type='weight') temp = pd.DataFrame({'feature': list(dd.keys()) ,f'importance_{global_variables.counter}': list(dd.values())}) # dd.keys() replace by self.ordered_list_test if self.feature_importance_table is None: # first time self.feature_importance_table = pd.DataFrame({'feature':dd.keys(),f'importance_{global_variables.counter}':dd.values()}) else: self.feature_importance_table = pd.merge(self.feature_importance_table, temp, on="feature",how="left") del dd , temp gc.collect() print(self.feature_importance_table) elif self.model_name == "cbc": # for catboost params,eval_metric are set while creating model actually it is done for every wrapper function. # for low level like lgb we call it from .train() model.fit( self.xtrain, self.ytrain, eval_set = [(self.xvalid, self.yvalid)], verbose=0, #eval_metric= [lgb_amex_metric], no eval metric for catboost, actually below true # we can call it when instantiating # https://stackoverflow.com/questions/65462220/how-to-create-custom-eval-metric-for-catboost # https://www.kaggle.com/code/thedevastator/ensemble-lightgbm-catboost-xgboost ) elif self.model_name == "cbr": # for catboost params,eval_metric are set while creating model actually it is done for every wrapper function. # for low level like lgb we call it from .train() model.fit( self.xtrain, self.ytrain, eval_set = [(self.xvalid, self.yvalid)], verbose=0, ) elif self.model_name in ["tabnetr", "tabnetc", "k1", "k2", "k3", "k4"]: # keras # ------------------------- general for all keras model # lr ranges from 1--> 0.001 # in cosine_decay : start 0.01 --> end 0.0001 # in exponential_decay : start 0.01 --> end 0.00001 # [ when we use lr scheduler] # -> we will use lr as the start while keeping end fixed to be either 0.0001 or 0.00001 # [ when we don't use lr scheduler] # -> we will use lr as constant , in this way we don't have to make much changes. # Althouh it is better to always use lr_scheduler verbose = 0 if self.model_name not in ["tabnetc", "tabnetr"]: callbacks = self.get_callbacks(params, verbose) # if self.model_name == "k4" or self.model_name == "k1": # for now we are passing "k1" as well later make it as param [callbacks] # # DLastStark model # callbacks = [lr, chk_point, TerminateOnNaN()] # else: # callbacks = [ checkpoint, reduce_lr], #[stop, checkpoint, reduce_lr], # ---------------------- if self.model_name in ["tabnetc", "tabnetr"]: model.fit( self.xtrain, self.ytrain, eval_set=[(self.xvalid, self.yvalid)], max_epochs = 30, # patience = 50, batch_size = 64, virtual_batch_size = 128, #128*20, num_workers = 4, drop_last = False, ) elif self.locker["data_type"] == "tabular": history = model.fit( x=self.xtrain, y=self.ytrain, validation_data=( self.xvalid, self.yvalid, ), # validation_data will override validation_split batch_size=params["batchsize"], epochs=params["epochs"], shuffle=True, validation_split=0.15, callbacks=callbacks, ) if self.model_name == "k4": model = load_model( f"../models/models-{self.locker['comp_name']}/model_exp_{self.current_dict['current_exp_no']}.h5", ) if self.locker["data_type"] == "image": history = model.fit( self.train_dataset, steps_per_epoch=np.ceil( len(self.xtrain) / self.params["batchsize"] ), epochs=self.params["epochs"], verbose=2, validatioin_data=self.valid_dataset, validation_step=8, callbacks=[stop, checkpoint, reduce_lr], ) model.evaluate_generator( generator=self.valid_dataset, steps=1 ) # 1 to make it perfectly divisible if self.model_name not in ["tabnetc", "tabnetr"]: self._history = history.history elif self.model_name in ["tez1", "tez2", "p1", "pretrained"]: # pytorch model_path_es = f"../models/models-{self.locker['comp_name']}/model_exp_{self.current_dict['current_exp_no'] + 1}_es" # 'model_es_s' + str(CFG.img_size) + '_f' +str(fold) + '.bin', model_path_s = f"../models/models-{self.locker['comp_name']}/model_exp_{self.current_dict['current_exp_no'] + 1}_s" if self._state == "seed": model_path_es = model_path_es + f"_seed_{self._random_state}" model_path_s = model_path_s + f"_seed_{self._random_state}" stop = EarlyStopping( monitor="valid_loss", model_path=model_path_es, patience=params["patience"], mode="min", ) es = EarlyStopping( monitor="valid_accuracy", model_path=model_path_es, patience=10, mode="max", save_weights_only=True, ) if self.model_name == "tez1": history = model.fit( self.train_dataset, # self.train_dataset valid_dataset=self.valid_dataset, # self.valid_dataset train_bs=params["batch_size"], valid_bs=16, device="cuda", epochs=params["epochs"], callbacks=[stop], fp16=True, ) # self._history = history.history model.save( model_path_s, ) elif self.model_name == "tez2": config = TezConfig( training_batch_size=params["batch_size"], validation_batch_size=2 * params["batch_size"], test_batch_size=2 * params["batch_size"], gradient_accumulation_steps=1, epochs=params["epochs"], step_scheduler_after="epoch", step_scheduler_metric="valid_accuracy", fp16=True, ) model.fit( self.train_dataset, valid_dataset=self.valid_dataset, callbacks=[es], config=config, ) elif self.model_name in ["p1", "pretrained"]: model.fit(n_iter=params["epochs"]) # self._history = history.history model.save( model_path_s, ) else: # tabular model.fit(self.xtrain, self.ytrain) # for now calculate here feature_permutation importance for xgb """ Can be modified to test only the new features, to make it fast """ # col1, col2, col3, col4 if self.calculate_permutation_feature_importance: prefix_name = "perm_importance" perm = {} #dvalid = xgb.DMatrix(data = self.xvalid, label = self.yvalid) #self.valid_preds = [model.predict(self.xvalid)] if self.model_name.startswith("k"): self.valid_preds = [model.predict(self.xvalid).flatten()] # NN else: self.valid_preds = [model.predict(self.xvalid)] #baseline= amex_metric_mod(self.yvalid, self.valid_preds[0][:]) # score valid predictions if self.metrics_name in [ "auc", "accuracy", "f1", "recall", "precision", "logloss", "auc_tf", "amex_metric", "amex_metric_mod", "amex_metric_lgb_base", ]: # Classification cl = ClassificationMetrics() if self.locker["comp_type"] == "multi_label": s1 = cl(self.metrics_name, self.yvalid[:, 0], self.valid_preds[0][:]) s2 = cl(self.metrics_name, self.yvalid[:, 1], self.valid_preds[1][:]) s3 = cl(self.metrics_name, self.yvalid[:, 2], self.valid_preds[2][:]) baseline = (s1 + s2 + s3) / 3 elif self.metrics_name in ["auc", "log_loss", "amex_metric"]: # then y proba can't be None # sanity check : convert it to numpy array baseline = cl( self.metrics_name, np.array(self.yvalid), "y_pred_dummy", np.array(self.valid_preds[0]), ) elif self.metrics_name == "amex_metric_mod": baseline= amex_metric_mod(self.yvalid, self.valid_preds[0][:]) elif self.metric_name == "amex_metric_lgb_base": baseline = amex_metric_lgb_base(self.yvalid, self.yvalid_preds[0][:]) else: baseline = cl(self.metrics_name, self.yvalid, self.valid_preds[0][:]) elif self.metrics_name in ["mae", "mse", "rmse", "msle", "rmsle", "r2"]: # Regression rg = RegressionMetrics() baseline = rg(self.metrics_name, self.yvalid, self.valid_preds) elif self.metrics_name == "getaroom_metrics": baseline = getaroom_metrics(self.yvalid, self.valid_preds[0]) elif self.metrics_name == "amzcomp1_metrics": baseline = amzcomp1_metrics(self.yvalid, self.valid_preds[0]) #care_feat = amex4_settings().feature_dict2["date"] # care_feat = getaroom_settings().feature_dict['base'] # care_feat += getaroom_settings().feature_dict['base_interact'] # care_feat = list(set(care_feat)) # don't know why it duplicated care_feat = list(set(self.useful_features)) print("Total no of features:", len(care_feat), len(set(care_feat))) print(f"Expected Time: {len(care_feat)*2/60} minutes" ) index_list = [self.useful_features.index(i) for i in care_feat] self.xvalid1 = self.xvalid.copy() for i,f_name in zip(index_list, care_feat): #range(self.xvalid.shape[1]): # no of features #print(f"perm: {i}") value = self.xvalid1[:,i].copy() # permute 10 times score = [] for j in range(3): self.xvalid1[:,i] = np.random.permutation(self.xvalid1[:,i].copy()) #dvalid = xgb.DMatrix(data = self.xvalid, label = self.yvalid) if self.model_name.startswith("k"): self.valid_preds = [model.predict(self.xvalid).flatten()] # NN else: self.valid_preds = [model.predict(self.xvalid1)] # score valid predictions if self.metrics_name in [ "auc", "accuracy", "f1", "recall", "precision", "logloss", "auc_tf", "amex_metric", "amex_metric_mod", "amex_metric_lgb_base", ]: # Classification cl = ClassificationMetrics() if self.locker["comp_type"] == "multi_label": s1 = cl(self.metrics_name, self.yvalid[:, 0], self.valid_preds[0][:]) s2 = cl(self.metrics_name, self.yvalid[:, 1], self.valid_preds[1][:]) s3 = cl(self.metrics_name, self.yvalid[:, 2], self.valid_preds[2][:]) baseline1 = (s1 + s2 + s3) / 3 elif self.metrics_name in ["auc", "log_loss", "amex_metric"]: # then y proba can't be None # sanity check : convert it to numpy array baseline1 = cl( self.metrics_name, np.array(self.yvalid), "y_pred_dummy", np.array(self.valid_preds[0]), ) elif self.metrics_name == "amex_metric_mod": baseline1= amex_metric_mod(self.yvalid, self.valid_preds[0][:]) elif self.metric_name == "amex_metric_lgb_base": baseline1 = amex_metric_lgb_base(self.yvalid, self.yvalid_preds[0][:]) else: baseline1 = cl(self.metrics_name, self.yvalid, self.valid_preds[0][:]) elif self.metrics_name in ["mae", "mse", "rmse", "msle", "rmsle", "r2"]: # Regression rg = RegressionMetrics() baseline1 = rg(self.metrics_name, self.yvalid, self.valid_preds) elif self.metrics_name == "getaroom_metrics": baseline1 = getaroom_metrics(self.yvalid, self.valid_preds[0]) elif self.metrics_name == "amzcomp1_metrics": baseline1 = amzcomp1_metrics(self.yvalid, self.valid_preds[0]) score.append(baseline1) #score.append( amex_metric_mod(self.yvalid, self.valid_preds[0][:]) ) #perm[self.useful_features[i]] = np.mean(score) - baseline perm[f_name] = np.mean(score) - baseline # reset back self.xvalid1[:,i] = value gc.collect() temp = pd.DataFrame({'feature': list(perm.keys()) ,f'{prefix_name}_{global_variables.counter}': list(perm.values())}) # dd.keys() replace by self.ordered_list_test if self.feature_importance_table is None: # first time self.feature_importance_table = pd.DataFrame({'feature':perm.keys(),f'{prefix_name}_{global_variables.counter}':perm.values()}) else: self.feature_importance_table = pd.merge(self.feature_importance_table, temp, on="feature",how="left") del perm , temp , self.xvalid1 gc.collect() print(self.feature_importance_table) """ Make prediction [keras datagen pytorch dataset ] tabular xvalid yvalid # because of multi-class evearything is a list even for 1d it is list of single element i.e. 1d array """ metrics_name = self.metrics_name if self.locker["data_type"] in ["image_path", "image_df", "image_folder"]: # storage for oof and submission # produce predictions - oof if self.model_name in ["tabnetr", "tabnetc", "k1", "k2", "k3"]: # keras image self.valid_dataset.reset() temp_preds = model.predict_generator( self.valid_dataset, steps=STEP_SIZE_TEST, verbose=1 ) elif self.model_name in ["tez1", "tez2"]: valid_preds = model.predict( self.valid_dataset, batch_size=16, n_jobs=-1 ) temp_preds = None for p in valid_preds: if temp_preds is None: temp_preds = p else: temp_preds = np.vstack((temp_preds, p)) gc.collect() elif self.model_name in ["p1", "pretrained"]: valid_preds = model.predict(self.valid_loader) # valid_preds = valid_preds.to("cpu") if self.locker["comp_type"] == "multi_label": temp_preds = [None, None, None] else: temp_preds = [None] for i, n in enumerate(temp_preds): valid_preds[i] = valid_preds[i].to("cpu") for p in valid_preds[i]: if temp_preds[i] is None: temp_preds[i] = p else: temp_preds[i] = np.vstack((temp_preds[i], p)) gc.collect() gc.collect() # for now done only for pretrained part self.valid_preds = [ np.argmax(temp_pred, axis=1) for temp_pred in temp_preds ] if self.locker["comp_type"] == "multi_label": print("Cal valid preds") print(self.valid_preds[0][:3]) print(self.valid_preds[1][:3]) print(self.valid_preds[2][:3]) if ( self._state == "seed" or self._state == "fold" ): # so create test prediction # produce predictions - test data if self.model_name in ["tabnetr", "tabnetc", "k1", "k2", "k3"]: self.valid_dataset = model.predict_generator( self.test_dataset, steps=STEP_SIZE_TEST, verbose=1 ) elif self.model_name in ["tez1", "tez2"]: test_preds = model.predict( self.test_dataset, batch_size=params["batch_size"], n_jobs=-1 ) temp_preds = None for p in test_preds: if temp_preds is None: temp_preds = p else: temp_preds = np.vstack((temp_preds, p)) gc.collect() elif self.model_name in ["p1", "pretrained"]: test_preds = model.predict(self.test_loader) # test_preds = test_preds.to("cpu") if self.locker["comp_type"] == "multi_label": temp_preds = [None, None, None] else: temp_preds = [None] for i, n in enumerate(temp_preds): test_preds[i] = test_preds[i].to("cpu") for p in test_preds[i]: if temp_preds[i] is None: temp_preds[i] = p else: temp_preds[i] = np.vstack((temp_preds[i], p)) gc.collect() gc.collect() # temp_preds = None # for p in test_preds: # if temp_preds is None: # temp_preds = p # else: # temp_preds = np.vstack((temp_preds, p)) # self.test_preds = temp_preds.argmax(axis=1) self.test_preds = [temp_pred.argmax(axis=1) for temp_pred in temp_preds] if self.locker["comp_type"] == "multi_label": print("Cal test preds") print(self.test_preds[0][:3]) print(self.test_preds[1][:3]) print(self.test_preds[2][:3]) elif self.locker["data_type"] == "tabular": if self.comp_type in [ "2class", "multi_class", "multi_label", ] and self.model_name in [ "xgb", # let's say we use it as a classifier "xgbc", "cbc", "mlpc", "knnc", "dtc", "adbc", "gbmc", "hgbc", "lgbmc", "rfc", ]: # self.model_name not in ["xgbr","lgr","lir", "lgbmr"]: if self.model_name == "lgbmc": #self.valid_preds = model.predict_proba(self.xvalid, raw_score=True) #[:, 1] self.valid_preds = model.predict_proba(self.xvalid)[:, 1] elif self.model_name in ["xgb"]: # lgb normal self.valid_preds = model.predict(dvalid ) #, iteration_range = (0, self.best_ntree_limit_value)) else: self.valid_preds = model.predict_proba(self.xvalid)[:, 1] elif self.model_name.startswith("k")or self.model_name in ["tabnetr", "tabnetc"]: self.valid_preds = model.predict(self.xvalid).flatten() # NN print("k") print(self.xvalid.shape, self.valid_preds.shape) print(self.valid_preds) else: self.valid_preds = model.predict(self.xvalid) # ML self.valid_preds = [ self.valid_preds ] # list of predictions maintain to sink with multilabel if ( self._state == "seed" or self._state == "fold" ): # so create test prediction # produce predictions - test data if self.locker["comp_type"] == "multi_label": temp_preds = [None, None, None] else: temp_preds = [None] # special case if self.comp_type in [ "2class", "multi_class", "multi_label", ] and self.model_name in [ "xgbc", "cbc", "mlpc", "knnc", "dtc", "adbc", "gbmc", "hgbc", "lgbmc", "rfc", ]: # self.model_name not in ["xgbr","lgr","lir", "lgbmr"]: # shape should be 1D (15232,) if self.model_name == "xgb": temp_preds[0] = model.predict(xgb.DMatrix(data=self.xtest) ) # , iteration_range = (0, self.best_ntree_limit_value)) # Yes this was causing issue else: temp_preds[0] = model.predict_proba(self.xtest)[:, 1] else: temp_preds[0] = model.predict(self.xtest) # else: # temp_preds[0] = model.predict(self.xtest) self.test_preds = temp_preds del temp_preds gc.collect() else: raise Exception(f"metrics not set yet of type {self.locker['data_type']}") # score valid predictions if self.metrics_name in [ "auc", "accuracy", "f1", "recall", "precision", "logloss", "auc_tf", "amex_metric", "amex_metric_mod", "amex_metric_lgb_base", ]: # Classification cl = ClassificationMetrics() if self.locker["comp_type"] == "multi_label": s1 = cl(self.metrics_name, self.yvalid[:, 0], self.valid_preds[0][:]) s2 = cl(self.metrics_name, self.yvalid[:, 1], self.valid_preds[1][:]) s3 = cl(self.metrics_name, self.yvalid[:, 2], self.valid_preds[2][:]) score = (s1 + s2 + s3) / 3 elif self.metrics_name in ["auc", "log_loss", "amex_metric"]: # then y proba can't be None # sanity check : convert it to numpy array score = cl( self.metrics_name, np.array(self.yvalid), "y_pred_dummy", np.array(self.valid_preds[0]), ) elif self.metrics_name == "amex_metric_mod": score= amex_metric_mod(self.yvalid, self.valid_preds[0][:]) elif self.metric_name == "amex_metric_lgb_base": score = amex_metric_lgb_base(self.yvalid, self.yvalid_preds[0][:]) else: print(self.metrics_name) print(self.yvalid[:5]) print(self.valid_preds[:5]) print() score = cl(self.metrics_name, self.yvalid, self.valid_preds[0][:]) elif self.metrics_name in ["mae", "mse", "rmse", "msle", "rmsle", "r2"]: # Regression rg = RegressionMetrics() score = rg(self.metrics_name, self.yvalid, self.valid_preds) elif self.metrics_name == "getaroom_metrics": #print(self.yvalid.shape, len(self.valid_preds[0]), "this si sit") print(self.yvalid[:4], self.valid_preds[0][:5]) score = getaroom_metrics(self.yvalid, self.valid_preds[0]) elif self.metrics_name == "amzcomp1_metrics": #print(self.yvalid.shape, len(self.valid_preds[0]), "this si sit") print(self.yvalid[:4], self.valid_preds[0][:5]) score = amzcomp1_metrics(self.yvalid, self.valid_preds[0]) if self._state == "opt": # Let's save these values self._trial_score = score # save it to save in log_table because params don't contain our metrics score self.save_logs(params) check_memory_usage("Obj", self, 0) #trial no, It records no of times objective function is called global_variables.counter += 1 return score def run( self, useful_features, with_gpu="--|--", prep_list="--|--", optimize_on="--|--", ): """ Run is used to call Optuna trials Run is also used to initialize variables while making prediction, so we call run --> then we call obj while Predicting """ if with_gpu != "--|--": self.with_gpu = with_gpu if optimize_on != "--|--": self.optimize_on = optimize_on if prep_list != "--|--": self.prep_list = prep_list self.useful_features = useful_features # ["pixel"] """ ###################################### # Memory uage # ###################################### """ print(f"Optimize on fold name {self.fold_name} and fold no: {self.optimize_on}") # BOTTLENECK return_type = "numpy_array" # "numpy_array" # "tensor" # xtest is not needed in optimiztion # xtest is needed in predict but only once so called from outside # slow # for i,f in enumerate(self.optimize_on): # if i==0: # # first time # self.val_idx, self.xtrain, self.xvalid, self.ytrain, self.yvalid, self.ordered_list_train = bottleneck(self.locker['comp_name'],self.useful_features, self.fold_name, f, self._state, return_type) # else: # # second time # val_idx, xtrain, xvalid, ytrain, yvalid, self.ordered_list_train = bottleneck(self.locker['comp_name'],self.useful_features, self.fold_name, f, self._state, return_type) # if val_idx is not None: # self.val_idx += val_idx # else: # self.valid_idx = None # if xtrain is not None: # self.xtrain = np.concatenate([self.xtrain, xtrain], axis=0) # else: # self.xtrain = None # if xvalid is not None: # self.xvalid = np.concatenate([self.xvalid, xvalid], axis=0) # else: # self.xvalid = None # if ytrain is not None: # self.ytrain = np.concatenate([self.ytrain, ytrain], axis=0) # else: # self.ytrain = None # if yvalid is not None: # self.yvalid = np.concatenate([self.yvalid, yvalid], axis=0) # else: # self.yvalid = None # del val_idx, xtrain, xvalid, ytrain, yvalid # gc.collect() # optimize on fold no [2] mean optimizing on [0,1,3,4] self.val_idx, self.xtrain, self.xvalid, self.ytrain, self.yvalid, self.ordered_list_train = bottleneck(self.locker['comp_name'],self.useful_features, self.fold_name, self.optimize_on, self._state, return_type) if self.model_name == "tabnetr": self.ytrain = self.ytrain.reshape(-1,1) self.yvalid = self.yvalid.reshape(-1,1) print("printing xtrain") print(self.xtrain) print() print("printing xvalid") print(self.xvalid) print("-"*40) if self.model_name.startswith("k") or self.model_name in ["tabnetr", "tabnetc", 'gbmr','gbmc']: if self._state == "fold": # has train , valid , test # fill with na in case of NN self.xtrain[np.isnan(self.xtrain)] = 0 self.xvalid[np.isnan(self.xvalid)] = 0 self.xtest[np.isnan(self.xtest)] = 0 #print("self.xtrain.shape, self.ytrain.shape, self.xvalid.shape, self.yvalid.shape, len(self.val_idx), self.xtest.shape") #print(self.xtrain.shape, self.ytrain.shape, self.xvalid.shape, self.yvalid.shape, len(self.val_idx), self.xtest.shape) pass elif self._state == "opt": self.xtrain[np.isnan(self.xtrain)] = 0 self.xvalid[np.isnan(self.xvalid)] = 0 #print("self.xtrain.shape, self.ytrain.shape, self.xvalid.shape, self.yvalid.shape, len(self.val_idx)") #print(self.xtrain.shape, self.ytrain.shape, self.xvalid.shape, self.yvalid.shape, len(self.val_idx)) pass else: raise Exception("Can't enter run function if _state != opt/fold") if self._state != 'opt': # "fold", "_seed" then we would have xtest and corresponding self.ordered_list_test # sanity check: for c,(i,j) in enumerate(zip(self.ordered_list_test, self.ordered_list_train)): if i != j: print() print("--+"*40) print(self.ordered_list_test) print() print(self.ordered_list_train) raise Exception(f"Feature no {c} don't correspond in test - train {i},{j}") # be sure self.useful_features = self.ordered_list_train print("No of Features:",len(self.useful_features)) if self.model_name in ["lgr"]: # fill missing values for item in [self.xtrain, self.xvalid]: if item is not None: item[ np.isnan(item)] = -999 #item.fillna(-999, inplace=True) #self.val_idx, self.xtrain, self.xvalid, self.ytrain, self.yvalid, self.xtest = bottleneck(self.locker['comp_name'],self.useful_features, self.fold_name, self.optimize_on, self._state, return_type) # print(len(self.val_idx)) # print(self.xtrain.shape) # print(self.xvalid.shape) # print(self.ytrain.shape) # print(self.yvalid.shape) #print(self.xtest.shape) """ image_df : image is stored in dataframe image_path: image path is there in dataframe image_folder: there are image folders returns self.valid_dataset self.train_dataset """ # => albumations augmentations # tez1 if self.aug_type == "aug1": self.train_aug = A.Compose( [ A.Normalize( mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0, p=1.0, ) ], p=1.0, ) self.valid_aug = A.Compose( [ A.Normalize( mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0, p=1.0, ) ], p=1.0, ) # tez2 elif self.aug_type == "aug2": self.train_aug = A.Compose( [ albumentations.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0, ), ], p=1.0, ) self.valid_aug = A.Compose( [ albumentations.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0, ), ], p=1.0, ) # kaggle tv elif self.aug_type == "aug3": self.train_aug = A.Compose([Rotate(20), ToTensor()]) self.valid_aug = A.Compose([ToTensor()]) # abhishek bengaliai video elif self.aug_type == "aug4": # valid self.valid_aug = albumentations.Compose( [ albumentations.Resize(128, 128, always_apply=True), albumentations.Normalize( (0.485, 0.456, 0.406), (9, 0.224, 0.225), always_apply=True ), ] ) # train self.train_aug = albumentations.Compose( [ albumentations.Resize(128, 128, always_apply=True), albumentations.ShiftScaleRotate( shift_limit=0.0625, scale_limit=5, p=0.9 ), albumentations.Normalize( (0.485, 0.456, 0.406), (9, 0.224, 0.225), always_apply=True ), ] ) # self.sample = pd.read_csv( # f"../input/input-{self.locker['comp_name']}/" + "sample.csv" # ) # self.test = pd.read_csv(f"../configs/configs-{self.locker['comp_name']}/" + "test.csv") # Do we need self.test Now because we are not mapping to id using whole putting whole as array # we definately not need it in optimization # self.test = pd.read_parquet( # f"../input/input-{self.locker['comp_name']}/" + "test.parquet" # ) # self.test[self.locker["target_name"]] = 0.0 # self.sample = self.test.copy() # temp No need now It only increases memory usage use inside "image_df" # => datasets if self.locker["data_type"] == "image_path": image_path = f"../input/input-{self.locker['comp_name']}/" + "train_images/" test_path = f"../input/input-{self.locker['comp_name']}/" + "test_images/" self.sample = self.test.copy()[ [self.locker["id_name"], self.locker["target_name"]] ] if self.model_name in ["tez1", "tez2", "pretrained", "p1"]: # now implemented for pytorch # use pytorch self.train_image_paths = [ os.path.join(image_path, str(x)) for x in self.xtrain[self.locker["id_name"]].values ] self.valid_image_paths = [ os.path.join(image_path, str(x)) for x in self.xvalid[self.locker["id_name"]].values ] # new # ------------------ prep test dataset # self.test_image_paths = [ # os.path.join( # test_path, str(x) # ) # f"../input/input-{self.locker['comp_name']}/" + "test_img/" + x # for x in self.sample[self.locker["id_name"]].values # ] # fake targets # correctly defince sample new_list = [] # do this to maintain order for i in self.sample[self.locker["id_name"]]: if i not in new_list: new_list.append(i) gc.collect() self.sample = pd.DataFrame(new_list, columns=[self.locker["id_name"]]) self.sample[self.locker["target_name"]] = 0 self.test_image_paths = [ os.path.join( test_path, str(x) ) # f"../input/input-{self.locker['comp_name']}/" + "test_img/" + x for x in self.sample[self.locker["id_name"]].values ] self.test_targets = self.sample[ self.locker["target_name"] ].values # dfx_te.digit_sum.values # ==========================================> if self._dataset in [ "BengaliDataset", ]: print("Entered here", self._dataset) # BengaliDataset self.train_dataset = BengaliDataset( # train_dataset image_paths=self.train_image_paths, targets=self.ytrain, img_height=128, img_width=128, transform=self.train_aug, ) self.valid_dataset = BengaliDataset( # valid_dataset image_paths=self.valid_image_paths, targets=self.yvalid, img_height=128, img_width=128, transform=self.valid_aug, ) self.test_dataset = BengaliDataset( image_paths=self.test_image_paths, targets=self.test_targets, img_height=128, img_width=128, transform=self.valid_aug, ) print(self.train_dataset) print(self.valid_dataset) print(self.test_dataset) else: # imageDataset self.train_dataset = ImageDataset( # train_dataset image_paths=self.train_image_paths, targets=self.ytrain, augmentations=self.train_aug, ) self.valid_dataset = ImageDataset( # valid_dataset image_paths=self.valid_image_paths, targets=self.yvalid, augmentations=self.valid_aug, ) self.test_dataset = ImageDataset( image_paths=self.test_image_paths, targets=self.test_targets, augmentations=self.valid_aug, ) elif self.model_name in ["k1", "k2", "k3"]: # now implemented for keras # use keras flow_from_dataframe train_datagen = ImageDataGenerator(rescale=1.0 / 255) valid_datagen = ImageDataGenerator(rescale=1.0 / 255) if self.use_cutmix != True: self.train_dataset = train_datagen.flow_from_dataframe( dataframe=self.xtrain, directory=image_path, target_size=(28, 28), # images are resized to (28,28) x_col=self.locker["id_name"], y_col=self.locker["target_name"], batch_size=32, seed=42, shuffle=True, class_mode="categorical", # "binary" ) elif self.use_cutmix == True: train_datagen1 = train_datagen.flow_from_dataframe( dataframe=self.xtrain, directory=image_path, target_size=(28, 28), # images are resized to (28,28) x_col=self.locker["id_name"], y_col=self.locker["target_name"], batch_size=32, seed=42, shuffle=True, # Required for cutmix class_mode="categorical", # "binary" ) train_datagen2 = train_datagen.flow_from_dataframe( dataframe=self.xtrain, directory=image_path, target_size=(28, 28), # images are resized to (28,28) x_col=self.locker["id_name"], y_col=self.locker["target_name"], batch_size=32, seed=42, shuffle=True, # Required for cutmix class_mode="categorical", # "binary" ) self.train_dataset = CutMixImageDataGenerator( generator1=train_generator1, generator2=train_generator2, img_size=(28, 28), batch_size=32, ) self.valid_dataset = valid_datagen.flow_from_dataframe( dataframe=self.xvalid, directory=image_path, target_size=(28, 28), # images are resized to (28,28) x_col=self.locker["id_name"], y_col=self.locker["target_name"], batch_size=32, seed=42, shuffle=True, class_mode="categorical", # "binary" ) elif self.locker["data_type"] == "image_df": # # it is not good to use whole image everytime # # create a seperate test_df and sample_df aka test to store test set # self.test = pd.read_csv(f"../configs/configs-{self.locker['comp_name']}/" + "test_df.csv") t = [] for n in self.useful_features: t += filter(lambda x: x.startswith(n), list(self.xtrain.columns)) gc.collect() self.filtered_features = t if self._dataset in [ "BengaliDataset", ]: # now implemented for pytorch # Can make our own custom dataset.. Note tez has dataloader inside the model so don't make self.train_dataset = BengaliDataset( # train_dataset csv=self.xtrain[ self.filtered_features + [self.locker["target_name"]] ], img_height=28, img_width=28, transform=self.train_aug, ) self.valid_dataset = BengaliDataset( # valid_dataset csv=self.xvalid[ self.filtered_features + [self.locker["target_name"]] ], img_height=28, img_width=28, transform=self.valid_aug, ) self.test_dataset = BengaliDataset( df=self.test[self.filtered_features + [self.locker["target_name"]]], img_height=28, img_width=28, augmentations=self.valid_aug, ) elif self._dataset in [ "DigitRecognizerDataset", ]: # DigitRecognizerDataset self.train_dataset = DigitRecognizerDataset( df=self.xtrain[ self.filtered_features + [self.locker["target_name"]] ], augmentations=self.train_aug, model_name=self.model_name, ) self.valid_dataset = DigitRecognizerDataset( df=self.xvalid[ self.filtered_features + [self.locker["target_name"]] ], augmentations=self.valid_aug, model_name=self.model_name, ) self.test_dataset = DigitRecognizerDataset( df=self.test[self.filtered_features + [self.locker["target_name"]]], augmentations=self.valid_aug, model_name=self.model_name, ) # nn.Crossentropy requires target to be single not from interval [0, #classes] # if self.model_name.startswith("p") and self.comp_type != "2class": # self.ytrain = np_utils.to_categorical(self.ytrain) # self.yvalid = np_utils.to_categorical(self.yvalid) elif self.locker["data_type"] == "image_folder": # folders of train test pass # use keras flow_from_directory don't use for now because it looks for subfolders with folder name as different targets like horses/humans elif self.locker["data_type"] == "tabular": # concept of useful feature don't make sense for image problem # self.xtrain = self.xtrain[self.useful_features] # self.xvalid = self.xvalid[self.useful_features] # self.xtest = self.test[self.useful_features] # del self.test # gc.collect() prep_dict = { "SiMe": SimpleImputer(strategy="mean"), "SiMd": SimpleImputer(strategy="median"), "SiMo": SimpleImputer(strategy="mode"), "Ro": RobustScaler(), "Sd": StandardScaler(), "Mi": MinMaxScaler(), } for f in self.prep_list: if f in list(prep_dict.keys()): sc = prep_dict[f] self.xtrain = sc.fit_transform(self.xtrain) if self._state != "opt": self.xtest = sc.transform(self.xtest) elif f == "Lg": self.xtrain = pd.DataFrame( self.xtrain, columns=self.useful_features ) self.xvalid = pd.DataFrame( self.xvalid, columns=self.useful_features ) if self._state != "opt": self.xtest = pd.DataFrame(self.xtest, columns=self.useful_features) # xtest = pd.DataFrame(xtest, columns=useful_features) for col in self.useful_features: self.xtrain[col] = np.log1p(self.xtrain[col]) self.xvalid[col] = np.log1p(self.xvalid[col]) if self._state != "opt": self.xtest[col] = np.log1p(self.xtest[col]) # xtest[col] = np.log1p(xtest[col]) gc.collect() else: raise Exception(f"scaler {f} is invalid!") gc.collect() # create instances # enable below for multiclass problem # if self.model_name.startswith("k") and self.comp_type != "2class": # ## to one hot # self.ytrain = np_utils.to_categorical(self.ytrain) # self.yvalid = np_utils.to_categorical(self.yvalid) gc.collect() if self._state == "opt": # optimization specific work do here # only looking at feature importance while optimizing self.calculate_feature_importance = True self.calculate_permutation_feature_importance = False global_variables.counter = 0 # since we can't get trial no from optuna we are maintaining our own # run() can be called for each new exp so we initialize here rather than at init self.feature_importance_table = None # store the feature importances each trial wise # create optuna study study = optuna.create_study(direction=self._aim, study_name=self.model_name) study.optimize( lambda trial: self.obj(trial), n_trials=self.n_trials, ) # it tries 50 different values to find optimal hyperparameter if self.save_models == True: # let's save logs c = self.current_dict[ "current_exp_no" ] # optuna is called once in each exp so c will be correct save_pickle( f"../configs/configs-{self.locker['comp_name']}/logs/log_exp_{c}.pkl", self._log_table, ) self._log_table = None # save feature importance if self.calculate_feature_importance or self.calculate_permutation_feature_importance: # need to calculate feature importance save_pickle(f"../configs/configs-{self.locker['comp_name']}/feature_importance/feature_importance_e_{c}_opt.pkl",self.feature_importance_table) del self.feature_importance_table gc.collect() print("=" * 40) print("Best parameters found:") print(study.best_trial.value) self.params = study.best_trial.params # crete params to be used in seed print(study.best_trial.params) print("=" * 40) # later put conditions on whether to put seed or not if self.model_name == "lgr": del self.params["c"] # seed_mean, seed_std = self._seed_it() # generate seeds seed is now generate seperately gc.collect() check_memory_usage("run", self, 0) return study, self._random_state # , seed_mean, seed_std check_memory_usage("run", self, 0) if __name__ == "__main__": import optuna a = OptunaOptimizer() del a """ {'objective': 'binary', 'metric': 'binary_logloss', 'boosting': 'dart', 'learning_rate': 0.009238354429187228, 'seed': 241, 'num_leaves': 105, 'feature_fraction': 0.16728169509013685, 'bagging_freq': 9, 'bagging_fraction': 0.500294582036411, 'n_jobs': -1, 'lambda_l2': 3, 'min_data_in_leaf': 45} """

File no 24: /src-framework3/output.py
import os import sys from utils import * import pandas as pd import numpy as np class out: def __init__(self, fold_name, exp_no=-1,file_type="parquet"): self.exp_no = exp_no self.file_type = file_type # initialize rest with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: self.comp_name = i x.close() self.Table = load_pickle(f"../configs/configs-{self.comp_name}/Table.pkl") self.locker = load_pickle(f"../configs/configs-{self.comp_name}/locker.pkl") assert fold_name != "" self.fold_name = fold_name # self.my_folds = pd.read_csv(f"../configs/configs-{self.comp_name}/my_folds.csv") self.my_folds = pd.read_parquet( f"../input/input-{self.comp_name}/my_folds.parquet" ) # self.test = pd.read_csv(f"../configs/configs-{self.comp_name}/test.csv") self.test = pd.read_parquet(f"../input/input-{self.comp_name}/test.parquet") # keep sample to input # self.sample = pd.read_csv(f"../input/input-{self.comp_name}/sample.csv") self.sample = pd.read_parquet(f"../input/input-{self.comp_name}/sample.parquet") def dump(self, exp_no="--|--", file_type="--|--"): if exp_no != "--|--": self.exp_no = exp_no if file_type != "--|--": self.file_type = file_type if not self.fold_name.startswith("e"): if self.exp_no == -1: row_e = self.Table[self.Table.exp_no == list(self.Table.exp_no.values)[-1]] self.exp_no = row_e.exp_no.values[0] else: row_e = self.Table[self.Table.exp_no == self.exp_no] assert self.exp_no == row_e.exp_no.values[0] # --> assert self.bv = row_e.bv.values[0] # confirming we are predcting correct experiment: print(f"Output of Exp No {self.exp_no}, whoose bv is {self.bv}") self.exp_no = row_e.exp_no.values[0] self.level_no = row_e.level_no.values[0] # found the row # BOTTLENECK # which oof_fold_name to pull if self.fold_name not in list(row_e.oof_fold_name.values[0]): raise Exception(f"This experiment has no prediction of type {self.fold_name}!!") self.useful_features = [f"pred_e_{self.exp_no}_{self.fold_name}"] return_type = "numpy_array" self.optimize_on = None # just to make sure it is not called xtest, ordered_list_test = bottleneck_test(self.comp_name, self.useful_features, return_type) #val_idx, xtrain, xvalid, ytrain, yvalid, xtest = bottleneck_test(self.locker['comp_name'],self.useful_features, fold_name, self.optimize_on, self._state, return_type) #xtest.shape : (234522,1) #xtrain.shape: (224224,1) print(xtest.shape) print(xtest[:4,:]) try: # fold if self.locker["comp_name"] == "twistmnist": # special case self.sample[self.locker["target_name"]] = ( self.test[f"pred_l_{self.level_no}_e_{self.exp_no}"].values.astype( int ) + 10 ) else: # self.sample[self.locker["target_name"]] = self.test[ # f"pred_l_{self.level_no}_e_{self.exp_no}" # ].values self.sample[self.locker["target_name"]] = xtest # else: # use it when want hard class # self.sample[self.locker["target_name"]] = self.test[f"pred_l_{self.level_no}_e_{self.exp_no}"].values.astype(int) print("Done here") if self.comp_name == 'amzcomp1': self.sample = self.sample.rename(columns= {self.locker['target_name']: 'Time_taken (min)'}) if self.file_type == "parquet": self.sample.to_parquet( f"../working/{self.locker['comp_name']}_sub_e_{int(self.exp_no)}_{self.fold_name}.parquet", index=False ) else: self.sample.to_csv( f"../working/{self.locker['comp_name']}_sub_e_{int(self.exp_no)}_{self.fold_name}.csv", index=False ) #################### Below files are already dumped in the WORKING directory # # seed all # #d1 = pd.read_csv(f"../configs/configs-{self.comp_name}/sub_seed_exp_{self.exp_no}_l_{self.level_no}_all.csv") # d1 = pd.read_parquet(f"../configs/configs-{self.comp_name}/sub_seed_exp_{self.exp_no}_l_{self.level_no}_all.parquet") # if self.locker["comp_name"] == "twistmnist": # d1[self.locker["target_name"]] = d1[self.locker["target_name"]] + 10 # else: # d1[self.locker["target_name"]] = d1[self.locker["target_name"]] # if self.file_type == "parquet": # d1.to_parquet(f"../working/sub_seed_exp_{self.exp_no}_l_{self.level_no}_all.parquet", index=False) # else: # d1.to_csv(f"../working/sub_seed_exp_{self.exp_no}_l_{self.level_no}_all.csv", index=False) # # seed single # #d1 = pd.read_csv(f"../configs/configs-{self.comp_name}/sub_seed_exp_{self.exp_no}_l_{self.level_no}_single.csv") # d1 = pd.read_parquet(f"../configs/configs-{self.comp_name}/sub_seed_exp_{self.exp_no}_l_{self.level_no}_single.parquet") # if self.locker["comp_name"] == "twistmnist": # d1[self.locker["target_name"]] = d1[self.locker["target_name"]] + 10 # twistmnist # else: # d1[self.locker["target_name"]] = d1[self.locker["target_name"]] # if self.file_type == "parquet": # d1.to_parquet(f"../working/sub_seed_exp_{self.exp_no}_l_{self.level_no}_single.parquet", index=False) # else: # d1.to_csv(f"../working/sub_seed_exp_{self.exp_no}_l_{self.level_no}_single.csv", index=False) ############################################################################################## print() print(self.sample.head(2)) except: raise Exception(f"exp_no {self.exp_no} not found with fold name : {self.fold_name}!") else: # starts with e so ensemble self.useful_features = [f"{self.comp_name}_ens_{self.exp_no}"] # this file should be in working directory val = pd.read_parquet(f"../working/{self.useful_features[0]}.parquet") self.sample[self.locker["target_name"]] = val[self.locker["target_name"]] if self.comp_name == 'amzcomp1': self.sample = self.sample.rename(columns= {self.locker['target_name']: 'Time_taken (min)'}) try: if self.file_type == "parquet": self.sample.to_parquet( f"../working/{self.useful_features[0]}.parquet", index=False ) else: self.sample.to_csv( f"../working/{self.useful_features[0]}.csv", index=False ) except: raise Exception("ensemble not created!!") if __name__ == "__main__": # creates fold submission from a predicted experiment """ file_type: parquet:- when working on remote ssh so that it is easy to version it csv:- when working on notebook so that it is easy to make submission as csv """ for exp_no in [70]: # exp_no = ( # 7 # ) # -1 for last prediction i.e. last experiment so you must predict last experiment before doing output.py # need to add which fold prediction to output file_type = "csv" # "csv" fold_name = "fold5" # need to pass some fold name can't keep it empty o = out( fold_name, exp_no, file_type) o.dump() # for exp_no in [0,2,5,11,19,43]: # o= out(exp_no, file_type) # o.dump()

File no 25: /src-framework3/predict.py
from optuna_search import OptunaOptimizer from utils import * from custom_models import * from custom_classes import * from utils import * import os import gc import sys import pandas as pd import numpy as np from scipy import stats import ast # for literal #pd.set_option("display.max_columns", None) # for time """ Inference file """ import global_variables class predictor(OptunaOptimizer): def __init__(self, exp_no): self.exp_no = exp_no # initialize rest with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: self.comp_name = i x.close() self.Table = load_pickle(f"../configs/configs-{self.comp_name}/Table.pkl") self.locker = load_pickle(f"../configs/configs-{self.comp_name}/locker.pkl") if self.exp_no == -1: row_e = self.Table[self.Table.exp_no == list(self.Table.exp_no.values)[-1]] self.exp_no = row_e.exp_no.values[0] else: row_e = self.Table[self.Table.exp_no == self.exp_no] global_variables.exp_no = self.exp_no # setting for making dir in lgb self.model_name = row_e.model_name.values[0] self.params = row_e.bp.values[0] self.bv = row_e.bv.values[0] # confirming we are predcting correct experiment: print(f"Predicting Exp No {self.exp_no}, whoose bv is {self.bv}") if self.model_name == "lgr": del self.params["c"] self._random_state = row_e.random_state.values[0] self.with_gpu = row_e.with_gpu.values[0] self.features_list = row_e.features_list.values[0] self.prep_list = row_e.prep_list.values[0] self.metrics_name = row_e.metrics_name.values[0] self.level_no = row_e.level_no.values[0] self.useful_features = row_e.features_list.values[0] self.aug_type = row_e.aug_type.values[0] self._dataset = row_e._dataset.values[0] self.use_cutmix = row_e.use_cutmix.values[0] super().__init__( model_name=self.model_name, comp_type=self.locker["comp_type"], metrics_name=self.metrics_name, prep_list=self.prep_list, with_gpu=self.with_gpu, aug_type=self.aug_type, _dataset=self._dataset, use_cutmix=self.use_cutmix, ) # When we call super() It is like calling their init # so all the default initialization of parent class is made here # So we must manually change it here after doing super() # if we did it before calling super() it will be overwritten by parent init # Overrite exp_no of OptunaOptimizer since it takes exp_no from current_dict self.exp_no = exp_no if self.exp_no == -1: row_e = self.Table[self.Table.exp_no == list(self.Table.exp_no.values)[-1]] self.exp_no = row_e.exp_no.values[0] # fold specific work do here # only looking at feature importance while optimizing self.calculate_feature_importance = False self.calculate_permutation_feature_importance = False global_variables.counter = 0 # since we can't get trial no from optuna we are maintaining our own # run() can be called for each new exp so we initialize here rather than at init self.feature_importance_table = None # store the feature importances each trial wise # We can set manually on which fold_name to create oof predictions self.fold_name = row_e.opt_fold_name.values[0] # 'fold3' # --- sanity check [new_feat, old_feat, feat_title] # --------------- self.feat_dict = load_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl" ) #new_features = [f"pred_e_{self.exp_no}_{self.exp_no}"] #useful_features = self.useful_features def run_folds(self, fold_name="--|--" ): check_memory_usage("run folds started", self, 0) ###################################### # Memory uage # ###################################### tracemalloc.start() if fold_name not in ["", "--|--"]: # empty self.fold_name = fold_name self._state = "fold" """ IF WE DON'T DO SANITY CHECK THEN OLD PREDICTION WILL BE REPLACED WITH NEW PREDICTION FOR THE FEATURE_DICT, It will not create new key but replace old key and the value Use it not in init because new fold_name is made in run_folds function """ self.isRepetition() # image_path = f'../input/input-{self.locker["comp_name"]}/' + "train_img/" # my_folds = pd.read_csv(f"../configs/configs-{self.comp_name}/my_folds.csv") # # my_folds = pd.read_parquet( # f"../input/input-{self.comp_name}/my_folds.parquet" # ) # test = pd.read_csv(f"../configs/configs-{self.comp_name}/test.csv") scores = [] oof_prediction = {} test_predictions = [] print("Running folds:") # BOTTLENECK return_type = "numpy_array" # "numpy_array" # "tensor" # don't delete ordered_list test because it is used later by obj function for the sanity check print("Before Bottleneck:", len(self.useful_features)) self.xtest , self.ordered_list_test = bottleneck_test(self.locker['comp_name'], self.useful_features, return_type) print("After Bottleneck:", len(self.ordered_list_test)) if self.model_name.startswith("k"): self.xtest[np.isnan(self.xtest)] = 0 if self.model_name == "xgb": # for lgb no need # if we are using feature importance it is better to convert it to dataframe if self.calculate_feature_importance: self.xtest = pd.DataFrame(self.xtest, columns = self.ordered_list_test) self.xtest = xgb.DMatrix(self.xtest) for fold in range(self.locker['fold_dict'][self.fold_name]): #self.locker["no_folds"]): self.optimize_on = [fold] # setting on which to optimize #set it also as list It is much more flexible like this # select data: xtrain xvalid etc self.run( self.useful_features ) # don't delete variables of self here # since used here scores.append(self.obj("--|--")) if self.locker["comp_type"] == "multi_label": # 3 to 1 so elongate self.test_preds = coln_3_1(self.test_preds) self.valid_preds = coln_3_1(self.valid_preds) else: self.valid_preds = self.valid_preds[0] self.test_preds = self.test_preds[0] oof_prediction.update(dict(zip(self.val_idx, self.valid_preds))) # oof test_predictions.append(self.test_preds) ###################################### # gc collect # ###################################### del self.xtrain, self.xvalid, self.val_idx, self.ytrain, self.yvalid #, self.test don't delete xtest since called only once # just see the importance of deleting self.test # one thing we can try is instead of reading multiple times just read once from outside _ = gc.collect() check_memory_usage(f"fold #{fold}", self, 0) print() #------------------------------ # save feature importance if self.calculate_feature_importance or self.calculate_permutation_feature_importance: # need to calculate feature importance save_pickle(f"../configs/configs-{self.locker['comp_name']}/feature_importance/feature_importance_e_{self.exp_no}_fold.pkl",self.feature_importance_table) del self.feature_importance_table gc.collect() del self.xtest gc.collect() # delete xtest now # CHECK MEMORY USAGE JUST AFTER ALL FOLDS check_memory_usage("AFTER_ALL_FOLDS", self, 1) """ Note: for multi_label we can't save oof in my_folds.csv since multiple target columns are flattened to create submission file. """ # save oof predictions temp_valid_predictions = pd.DataFrame.from_dict( oof_prediction, orient="index" ).reset_index() temp_valid_predictions.columns = [ f"{self.locker['id_name']}", f"{self.locker['target_name']}" ] # if regression problem then rank it if self.locker["comp_type"] in [ "regression", "2class", ] and self.metrics_name in [ "auc", "auc_tf", ]: # auc takes rank s #print("checking") # quite sensitive point temp_valid_predictions[f"{self.locker['target_name']}"]= stats.rankdata(temp_valid_predictions.loc[:,f"{self.locker['target_name']}"]) final_test_predictions = [stats.rankdata(f) for f in test_predictions] del test_predictions gc.collect() else: # temp_valid_predictions already defined final_test_predictions = test_predictions.copy() del test_predictions gc.collect() # save oof predictions if self.locker["comp_type"] != "multi_label": # SPLIT SUBMISSIONS #-> a = a.sort_values(by=['customer_ID']) #temp_valid_predictions = np.array(temp_valid_predictions.sort_values(by=[self.locker['id_name']])["prediction"].values) temp_valid_predictions = np.array(temp_valid_predictions.sort_values(by=[self.locker['id_name']])[f"{self.locker['target_name']}"].values) # temp_valid_predictions = temp_valid_predictions.set_index('customer_ID') # temp_valid_predictions = temp_valid_predictions.sort_index().reset_index().values # pkl # it is 1D array save_pickle(f"../configs/configs-{self.locker['comp_name']}/oof_preds/oof_pred_e_{self.exp_no}_{self.fold_name}.pkl", temp_valid_predictions) # clear memory del temp_valid_predictions gc.collect() else: input("We have not found way to save multi-label problem yet!!") # SPLIT PREDICTIONS # save test predictions # mode is good for classification problem but not for regression problem if self.locker["comp_type"] in ["regression", "2class"]: # so we will use regression methods [ for now using 0.2] final_test_predictions = [ np.sum([0.2 * i for i in f], axis=0) for f in [final_test_predictions] ][0] #final_test_predictions = [np.mean(f, axis=0) for f in [final_test_predictions]][0] else: final_test_predictions = stats.mode(np.column_stack(final_test_predictions), axis=1)[0] print("Test preds") print(final_test_predictions) save_pickle(f"../configs/configs-{self.locker['comp_name']}/test_preds/test_pred_e_{self.exp_no}_{self.fold_name}.pkl", np.array(final_test_predictions)) del final_test_predictions gc.collect() # --------------- # feature name should be unique enough not to match with base features #new_features = [f"pred_e_{self.exp_no}_{self.fold_name}"] #useful_features = self.useful_features # -----------------------------update current dict self.current_dict["current_feature_no"] = ( self.current_dict["current_feature_no"] + 1 ) feat_no = self.current_dict["current_feature_no"] level_no = self.current_dict["current_level"] save_pickle( f"../configs/configs-{self.locker['comp_name']}/current_dict.pkl", self.current_dict, ) # -----------------------------dump feature dictionary feat_dict = load_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl" ) #-- # sanity check is already done that's why we are dumping is_it_repetition = False if f"exp_{self.exp_no}" in list(feat_dict.keys()): # experiment already predicted so append feature if f"pred_e_{self.exp_no}_{self.fold_name}" in list(feat_dict[f"exp_{self.exp_no}"][0]): #this feature already present no need to put again either raise error or let it overwrite print(f"This feature pred_e_{self.exp_no}_{self.fold_name} is already present no need to put again either raise error or let it overwrite") s = input("Type Y/y to overwrite or Type N/n to raise error.") if s.upper() == "N": raise Exception("Repeating feture") is_it_repetition = True print("Overwritting") else: feat_dict[f"exp_{self.exp_no}"][0] += [f"pred_e_{self.exp_no}_{self.fold_name}"] else: # first time so add feat_dict[f"exp_{self.exp_no}"] = [ [f"pred_e_{self.exp_no}_{self.fold_name}"], self.useful_features ] #--- # feat_dict[f"l_{level_no}_f_{feat_no}"] = [ # new_features, # useful_features, # f"exp_{self.exp_no}", # ] save_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl", feat_dict, ) # ----------------------- print("New features create:- ") print([f"pred_e_{self.exp_no}_{self.fold_name}"]) # ----------------------------- print("scores: ") print(scores) # ---- update table if not is_it_repetition: self.Table.loc[self.Table.exp_no == self.exp_no, "fold_mean"].values[0] += [np.mean(scores)] self.Table.loc[self.Table.exp_no == self.exp_no, "fold_std"].values[0] += [np.std(scores)] self.Table.loc[self.Table.exp_no == self.exp_no, "oof_fold_name"].values[0] += [self.fold_name] # pblb to be updated mannually # ---------------- dump table save_pickle( f"../configs/configs-{self.locker['comp_name']}/Table.pkl", self.Table ) gc.collect() check_memory_usage("run folds stop", self) tracemalloc.stop() def isRepetition(self): title_new = f"exp_{self.exp_no}" if title_new in list(self.feat_dict.keys()): # So this experiment is done but is that fold_name is used if f"pred_e_{self.exp_no}_{self.fold_name}" in self.feat_dict[title_new][0]: # pred_e_10_fold10, pred_e_10_fold5 raise Exception(f"This feature : pred_e_{self.exp_no}_{self.fold_name}, with title : {title_new} is already present in my_folds!") gc.collect() if __name__ == "__main__": #p = predictor(exp_no=-1) # last exp #p = predictor(exp_no=236) # last exp # more the no of folds bigger training size in each fold # fold_name = "fold5" #fold5" #"fold5" # "fold3" , "fold5", "fold10" , "fold20", "" # p.run_folds(fold_name) # del p # p = predictor(exp_no=3) # exp_4 #p.run_folds() for exp_no in [-1]: fold_name = "fold5" p = predictor(exp_no = exp_no) p.run_folds(fold_name) del p gc.collect()

File no 26: /src-framework3/preprocess.py
import pandas as pd import numpy as np import matplotlib.pyplot as plt import joblib from tqdm import tqdm import os def prep(): data_dir = "../input/input-bengali/" files_train = [f"train_image_data_{fid}.parquet" for fid in range(4)] for fname in files_train: F = os.path.join(data_dir, fname) df_train = pd.read_parquet(F) img_ids = df_train["image_id"].values img_array = df_train.iloc[:, 1:].values for idx in tqdm(range(len(df_train))): img_id = img_ids[idx] img = img_array[idx] joblib.dump(img, f"../input/input-bengali/train_images/{img_id}.pkl") if __name__ == "__main__": prep()

File no 27: /src-framework3/pull.py
import os import sys import pandas as pd import json def sync_(configs, models, src, input_): framework_name = "framework3" with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() print("comp name:", comp_name) # download data if configs: # configs #!kaggle datasets list try: os.mkdir(f"../configs/configs-{comp_name}/") print(f"configs-{comp_name} folder created.") except: print(f"configs-{comp_name} already exists.") os.system( f"kaggle datasets download raj401/configs-{comp_name} -p ../configs/configs-{comp_name}/ --unzip" ) print(f"configs-{comp_name} downloaded") if models: try: os.mkdir(f"../models/models-{comp_name}/") print(f"models-{comp_name} folder created.") except: print(f"models-{comp_name} already exists.") # models #!kaggle datasets download raj401/models-tmay -p ../models/models-tmay/ --unzip os.system( f"kaggle datasets download raj401/models-{comp_name} -p ../models/models-{comp_name}/ --unzip" ) print(f"models-{comp_name} downloaded") if src: try: os.mkdir(f"../src-{framework_name}/") print(f"src-{framework_name} folder created.") except: print(f"src-{framework_name} already exists.") # src # !kaggle datasets download raj401/src-{framework_name} -p ../src-{framework_name}/ --unzip os.system( f"kaggle datasets download raj401/src-{framework_name} -p ../src-{framework_name}/ --unzip" ) print(f"src-{framework_name} downloaded") if input_: try: os.mkdir(f"../input/input-{comp_name}/") print(f"input-{comp_name} folder created.") except: print(f"input-{comp_name} already exists.") # input #!kaggle datasets download raj401/input-tmay -p ../input/input-tmay/ --unzip os.system( f"kaggle datasets download raj401/input-{comp_name} -p ../input/input-{comp_name}/ --unzip" ) print(f"input-{comp_name} downloaded") if __name__ == "__main__": configs = True # configs models = False # models src = False # src_framework3 input_ = False sync_(configs, models, src, input_) print("Done")

File no 28: /src-framework3/push.py
import os import sys import pandas as pd import json from datetime import datetime def sync_(configs, models, src, working, input_, comment=""): framework_name = "framework3" with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() print("comp name:", comp_name) version_name = datetime.now().strftime("%Y%m%d-%H%M%S") if comment != "": version_name += "-" + comment print(f"Versioning at {version_name}") print("=" * 40) # upload data if configs: # configs #!kaggle datasets list try: os.system( f"kaggle datasets version -m {version_name} -p ../configs/configs-{comp_name}/ -r zip" ) print(f"configs-{comp_name} uploaded") except: print(f"configs-{comp_name} doesn't exists.") if models: # models try: os.system( f"kaggle datasets version -m {version_name} -p ../models/models-{comp_name}/ -r zip -q" ) print(f"models-{comp_name} uploaded") except: print(f"models-{comp_name} doesn't exists.") if src: # src try: os.system( f"kaggle datasets version -m {version_name} -p ../src-{framework_name}/" ) print(f"src-{framework_name} uploaded") except: print(f"src-{framework_name} doesn't exists.") if working: # src try: os.system(f"kaggle datasets version -m {version_name} -p ../working/") print(f"working uploaded") except: print(f"working doesn't exists.") if input_: try: os.system( f"kaggle datasets version -m {version_name} -p ../input/input-{comp_name}/ -r zip -q" ) print(f"input-{comp_name} uploaded") except: print(f"input-{comp_name} doesn't exists.") if __name__ == "__main__": configs = False # configs models = False # models src = True # src_framework3 working = False # working input_ = False # we never push input_ twice [already pushed once] comment = "amz_reg_tabnet_src" # don't keep space sync_(configs, models, src, working, input_, comment) print("Done")

File no 29: /src-framework3/ref.txt
amzcomp1

File no 30: /src-framework3/removed_dup_in_dict.py
from settings import * if __name__ == "__main__": a = amzcomp1_settings() b = list(a.auto_filtered_features.keys()) print(b[::-1])

File no 31: /src-framework3/requirements.txt
numpy scipy pandas matplotlib scikit-learn

File no 32: /src-framework3/run.sh
# /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip uninstall optuna # /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip uninstall tensorflow # /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip uninstall catboost # /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip uninstall timm # /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip uninstall pretrainedmodels # /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip uninstall -Iv tez==0.6.0 # /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip uninstall torchcontrib # /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip uninstall iterative-stratification /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip install tensorflow /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip install --upgrade pip /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip install optuna /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip install catboost /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip install timm /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip install pretrainedmodels /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip install -Iv tez==0.6.0 /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip install torchcontrib /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip install iterative-stratification #/home/pramit_mazumdar/anaconda3/envs/AKR_env3/bin/pip install iterative-stratification # /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip uninstall lightgbm --install-option=--cuda #/home/pramit_mazumdar/anaconda3/envs/AKR_env2/bin/pip install pytorch-tabnet """ what worked for tensorflow issue: pip uninstall tensorflow pip install tensorflow-gpu pip uninstall tensorflow-gpu pip uninstall tensorflow-gpu --upgrade """ # # /home/pramit_mazumdar/anaconda3/envs/AKR_env2/bin/pip install optuna # # /home/pramit_mazumdar/anaconda3/envs/AKR_env2/bin/pip install kaggle --upgrade # # conda install -c conda-forge py-xgboost-gpu # # /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip install https://s3-us-west-2.amazonaws.com/xgboost-nightly-builds/xgboost-1.4.0_SNAPSHOT%2B4224c08cacceba3f83f90e387c07aa6205a83bfa-py3-none-manylinux2010_x86_64.whl # # /home/pramit_mazumdar/anaconda3/envs/AKR_env/bin/pip install xgboost-0.81-py2.py3-none-manylinux1_x86_64.zip

File no 33: /src-framework3/seed_it.py
from optuna_search import OptunaOptimizer from utils import * from custom_models import * from custom_classes import * from utils import * import os import gc import sys import pandas as pd import numpy as np from scipy import stats from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler from sklearn.preprocessing import RobustScaler """ Seed file Trains of full dataset for n different seeds and Outputs two files Single seed ( output based on one of the seed) all seed (ensemble of all the seeds) """ class seeds(OptunaOptimizer): def __init__(self, exp_no): self.exp_no = exp_no # initialize rest with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: self.comp_name = i x.close() self.Table = load_pickle(f"../configs/configs-{self.comp_name}/Table.pkl") self.locker = load_pickle(f"../configs/configs-{self.comp_name}/locker.pkl") if self.exp_no == -1: row_e = self.Table[self.Table.exp_no == list(self.Table.exp_no.values)[-1]] self.exp_no = row_e.exp_no.values[0] else: row_e = self.Table[self.Table.exp_no == self.exp_no] self.model_name = row_e.model_name.values[0] self.params = row_e.bp.values[0] self.bv = row_e.bv.values[0] # confirming we are predcting correct experiment: print(f"Predicting Exp No {self.exp_no}, whoose bv is {self.bv}") if self.model_name == "lgr": del self.params["c"] self._random_state = row_e.random_state.values[0] # This value will be changed self.with_gpu = row_e.with_gpu.values[0] self.features_list = row_e.features_list.values[0] self.prep_list = row_e.prep_list.values[0] self.metrics_name = row_e.metrics_name.values[0] self.level_no = row_e.level_no.values[0] self.useful_features = row_e.features_list.values[0] self.aug_type = row_e.aug_type.values[0] self._dataset = row_e._dataset.values[0] self.use_cutmix = row_e.use_cutmix.values[0] super().__init__( model_name=self.model_name, comp_type=self.locker["comp_type"], metrics_name=self.metrics_name, prep_list=self.prep_list, with_gpu=self.with_gpu, aug_type=self.aug_type, _dataset=self._dataset, use_cutmix=self.use_cutmix, ) # When we call super() It is like calling their init # so all the default initialization of parent class is made here # So we must manually change it here after doing super() # if we did it before calling super() it will be overwritten by parent init # Overrite exp_no of OptunaOptimizer since it takes exp_no from current_dict self.exp_no = exp_no if self.exp_no == -1: row_e = self.Table[self.Table.exp_no == list(self.Table.exp_no.values)[-1]] self.exp_no = row_e.exp_no.values[0] # --- sanity check [new_feat, old_feat, feat_title] # --------------- self.feat_dict = load_pickle( f"../configs/configs-{self.locker['comp_name']}/features_dict.pkl" ) useful_features = self.useful_features # It's ok to run seed any no of time but it's not ok to run predict # so sanity check in predict but not in seed_it def run_seeds(self): check_memory_usage("run seeds started", self, 0) ###################################### # Memory uage # ###################################### tracemalloc.start() # --------------------------------------> print("SEEDING") self._state = "seed" self.generate_random_no() no_seeds = 5 # 20 #3 random_list = np.random.randint(1, 1000, no_seeds) # 100 print(f"Running {no_seeds} seeds!") """ Use full train set and test set. call it train and valid """ # --> test set # read only what is necessary # self.test = pd.read_csv(f"../configs/configs-{self.locker['comp_name']}/" + "test.csv")[self.useful_features + [ self.locker["id_name"]] ] # self.test = pd.read_parquet( # f"../input/input-{self.locker['comp_name']}/" + "test.parquet", # columns=self.useful_features + [self.locker["id_name"]], # ) # [self.useful_features + [ self.locker["id_name"]] ] # self.test[self.locker["target_name"]] = 0.0 # # Create Folds since deleted in run # # self.my_folds = pd.read_csv(f"../configs/configs-{self.locker['comp_name']}/my_folds.csv")[ [ self.locker["id_name"], self.locker["target_name"]] + self.useful_features ] # self.my_folds = pd.read_parquet( # f"../input/input-{self.locker['comp_name']}/my_folds.parquet", # columns=[self.locker["id_name"], self.locker["target_name"]] # + self.useful_features, # ) # [ [ self.locker["id_name"], self.locker["target_name"]] + self.useful_features ] # if not multi_label # keep sample to input # self.sample = pd.read_csv( # f"../input/input-{self.locker['comp_name']}/" + "sample.csv" # ) # # self.sample = pd.read_parquet( # f"../input/input-{self.locker['comp_name']}/" + "sample.parquet" # ) # if self.locker["comp_type"] == "multi_label": # self.sample = self.test.copy() # temp # else: # self.sample = pd.read_csv( # f"../input/input-{self.locker['comp_name']}/" + "sample.csv" # ) # BOTTLENECK return_type = "numpy_array" self.optimize_on = None # just to make sure it is not called fold_name = "fold_check" self.val_idx, self.xtrain, self.xvalid, self.ytrain, self.yvalid, self.ordered_list_train = bottleneck(self.locker['comp_name'],self.useful_features, fold_name, self.optimize_on, self._state, return_type) self.xvalid = None self.yvalid = None self.val_idx = None self.xtest, self.ordered_list_test = bottleneck_test(self.locker['comp_name'], self.useful_features, return_type) # sanity check: for i,j in zip(self.ordered_list_test, self.ordered_list_train): if i != j: raise Exception(f"Features don't correspond in test - train {i},{j}") self.ordered_list_test = None self.ordered_list_train = None # print("self.xtrain.shape, self.ytrain.shape, self.xtest.shape") # print(self.xtrain.shape, self.ytrain.shape, self.xtest.shape) if self.locker["data_type"] == "image_path": image_path = f"../input/input-{self.locker['comp_name']}/" + "train_img/" # test_path = f"../input/input-{self.locker['comp_name']}/" + "test_img/" if self.model_name in ["tez1", "tez2", "pretrained"]: # now implemented for pytorch print("one") # use pytorch self.train_image_paths = [ os.path.join(image_path, str(x)) for x in self.my_folds[self.locker["id_name"]].values # => ] print("two") self.valid_image_paths = [ os.path.join(image_path, str(x)) for x in self.my_folds[self.locker["id_name"]].values # => ] print("three") self.ytrain = self.my_folds[self.locker["target_name"]].values # => self.yvalid = self.my_folds[self.locker["target_name"]].values # => # ------------------ prep test dataset # self.test_image_paths = [ # os.path.join( # test_path, str(x) # ) # f"../input/input-{self.locker['comp_name']}/" + "test_img/" + x # for x in self.sample[self.locker["id_name"]].values # ] print("four") # fake targets # self.test_targets = self.sample[ # self.locker["target_name"] # ].values # dfx_te.digit_sum.values print("five") if self._dataset in [ "BengaliDataset", ]: self.train_dataset = BengaliDataset( # train_dataset image_paths=self.train_image_paths, targets=self.ytrain, img_height=128, img_width=128, transform=self.train_aug, ) print("six") self.valid_dataset = BengaliDataset( # train_dataset image_paths=self.valid_image_paths, targets=self.yvalid, img_height=128, img_width=128, transform=self.valid_aug, ) print("seven") # already defined # self.test_dataset = BengaliDataset( # train_dataset # image_paths=self.test_image_paths, # targets=self.test_targets, # img_height = 128, # img_width = 128, # transform=self.valid_aug, # ) print("eight") # now implemented for pytorch # Can make our own custom dataset.. Note tez has dataloader inside the model so don't make else: # imageDataset self.train_dataset = ImageDataset( # train_dataset image_paths=self.train_image_paths, targets=self.ytrain, augmentations=self.aug, ) self.valid_dataset = ImageDataset( # valid_dataset image_paths=self.valid_image_paths, targets=self.yvalid, augmentations=self.aug, ) # self.test_dataset = ImageDataset( # image_paths=self.test_image_paths, # targets=self.test_targets, # augmentations=self.aug, # ) elif self.model_name in ["k1", "k2", "k3"]: # now implemented for keras # use keras flow_from_dataframe train_datagen = ImageDataGenerator(rescale=1.0 / 255) valid_datagen = ImageDataGenerator(rescale=1.0 / 255) if self.use_cutmix != True: self.train_dataset = train_datagen.flow_from_dataframe( dataframe=self.my_folds, directory=image_path, target_size=(28, 28), # images are resized to (28,28) x_col=self.locker["id_name"], y_col=self.locker["target_name"], batch_size=32, seed=42, shuffle=True, class_mode="categorical", # "binary" ) elif self.use_cutmix == True: train_datagen1 = train_datagen.flow_from_dataframe( dataframe=self.my_folds, directory=image_path, target_size=(28, 28), # images are resized to (28,28) x_col=self.locker["id_name"], y_col=self.locker["target_name"], batch_size=32, seed=42, shuffle=True, # Required for cutmix class_mode="categorical", # "binary" ) train_datagen2 = train_datagen.flow_from_dataframe( dataframe=self.my_folds, directory=image_path, target_size=(28, 28), # images are resized to (28,28) x_col=self.locker["id_name"], y_col=self.locker["target_name"], batch_size=32, seed=42, shuffle=True, # Required for cutmix class_mode="categorical", # "binary" ) self.train_dataset = CutMixImageDataGenerator( generator1=train_generator1, generator2=train_generator2, img_size=(28, 28), batch_size=32, ) self.valid_dataset = valid_datagen.flow_from_dataframe( dataframe=self.my_folds, directory=image_path, target_size=(28, 28), # images are resized to (28,28) x_col=self.locker["id_name"], y_col=self.locker["target_name"], batch_size=32, seed=42, shuffle=True, class_mode="categorical", # "binary" ) test_datagen = ImageDataGenerator(rescale=1.0 / 255.0) test_generator = test_datagen.flow_from_dataframe( dataframe=self.test, directory=test_path, target_size=(28, 28), # images are resized to (28,28) x_col=self.locker["id_name"], y_col=None, batch_size=32, seed=42, shuffle=True, class_mode="None", # "binary" ) elif self.locker["data_type"] == "image_df": # here we create filtered_features from useful_features # self.test = pd.read_csv( # f"../configs/configs-{self.locker['comp_name']}/" + "test.csv" # ) self.test = pd.read_parquet( f"../input/input-{self.locker['comp_name']}/" + "test.parquet" ) self.yvalid = self.my_folds[self.locker["target_name"]] self.ytrain = self.my_folds[self.locker["target_name"]] # create fake labels self.test[self.locker["target_name"]] = 0.0 if self._dataset == "DigitRecognizerDataset": # DigitRecognizerDataset self.train_dataset = DigitRecognizerDataset( df=self.my_folds[ self.filtered_features + [self.locker["target_name"]] ], augmentations=self.train_aug, model_name=self.model_name, ) self.valid_dataset = DigitRecognizerDataset( df=self.my_folds[ self.filtered_features + [self.locker["target_name"]] ], augmentations=self.valid_aug, model_name=self.model_name, ) self.test_dataset = DigitRecognizerDataset( df=self.test[self.filtered_features + [self.locker["target_name"]]], augmentations=self.valid_aug, model_name=self.model_name, ) elif self._dataset == "BengaliDataset": # now implemented for pytorch # Can make our own custom dataset.. Note tez has dataloader inside the model so don't make self.train_dataset = BengaliDataset( # train_dataset csv=self.my_folds[ self.filtered_features + [self.locker["target_name"]] ], img_height=28, img_width=28, transform=self.train_aug, ) self.valid_dataset = BengaliDataset( # valid_dataset csv=self.my_folds[ self.filtered_features + [self.locker["target_name"]] ], img_height=28, img_width=28, transform=self.valid_aug, ) self.test_dataset = BengaliDataset( df=self.test[self.filtered_features + [self.locker["target_name"]]], img_height=28, img_width=28, augmentations=self.valid_aug, ) elif self.locker["data_type"] == "image_folder": # folders of train test pass # use keras flow_from_directory don't use for now because it looks for subfolders with folder name as different targets like horses/humans elif self.locker["data_type"] == "tabular": # concept of useful feature don't make sense for image problem # self.xtrain = self.my_folds[self.useful_features] # self.xvalid = self.my_folds[self.useful_features] # self.yvalid = self.my_folds[self.locker["target_name"]] # self.ytrain = self.my_folds[self.locker["target_name"]] # self.xtest = self.test[self.useful_features] # del self.test # del self.my_folds # gc.collect() prep_dict = { "SiMe": SimpleImputer(strategy="mean"), "SiMd": SimpleImputer(strategy="median"), "SiMo": SimpleImputer(strategy="mode"), "Ro": RobustScaler(), "Sd": StandardScaler(), "Mi": MinMaxScaler(), } for f in self.prep_list: if f in list(prep_dict.keys()): sc = prep_dict[f] self.xtrain = sc.fit_transform(self.xtrain) if self._state != "seed": # understand carefully it is correct self.xvalid = sc.transform(self.xvalid) if self._state != "opt": self.xtest = sc.transform(self.xtest) elif f == "Lg": self.xtrain = pd.DataFrame( self.xtrain, columns=self.useful_features ) self.xvalid = pd.DataFrame( self.xvalid, columns=self.useful_features ) self.xtest = pd.DataFrame(self.xtest, columns=self.useful_features) # xtest = pd.DataFrame(xtest, columns=useful_features) for col in self.useful_features: self.xtrain[col] = np.log1p(self.xtrain[col]) self.xvalid[col] = np.log1p(self.xvalid[col]) self.xtest[col] = np.log1p(self.xtest[col]) # xtest[col] = np.log1p(xtest[col]) gc.collect() else: raise Exception(f"scaler {f} is invalid!") gc.collect() # create instances if self.model_name.startswith("k") and self.comp_type != "2class": ## to one hot self.ytrain = np_utils.to_categorical(self.ytrain) self.yvalid = np_utils.to_categorical(self.yvalid) scores = [] if self.locker["comp_type"] == "multi_label": final_test_predictions = [[], [], []] else: final_test_predictions = [[]] for i, rn in enumerate(random_list): print() print(f"Seed no: {i}, seed: {rn}") self._random_state = rn # run an algorithm for 100 times scores.append(self.obj("--no-trial--")) for i, f in enumerate(final_test_predictions): final_test_predictions[i].append(self.test_preds[i]) gc.collect() gc.collect() # if regression problem then rank it if self.locker["comp_type"] in [ "regression", "2class", ] and self.metrics_name in [ "auc", "auc_tf", ]: # auc takes rank s # no oof while creating seed submissions # temp_valid_prediction[f"pred_l_{self.current_dict['current_level']}_e_{self.exp_no}"] = [stats.rankdata(f) for f in temp_valid_prediction[f"pred_l_{self.current_dict['current_level']}_e_{self.exp_no}"]] # [[p1, p2, p3]] for i, f in enumerate(final_test_predictions): final_test_predictions[i] = [ stats.rankdata(f) for f in final_test_predictions[i] ] gc.collect() if self.locker["comp_type"] == "multi_label": # convert multi column target to single column # input=> 3 columns , output=> 1 column single_column = [np.array(f[0]) for f in final_test_predictions] multiple_columns = [ stats.mode(np.column_stack(f), axis=1)[0] for f in final_test_predictions ] # keep sample to input # sample_real = pd.read_csv(f"../input/input-{self.locker['comp_name']}/sample.csv") sample_real = pd.read_parquet( f"../input/input-{self.locker['comp_name']}/sample.parquet" ) sample_real["target"] = coln_3_1(single_column) # sample_real.to_csv( # f"../configs/configs-{self.locker['comp_name']}/sub_seed_exp_{self.current_dict['current_exp_no']}_l_{self.current_dict['current_level']}_single.csv", # index=False, # ) sample_real.to_parquet( f"../working/{self.locker['comp_name']}_sub_e_{int(self.exp_no)}_single.parquet", index=False, ) sample_real["target"] = coln_3_1(multiple_columns) # sample_real.to_csv( # f"../configs/configs-{self.locker['comp_name']}/sub_seed_exp_{self.current_dict['current_exp_no']}_l_{self.current_dict['current_level']}_all.csv", # index=False, # ) sample_real.to_parquet( f"../working/{self.locker['comp_name']}_sub_e_{int(self.exp_no)}_all.parquet", index=False, ) else: self.sample = pd.read_parquet(f"../input/input-{self.locker['comp_name']}/" + "sample.parquet") self.sample[self.locker["target_name"]] = [ np.array(f[0]) for f in final_test_predictions ][0] # self.sample.to_csv( # f"../configs/configs-{self.locker['comp_name']}/sub_seed_exp_{self.current_dict['current_exp_no']}_l_{self.current_dict['current_level']}_single.csv", # index=False, # ) self.sample.to_parquet( f"../working/{self.locker['comp_name']}_sub_e_{int(self.exp_no)}_single.parquet", index=False, ) # mode is good for classification proble but not for regression problem if self.locker["comp_type"] in ["regression", "2class"]: # so we will use regression methods for i, f in enumerate(final_test_predictions): final_test_predictions[i] = [ 0.2 * f for f in final_test_predictions[i] ] gc.collect() self.sample[self.locker["target_name"]] = np.sum( np.array(final_test_predictions[0]), axis=0 ) else: self.sample[self.locker["target_name"]] = [ stats.mode(np.column_stack(f), axis=1)[0] for f in final_test_predictions ][0] # self.sample.to_csv( # f"../configs/configs-{self.locker['comp_name']}/sub_seed_exp_{self.current_dict['current_exp_no']}_l_{self.current_dict['current_level']}_all.csv", # index=False, # ) self.sample.to_parquet( f"../working/{self.locker['comp_name']}_sub_e_{int(self.exp_no)}_all.parquet", index=False, ) check_memory_usage("seed_it", self, 0) # ---- update table # self.Table.loc[self.Table.exp_no == self.exp_no, "fold_mean"] = np.mean(scores) # self.Table.loc[self.Table.exp_no == self.exp_no, "fold_std"] = np.std(scores) self.Table.loc[self.Table.exp_no == self.exp_no, "seed_mean"] = np.mean(scores) self.Table.loc[self.Table.exp_no == self.exp_no, "seed_std"] = np.std(scores) # pblb to be updated mannually # ---------------- dump table save_pickle( f"../configs/configs-{self.locker['comp_name']}/Table.pkl", self.Table ) gc.collect() check_memory_usage("run seed stop", self) tracemalloc.stop() def isRepetition(self, gen_features, old_features, feat_title): # self.curr for key, value in self.feat_dict.items(): f1, f2, ft = value if f1 == gen_features and f2 == old_features and ft == feat_title: raise Exception("This feature is already present in my_folds!") gen_features_modified = gen_features old_features_modified = old_features f1_modified = f1 f2_modified = f2 if f2 == 0: # from base pass elif len(f1[0].split("_")[0]) < 5 or ( f1[0].split("_")[0][0] == "l" and f1[0].split("_")[0][2] == "f" ): # originate from base so f2 can't be split f1_modified = ["_".join(f.split("_")[2:]) for f in f1_modified] gen_features_modified = [ "_".join(f.split("_")[2:]) for f in gen_features_modified ] else: f2_modified = ["_".join(f.split("_")[2:]) for f in f2_modified] old_features_modified = [ "_".join(f.split("_")[2:]) for f in old_features_modified ] f1_modified = ["_".join(f.split("_")[2:]) for f in f1_modified] gen_features_modified = [ "_".join(f.split("_")[2:]) for f in gen_features_modified ] if ( f1_modified == gen_features_modified and f2_modified == old_features_modified and ft == feat_title ): raise Exception("This feature is already present in my_folds!") gc.collect() if __name__ == "__main__": #s = seeds(exp_no=-1) # last exp s = seeds(exp_no=44) s.run_seeds() del s # p = predictor(exp_no=3) # exp_4 # p.run_folds()

File no 34: /src-framework3/settings.py
Framework3/src-framework3/settings.py at master · ar8372/Framework3 · GitHub Skip to content Toggle navigation Sign up In this repository All GitHub ↵ Jump to ↵ No suggested jump to results In this repository All GitHub ↵ Jump to ↵ In this user All GitHub ↵ Jump to ↵ In this repository All GitHub ↵ Jump to ↵ Sign in Sign up You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. ar8372 / Framework3 Public Notifications Fork 0 Star 0 Permalink master Switch branches/tags Branches Tags Name already in use A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch? Framework3/src-framework3/settings.py Go to file Go to file T Go to line L Copy path Copy permalink This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository. Cannot retrieve contributors at this time 1.87 MB Download Open with Desktop Download Delete file View raw (Sorry about that, but we can’t show files that are this big right now.) Go You can’t perform that action at this time.

File no 35: /src-framework3/show_importance.py
from utils import * import os pd.set_option("display.max_rows", None) import matplotlib.pyplot as plt import seaborn as sns class Importance: def __init__(self, exp_no): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() self.comp_name = comp_name self.feature_importance_table = None self.log_table = None assert exp_no >= 0 # pull data try: self.feature_importance_table = load_pickle(f"../configs/configs-{self.comp_name}/feature_importance/feature_importance_e_{exp_no}.pkl") except: try: self.feature_importance_table = load_pickle(f"../configs/configs-{self.comp_name}/feature_importance/feature_importance_e_{exp_no}_fold.pkl") print(self.feature_importance_table.head(3)) print("got") except: raise Exception("Feature importance is not saved!!") # fill nan self.feature_importance_table.fillna(0,inplace=True) try: self.log_table = load_pickle(f"../configs/configs-{self.comp_name}/logs/log_exp_{exp_no}.pkl") except: raise Exception("Log is not saved!!") self.scores = list(self.log_table.trial_score.astype(float)) print("Trial Scores:- ") print(self.scores) print("---+"*30) print("Log Table") print(self.log_table) print("---+"*30) print("Feature Importance") print(self.feature_importance_table.head(2)) def show(self,technique = "weighted_mean", top= 20, threshold=100, direction="maximize", pick = None, type_importance="opt", base_features=[]): filtered_trials = list(self.log_table.sort_values(by=['trial_score'], axis=0, ascending = False).index.values) # if pick is not none if pick is not None: if technique == "weighted_mean": # time to pick trials filtered_trials = list(self.log_table.sort_values(by=['trial_score'], axis=0, ascending = False).head(pick).index.values) # remove extra trials self.log_table = self.log_table.loc[filtered_trials] self.scores = list(self.log_table.trial_score.astype(float)) self.feature_importance_table = self.feature_importance_table.iloc[:,[0]+ [i+1 for i in filtered_trials]] elif type_importance=="fold" and technique == "mean": filtered_trials = [0,1,2,3,4] self.feature_importance_table = self.feature_importance_table.iloc[:,[0]+ [i+1 for i in filtered_trials]] if technique == "mean": self.feature_importance_table[technique]= self.feature_importance_table.drop("feature", axis=1).mean(axis=1) if direction == "minimize": self.feature_importance_table= self.feature_importance_table.sort_values(by=[technique], axis=0, ascending = True) else: self.feature_importance_table= self.feature_importance_table.sort_values(by=[technique], axis=0, ascending = False) print() print(self.feature_importance_table.head(10)) if technique == "bagging": L = self.feature_importance_table.drop("feature", axis=1) L = L<0 L = L.sum(axis=1) self.feature_importance_table[technique] = L self.feature_importance_table= self.feature_importance_table.sort_values(by=[technique], axis=0, ascending = False) print(self.feature_importance_table.head(3)) #self.feature_importance_table[technique] = self.feature_importance_table[L>= 1] if technique == "weighted_mean": val = [] for col,w in zip(self.feature_importance_table.drop("feature", axis=1).columns, self.scores): val.append(self.feature_importance_table[col]*w/sum(self.scores) ) self.feature_importance_table[technique] = np.sum(np.array(val), axis=0) if direction == "minimize": self.feature_importance_table= self.feature_importance_table.sort_values(by=[technique], axis=0, ascending = True) else: self.feature_importance_table= self.feature_importance_table.sort_values(by=[technique], axis=0, ascending = False) if technique == "best": ind = self.scores.index(max(self.scores)) useful_cols = self.feature_importance_table.drop("feature", axis=1).columns.values best_col = useful_cols[ind] print("best feature:", best_col) val = self.feature_importance_table[best_col] self.feature_importance_table[technique] = val if direction == "minimize": self.feature_importance_table= self.feature_importance_table.sort_values(by=[technique], axis=0, ascending = True) else: self.feature_importance_table= self.feature_importance_table.sort_values(by=[technique], axis=0, ascending = False) try: val= self.feature_importance_table[["feature", technique]] except: print("couldn't") val= self.feature_importance_table[["feature", technique]] print("---+"*30) print("VALUE") #print(val) print() print([i.round(2) for i in val[technique].values]) print(f"originally {val.shape[0]} features.") print("---+"*30) if top is not None: print(f"Top {top} features") print() if base_features is not None: filtered_feat = list(set(list(val.iloc[:top,:].feature))- set(base_features)) print("Original length:", len(list(set(list(val.iloc[:top,:].feature))))) print("After filter:", len(filtered_feat)) print(filtered_feat ) else: print(list(val.iloc[:top,:].feature)) elif threshold is not None: # threshold print(f"Threshold of {threshold} features") if technique == "bagging": l = list(val.feature) val = val[val[technique] >= threshold] print([i.round(4) for i in val[technique].values]) print(f"after filter: {val.shape[0]} features.") print(f"left features:", list(set(l)- set(val.feature))) elif direction == "maximize": val = val[val[technique]>= threshold] print([i.round(4) for i in val[technique].values]) print(f"after filter: {val.shape[0]} features.") else: # minimize val = val[val[technique]<= threshold] print([i.round(4) for i in val[technique].values]) print(f"after filter {val.shape[0]} features.") if base_features is not None: filtered_feat = list(set(val.feature)- set(base_features)) print("Original length:", len(val.feature)) print("After filter:", len(filtered_feat)) print(filtered_feat ) else: print(list(val.feature)) else: if base_features is not None: filtered_feat = list(set(val.feature)- set(base_features)) print("Original length:", len(val.feature)) print("After filter:", len(filtered_feat)) print(filtered_feat ) else: print(list(val.feature)) print("---+"*30) def give(self,technique = "weighted_mean", top= 20, threshold=100, direction="maximize", pick = None, type_importance="opt", base_features=[]): filtered_trials = list(self.log_table.sort_values(by=['trial_score'], axis=0, ascending = False).index.values) # if pick is not none if pick is not None: if technique == "weighted_mean": # time to pick trials filtered_trials = list(self.log_table.sort_values(by=['trial_score'], axis=0, ascending = False).head(pick).index.values) # remove extra trials self.log_table = self.log_table.loc[filtered_trials] self.scores = list(self.log_table.trial_score.astype(float)) self.feature_importance_table = self.feature_importance_table.iloc[:,[0]+ [i+1 for i in filtered_trials]] elif type_importance=="fold" and technique == "mean": filtered_trials = [0,1,2,3,4] self.feature_importance_table = self.feature_importance_table.iloc[:,[0]+ [i+1 for i in filtered_trials]] if technique == "mean": self.feature_importance_table[technique]= self.feature_importance_table.drop("feature", axis=1).mean(axis=1) if direction == "minimize": self.feature_importance_table= self.feature_importance_table.sort_values(by=[technique], axis=0, ascending = True) else: self.feature_importance_table= self.feature_importance_table.sort_values(by=[technique], axis=0, ascending = False) print() print(self.feature_importance_table.head(10)) if technique == "bagging": L = self.feature_importance_table.drop("feature", axis=1) L = L<0 L = L.sum(axis=1) self.feature_importance_table[technique] = L self.feature_importance_table= self.feature_importance_table.sort_values(by=[technique], axis=0, ascending = False) print(self.feature_importance_table.head(3)) #self.feature_importance_table[technique] = self.feature_importance_table[L>= 1] if technique == "weighted_mean": val = [] for col,w in zip(self.feature_importance_table.drop("feature", axis=1).columns, self.scores): val.append(self.feature_importance_table[col]*w/sum(self.scores) ) self.feature_importance_table[technique] = np.sum(np.array(val), axis=0) if direction == "minimize": self.feature_importance_table= self.feature_importance_table.sort_values(by=[technique], axis=0, ascending = True) else: self.feature_importance_table= self.feature_importance_table.sort_values(by=[technique], axis=0, ascending = False) if technique == "best": ind = self.scores.index(max(self.scores)) useful_cols = self.feature_importance_table.drop("feature", axis=1).columns.values best_col = useful_cols[ind] print("best feature:", best_col) val = self.feature_importance_table[best_col] self.feature_importance_table[technique] = val if direction == "minimize": self.feature_importance_table= self.feature_importance_table.sort_values(by=[technique], axis=0, ascending = True) else: self.feature_importance_table= self.feature_importance_table.sort_values(by=[technique], axis=0, ascending = False) try: val= self.feature_importance_table[["feature", technique]] except: print("couldn't") val= self.feature_importance_table[["feature", technique]] print("---+"*30) print("VALUE") #print(val) print() print([i.round(2) for i in val[technique].values]) print(f"originally {val.shape[0]} features.") print("---+"*30) if top is not None: print(f"Top {top} features") print() if base_features is not None: filtered_feat = list(set(list(val.iloc[:top,:].feature))- set(base_features)) print("Original length:", len(list(set(list(val.iloc[:top,:].feature))))) print("After filter:", len(filtered_feat)) print(filtered_feat ) return filtered_feat else: print(list(val.iloc[:top,:].feature)) return list(val.iloc[:top,:].feature) elif threshold is not None: # threshold print(f"Threshold of {threshold} features") if technique == "bagging": l = list(val.feature) val = val[val[technique] >= threshold] print([i.round(4) for i in val[technique].values]) print(f"after filter: {val.shape[0]} features.") print(f"left features:", list(set(l)- set(val.feature))) elif direction == "maximize": val = val[val[technique]>= threshold] print([i.round(4) for i in val[technique].values]) print(f"after filter: {val.shape[0]} features.") else: # minimize val = val[val[technique]<= threshold] print([i.round(4) for i in val[technique].values]) print(f"after filter {val.shape[0]} features.") if base_features is not None: filtered_feat = list(set(val.feature)- set(base_features)) print("Original length:", len(val.feature)) print("After filter:", len(filtered_feat)) print(filtered_feat ) return filtered_feat else: print(list(val.feature)) return list(val.feature) else: if base_features is not None: filtered_feat = list(set(val.feature)- set(base_features)) print("Original length:", len(val.feature)) print("After filter:", len(filtered_feat)) print(filtered_feat ) return filtered_feat else: print(list(val.feature)) return list(val.feature) print("---+"*30) from settings import * if __name__ == "__main__": """ best: picke features from beast trial mean: take simple mean of all the trials weighted mean: take weighted mean based on the score top: 20 filters top 20 threshold: 121 removes all whoose value is less than 121 """ exp_no = 142 #-1 120, 122, 127 direction = "minimize" technique = "bagging" # "weighted_mean" , "best" , "mean", "top50", "bagging" f = Importance(exp_no=exp_no) # helps when doing weighted mean base_features = None type_importance = "fold" #"fold", "opt" pick = None # pick top 2 trials out of 5 top = None threshold = 1 f.show(technique= technique, top=top, threshold=threshold, direction=direction, pick = pick, type_importance= type_importance, base_features=base_features) """ # mean ['D_39_last', 'P_2_last', 'P_2_last_2round2', 'S_3_mean', 'B_4_last_mean_diff', 'D_39_last_mean_diff', 'D_41_last_mean_diff', 'B_1_last', 'S_3_last', 'B_4_last', 'D_43_last', 'D_42_mean', 'D_39_std', 'B_4_std', 'D_39_max', 'B_3_last', 'B_5_last', 'D_42_min', 'D_43_mean', 'D_43_last_2round2'] # best ['D_39_last_mean_diff', 'P_2_last', 'D_39_last', 'B_1_last', 'B_4_last', 'D_43_last', 'D_43_last_2round2', 'S_3_last', 'D_39_std', 'S_3_mean', 'P_2_last_2round2', 'D_42_mean', 'P_2_mean', 'D_43_mean', 'D_41_last_mean_diff', 'S_3_min', 'S_8_mean', 'D_39_max', 'B_3_last', 'B_3_last_2round2'] # weighted ['D_39_last', 'P_2_last', 'P_2_last_2round2', 'S_3_mean', 'B_4_last_mean_diff', 'D_39_last_mean_diff', 'D_41_last_mean_diff', 'B_1_last', 'S_3_last', 'B_4_last', 'D_43_last', 'D_42_mean', 'D_39_std', 'B_4_std', 'D_39_max', 'B_3_last', 'B_5_last', 'D_42_min', 'D_43_mean', 'D_43_last_2round2'] """

File no 36: /src-framework3/show_input.py
import pandas as pd import numpy as np import os import sys import gc from utils import * def show_input_folders(): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() try: print("my_folds: ") my_folds = pd.read_parquet(f"../input/input-{comp_name}/my_folds.parquet") for col in my_folds.columns: print(col) #my_folds = my_folds[["0","1"]] print(my_folds.head(3)) print() print(my_folds.columns) print() print(my_folds.info()) print() # for f in list(zip(my_folds.columns, my_folds.dtypes, my_folds.nunique())): # print(f) # print() print(my_folds.shape) del my_folds gc.collect() print("="*40) print() except: print("Train not found") try: print("Train: ") train = pd.read_parquet(f"../input/input-{comp_name}/train.parquet") #train = train[["0","1"]] print(train.head(3)) print() print(train.columns) print() print(train.info()) print() print(train.shape) del train gc.collect() print("="*40) print() except: print("Train not found") try: print("Test: ") test = pd.read_parquet(f"../input/input-{comp_name}/test.parquet") # for col in test.columns: # print(col) # #test = test[["0","1"]] print(test.head(3)) print() print(test.columns) print() print(test.info()) print() print(test.shape) del test gc.collect() print("="*40) print() except: print("Test not found") try: print("Sample: ") sample = pd.read_parquet(f"../input/input-{comp_name}/sample.parquet") print(sample.head(3)) print() print(sample.columns) print() print(sample.info()) print() print(sample.shape) print("="*40) print() except: print("Sample not found") try: import glob # All files and directories ending with .txt and that don't begin with a dot: names = glob.glob(f"../configs/configs-{comp_name}/train_feats/*.pkl") for n in names: print(f"{n}: ") sample = load_pickle(n) print(sample) #input() if len(list(set(sample))) == 1: print("same value") # nan_count, num_missing_std, medianad, median, min, skew, print() # print(sample.head(3)) # print() # print(sample.columns) # print() # print(sample.info()) # print() # print(sample.shape) # print("="*40) # print() except: print(f"../configs/configs-{comp_name}/train_feats/train_feat_l_1_f_10_std.pkl not found") if __name__ == "__main__": show_input_folders()

File no 37: /src-framework3/show_table.py
import os import sys from utils import * import pandas as pd #pd.set_option("display.max_columns", None) pd.set_option("display.max_rows", None) def update_table_function( Table, exp_no, pblb_single_seed=None, pblb_all_seed=None, pblb_all_fold=None ): # pblb_single_seed pblb_all_seed pblb_all_fold Table.loc[exp_no, "pblb_single_seed"] = pblb_single_seed Table.loc[exp_no, "pblb_all_seed"] = pblb_all_seed Table.loc[exp_no, "pblb_all_fold"] = pblb_all_fold return Table def show_table(exp_list,col, is_sorted, which_table= ["base_table"]): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() bv_t = load_pickle(f"../configs/configs-{comp_name}/Table.pkl") # load for t in which_table: print() print("*--"*40) if t.split("_")[0] == "base": Table = load_pickle(f"../configs/configs-{comp_name}/Table.pkl") if exp_list != []: Table= Table[Table.exp_no.isin(exp_list)] if is_sorted: Table = Table.sort_values(by=["bv"], axis=0, ascending=False) #Table = Table[Table.fold_mean is list] #Table = Table.head(5) #print(Table) Table = Table.sort_values(by= ["fold_mean"], axis=0, ascending=False) # to compress if col == []: Table.rename(columns={'pblb_single_seed': 'single', 'pblb_all_seed': 'all', 'pblb_all_fold': 'fold', "no_iterations": "#iter"}, inplace=True) print(Table.set_index('exp_no')) else: if len(col) == 1: # asking for single value print(Table[col[0]].values[0]) else: Table= Table[Table.bv > 0.75][col] #Table.rename(columns={'pblb_single_seed': 'single', 'pblb_all_seed': 'all', 'pblb_all_fold': 'fold', "no_iterations": "#iter"}, inplace=True) print(Table) #print(self.Table[]) else: submissions = ["seed_mean", "fold_mean","pblb_single_seed","pblb_all_seed","pblb_all_fold"] Table = pd.read_csv(f"../configs/configs-{comp_name}/auto_exp_tables/auto_exp_table_{t.split('_')[0]}.csv") Table = pd.merge(Table, bv_t[["exp_no","bv"]+ submissions], on="exp_no",how="left") if exp_list != []: Table= Table[Table.exp_no.isin(exp_list)] if False: #is_sorted: Table = Table.sort_values(by=["bv"], axis=0, ascending=False) #print(tabulate(Table[Table.bv > 0.75].set_index('exp_no'))) #print(Table[Table.bv > 0.75].set_index('exp_no').to_markdown()) Table.rename(columns={'pblb_single_seed': 'single',"seed_mean":"s_mean", "fold_mean":"f_mean", "feature_names":"f_names", 'pblb_all_seed': 'all', 'pblb_all_fold': 'fold', "no_iterations": "#iter","optimize_on":"opt_on"}, inplace=True) print(Table[Table.bv > 0.75].set_index('exp_no')) #print(self.Table[]) def change_name(old_name, new_name): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() # load Table = load_pickle(f"../configs/configs-{comp_name}/Table.pkl") Table = Table.rename(columns = {old_name : new_name}) print(Table.head(2)) save_pickle(f"../configs/configs-{comp_name}/Table.pkl", Table) if __name__ == "__main__": col = [ "exp_no", "model_name", "bv", "bp", "random_state", "with_gpu", "aug_type", "_dataset", "use_cutmix", "features_list", "level_no", "oof_fold_name", "opt_fold_name", "fold_no", "no_iterations", "prep_list", # 'metrics_name', # 'seed_mean', # 'seed_std', # 'fold_mean', # 'fold_std', # 'pblb_single_seed', # 'pblb_all_seed', # 'pblb_all_fold', # 'notes', ] submissions = ["seed_mean", "seed_std","fold_mean","fold_std","pblb_single_seed","pblb_all_seed","pblb_all_fold"] general = ["prep_list","opt_fold_name","oof_fold_name", "fold_no","no_iterations"] base = ['exp_no',"bv"] explore_range = ["pblb_all_fold", "pblb_single_seed","pblb_all_seed"] pblb_cols = ["pblb_all_fold","pblb_single_seed","pblb_all_seed"] cv_cols = ["oof_fold_name", "fold_mean","fold_std", "seed_mean", "seed_std"] opt_cols = ["bv","no_iterations","prep_list","opt_fold_name","fold_no", "with_gpu"] #, "bp", "random_state", "metrics_name", "feaures_list" # things to reproduce prediction from an experiment exp_list = [17] #[435, 416] exp_list = [240] exp_list = [] which_table = [ "base_table", #"xgbr_table", #"xgb_table", #"cbc_table", #"lgbmc_table", #"lgb_table", ] # don't change col col = ["exp_no","model_name","no_iterations","bv"] + explore_range + ["fold_mean", "oof_fold_name", "opt_fold_name", "fold_no", "seed_mean", "fold_std", "seed_std"] #col = ['bp'] #col = ['features_list'] #col = [] is_sorted = True show_table(exp_list, col, is_sorted, which_table) # old_name = 'callbacks_listfeatures_list' # new_name = 'features_list' # change_name(old_name, new_name)

File no 38: /src-framework3/split_input.py
import pandas as pd import numpy as np from auto_exp import * from collections import defaultdict def split_base(): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() locker = load_pickle(f"../configs/configs-{comp_name}/locker.pkl") target_name = locker['target_name'] id_name = locker['id_name'] fold_list = ['fold3', 'fold5', 'fold10', 'fold20'] my_folds = pd.read_parquet(f"../input/input-{comp_name}/my_folds.parquet") print("my_folds shape is:", my_folds.shape) test = pd.read_parquet(f"../input/input-{comp_name}/test.parquet") all_columns = list(test.drop(id_name, axis=1).columns) my_dict = defaultdict() demo_set = set(all_columns) # step 1 feat = [id_name] + fold_list + [target_name] d = my_folds[feat] d.to_parquet(f"../input/input-{comp_name}/id_folds_target.parquet") my_dict['id_folds_target'] = feat # step 2 feat = all_columns d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_original.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_original.parquet", index=False) my_dict['original'] = feat demo_set = demo_set - set(feat) assert demo_set == set() save_pickle(f"../input/input-{comp_name}/input_dict.pkl", my_dict) def split(): comp_name = "amexdummy" target_name = "prediction" id_name = "customer_ID" fold_list = ["fold3", "fold5", "fold10", "fold20"] my_folds = pd.read_parquet(f"../input/input-{comp_name}/my_folds.parquet") print("my_folds shape is:", my_folds.shape) test = pd.read_parquet(f"../input/input-{comp_name}/test.parquet") #print(my_folds.columns) # cat_features = [ # "B_30", # "B_38", # "D_114", # "D_116", # "D_117", # "D_120", # "D_126", # "D_63", # "D_64", # "D_66", # "D_68" # ] # num_columns = ['P_2', 'D_39', 'B_1', 'B_2', 'R_1', 'S_3', 'D_41', 'B_3', 'D_42', 'D_43', 'D_44', 'B_4', 'D_45', 'B_5', 'R_2', 'D_46', 'D_47', 'D_48', 'D_49', 'B_6', 'B_7', 'B_8', 'D_50', 'D_51', 'B_9', 'R_3', 'D_52', 'P_3', 'B_10', 'D_53', 'S_5', 'B_11', 'S_6', 'D_54', 'R_4', 'S_7', 'B_12', 'S_8', 'D_55', 'D_56', 'B_13', 'R_5', 'D_58', 'S_9', 'B_14', 'D_59', 'D_60', 'D_61', 'B_15', 'S_11', 'D_62', 'D_65', 'B_16', 'B_17', 'B_18', 'B_19', 'B_20', 'S_12', 'R_6', 'S_13', 'B_21', 'D_69', 'B_22', 'D_70', 'D_71', 'D_72', 'S_15', 'B_23', 'D_73', 'P_4', 'D_74', 'D_75', 'D_76', 'B_24', 'R_7', 'D_77', 'B_25', 'B_26', 'D_78', 'D_79', 'R_8', 'R_9', 'S_16', 'D_80', 'R_10', 'R_11', 'B_27', 'D_81', 'D_82', 'S_17', 'R_12', 'B_28', 'R_13', 'D_83', 'R_14', 'R_15', 'D_84', 'R_16', 'B_29', 'S_18', 'D_86', 'D_87', 'R_17', 'R_18', 'D_88', 'B_31', 'S_19', 'R_19', 'B_32', 'S_20', 'R_20', 'R_21', 'B_33', 'D_89', 'R_22', 'R_23', 'D_91', 'D_92', 'D_93', 'D_94', 'R_24', 'R_25', 'D_96', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'D_102', 'D_103', 'D_104', 'D_105', 'D_106', 'D_107', 'B_36', 'B_37', 'R_26', 'R_27', 'D_108', 'D_109', 'D_110', 'D_111', 'B_39', 'D_112', 'B_40', 'S_27', 'D_113', 'D_115', 'D_118', 'D_119', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_127', 'D_128', 'D_129', 'B_41', 'B_42', 'D_130', 'D_131', 'D_132', 'D_133', 'R_28', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', 'D_143', 'D_144', 'D_145'] # # 11 + 177 = 188 all_columns = list(test.drop('customer_ID', axis=1).columns) # #base_columns =[i[:-6] for i in list(filter(lambda x:x.endswith("_first"), all_columns))] # base_columns = cat_features + num_columns # #num_columns = [c for c in base_columns if c not in cat_features] # print(base_columns) # print(cat_features) # print(num_columns) # print(len(cat_features)) # print(len(num_columns)) # print(len(base_columns)) demo_set = set(all_columns) print("all columns", len(all_columns), len(list(demo_set))) #print(all_columns) print() my_dict = defaultdict() # fold+target : id_fold_target feat = [id_name] + fold_list + [target_name] d = my_folds[feat] d.to_parquet(f"../input/input-{comp_name}/id_folds_target.parquet") my_dict['id_folds_target'] = feat # print(d.head(2)) # print(d.shape) # first : train_first, test_first feat = ['year_first', 'year_mean', 'year_std', 'year_min', 'year_max', 'year_last', 'year_nunique', 'year_count', 'month_first', 'month_mean', 'month_std', 'month_min', 'month_max', 'month_last', 'month_nunique', 'month_count', 'day_first', 'day_mean', 'day_std', 'day_min', 'day_max', 'day_last', 'day_nunique', 'day_count', 'dayofweek_first', 'dayofweek_mean', 'dayofweek_std', 'dayofweek_min', 'dayofweek_max', 'dayofweek_last', 'dayofweek_nunique', 'dayofweek_count'] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_date.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_date.parquet", index=False) my_dict['date'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) """ feat = [i+"_first" for i in base_columns] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_first.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_first.parquet", index=False) my_dict['first'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # last : train_last, test_last feat = [i+"_last" for i in base_columns] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_last.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_last.parquet", index=False) my_dict['last'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # min feat = [i+"_min" for i in num_columns] d1 = my_folds[feat] d2 = test[[i+"_min" for i in num_columns]] d1.to_parquet(f"../input/input-{comp_name}/train_min.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_min.parquet", index=False) my_dict['min'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # max feat = [i+"_max" for i in num_columns] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_max.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_max.parquet", index=False) my_dict['max'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # mean feat = [i+"_mean" for i in num_columns] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_mean.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_mean.parquet", index=False) my_dict['mean'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # std feat = [i+"_std" for i in num_columns] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_std.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_std.parquet", index=False) my_dict['std'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # count feat = [i+"_count" for i in cat_features] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_count.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_count.parquet", index=False) my_dict['count'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # nunique feat = [i+"_nunique" for i in cat_features] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_nunique.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_nunique.parquet", index=False) my_dict['nunique'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # lag_sq feat = [i+"_last_lag_sq" for i in num_columns] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_lag_sq.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_lag_sq.parquet", index=False) my_dict['lag_sq'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # lag_cb feat = [i+"_last_lag_cb" for i in num_columns] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_lag_cb.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_lag_cb.parquet", index=False) my_dict['lag_cb'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # lag_div feat = [i+"_last_lag_div" for i in num_columns] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_lag_div.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_lag_div.parquet", index=False) my_dict['lag_div'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # lag_sub feat = [i+"_last_lag_sub" for i in num_columns] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_lag_sub.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_lag_sub.parquet", index=False) my_dict['lag_sub'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # round2 Round lall ast float features to 2 decimal place # This is the one which keeps changing feat = list(filter(lambda x:x.endswith("_2round2"), all_columns)) d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_2round2.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_2round2.parquet", index=False) my_dict['2round2'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # lag_mmsub max-min feat = [i+"_max_lag_mmsub" for i in num_columns] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_lag_mmsub.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_lag_mmsub.parquet", index=False) my_dict['lag_mmsub'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # lag_mmsub max-min feat = [i+"_max_lag_mmsq" for i in num_columns] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_lag_mmsq.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_lag_mmsq.parquet", index=False) my_dict['lag_mmsq'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # lag_mmcb max-min feat = [i+"_max_lag_mmcb" for i in num_columns] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_lag_mmcb.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_lag_mmcb.parquet", index=False) my_dict['lag_mmcb'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # last_mean_diff feat = [i+"_last_mean_diff" for i in num_columns] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_last_mean_diff.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_last_mean_diff.parquet", index=False) my_dict['last_mean_diff'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # _diff1 feat = [i+"_diff1" for i in num_columns] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_diff1.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_diff1.parquet", index=False) my_dict['diff1'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) # count_nunique_diff feat = [i+"_count_nunique_diff" for i in cat_features] d1 = my_folds[feat] d2 = test[feat] d1.to_parquet(f"../input/input-{comp_name}/train_count_nunique_diff.parquet", index=False) d2.to_parquet(f"../input/input-{comp_name}/test_count_nunique_diff.parquet", index=False) my_dict['count_nunique_diff'] = feat print( len(feat)) #print(feat) print() demo_set = demo_set - set(feat) print("Done") print(demo_set) print(len(list(demo_set))) """ save_pickle(f"../input/input-{comp_name}/input_dict.pkl", my_dict) #split_base() #split() #print(11*3+177*13+188*2+642) d = load_pickle(f"../input/input-amzcomp1/input_dict.pkl") print(d) # 2633 # s = [188, # 188, # 177, # 177, # 177, # 177, # 11, # 11, # 177, # 177, # 177, # 177, # 642, # 177] # print(s) # print(sum(s)) # verified # amex = amex3_settings() # feature_keys = ['first','last','min', 'max' , 'mean', 'std', 'count', 'nunique', 'lag_sq', 'lag_cb','lag_div', 'lag_sub', 'round2', 'lag_mmsub'] # feature_keys2 = ['all_cat', 'all_num'] # all_cols = [] # for f in feature_keys: # all_cols += amex.feature_dict[f] # print(len(all_cols)) # assert len(all_cols) == len(set(all_cols)) # all_cols2 = [] # for r in feature_keys2: # all_cols2 += amex.feature_dict[r] # print(len(all_cols2)) # assert len(all_cols2) == len(set(all_cols2))

File no 39: /src-framework3/submit.py
import pandas as pd import numpy as np import os import sys import pickle from utils import * from output import * from predict import * from seed_it import * import time # for animation import itertools import threading import time import sys import global_variables import warnings warnings.filterwarnings("ignore") def update_it( comp_name, exp_no, sub_type, pb_score): # to update a table that is all we need # comp_name , exp_no, sub_table Table = load_pickle(f"../configs/configs-{comp_name}/Table.pkl") updated = False col_name = { "all" : "pblb_all_seed", "single" : "pblb_single_seed" } if sub_type.startswith("e"): # ensemble raise Exception("Ensemble should not try to update Table!!") elif sub_type.startswith("f"): # public fold submission # so first find index of fold full_row = Table.loc[Table.exp_no == exp_no] try: oof_fold_name_list = Table.loc[Table.exp_no == exp_no, "oof_fold_name"].values[0] # sanity check if oof_fold_name_list == []: # our oof_fold_name is empty : No prediction is done yet # that is not possible because we entered here only after some prediction has been done as "fold5/fold10/..." raise Exception("Already made prediction but Table is showing oof_fold_name empty!!") index_no= Table.loc[Table.exp_no == exp_no, "oof_fold_name"].values[0].index(sub_type) pblb_all_fold_list = Table.loc[Table.exp_no == exp_no, "pblb_all_fold"].values[0] # first correct it if pblb_all_fold_list == []: # it is first time pblb_all_fold_list = [None for i in range(len(oof_fold_name_list))] if pblb_all_fold_list[index_no] is None: # so this is a new submission so update print("Before updating") print(Table.loc[Table.exp_no == exp_no, ["exp_no","oof_fold_name", "pblb_all_fold"]]) pblb_all_fold_list[index_no] = pb_score full_row.pblb_all_fold = [[i for i in pblb_all_fold_list]] print("updating...") Table.loc[Table.exp_no == exp_no, :] = full_row.copy() print("After updating") print(Table.loc[Table.exp_no == exp_no, ["exp_no","oof_fold_name", "pblb_all_fold"]]) print("="*40) print() updated = True else: # already updated print("Already updated") print() pass except: raise Exception(f"Fold name {sub_type} is not found in Table") elif sub_type in ["single", "all"]: # all seeds if Table.loc[Table.exp_no == exp_no, col_name[sub_type]].values[0] is None: # first time print("Before updating") print(Table.loc[Table.exp_no == exp_no, ["exp_no","pblb_single_seed", "pblb_all_seed"]]) print("updating...") Table.loc[Table.exp_no == exp_no, col_name[sub_type]] = pb_score print("After updating") print(Table.loc[Table.exp_no == exp_no, ["exp_no","pblb_single_seed", "pblb_all_seed"]]) print("="*40) print() updated = True else: # no need to rename it already updated print("Already updated") print() pass else: raise Exception(f"sub_type {sub_type} is not valid name!") if updated: # since Table is updated save_pickle(f"../configs/configs-{comp_name}/Table.pkl", Table) def update_table_with_submission_log(comp_full_name, current_update_only = False): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() comp_full_name = "amex-default-prediction" log = pd.read_csv(f"../working/{comp_full_name}_submission_logs.csv") print(log) log = log[log.fileName == "submission.csv"] # ignore old if current_update_only is True: # it is asking to update only the current submission no need to go through the whole logs log = log.iloc[[0]] print("Only update single row!!") print() print(log) print() for row in log.iterrows(): items = row[1] if items[3] != "complete": # score is not updated yet continue pb_score = float(items[4]) # items[2] :=> message # f"{comp_name}_exp_{exp_no}_{pred_type}" # f"{comp_name}_ens_{exp_no}" comp_name = items[2].split("_")[0] marker = items[2].split("_")[1] exp_no = int(items[2].split("_")[2]) if marker != "exp": # ensemble case update_log_ens(comp_name, exp_no, pb_score) else: sub_type = items[2].split("_")[3] # fold5, all, single print(comp_name, marker, exp_no, sub_type, pb_score) update_it( comp_name, exp_no, sub_type, pb_score) def update_log_ens(comp_name, exp_no, pb_score): # update the public score ensemble_dict = load_json(f"../configs/configs-{comp_name}/ensemble_logs/ens_{exp_no}.json") ensemble_dict["pb_score"] = pb_score print(ensemble_dict) save_json(f"../configs/configs-{comp_name}/ensemble_logs/ens_{exp_no}.json", ensemble_dict) """ submit.py kaggle competitions list kaggle competitions leaderboard amex-default-prediction --show | --download kaggle competitions submissions amex-default-prediction kaggle competitions submit ventilator-pressure-prediction -f submission.csv -m "exp_{}_fold/single/all" #submit your submission.csv kaggle competitions submit -c [COMPETITION] -f [FILE] -m [MESSAGE] """ def submit_pred(comp_full_name, exp_no, pred_type, mode): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() locker = load_pickle(f"../configs/configs-{comp_name}/locker.pkl") Table = load_pickle(f"../configs/configs-{comp_name}/Table.pkl") if exp_no == -1: row_e = Table[Table.exp_no == list(Table.exp_no.values)[-1]] exp_no = row_e.exp_no.values[0] if pred_type == "ens": # pred type is good for exp as can be multiple fold but here each ensemble is unique so keep minimalistic name message = f"{comp_name}_ens_{exp_no}" else: message = f"{comp_name}_exp_{exp_no}_{pred_type}" ########################################## # Sanity check # ########################################## # checks whether it is already submitted log = pd.read_csv(f"../working/{comp_full_name}_submission_logs.csv") log = log[log.fileName == "submission.csv"] # ignore old if message in list(log.description.values): raise Exception(f"{message} already submitted!!!") ######################################### if pred_type == "ens": # it's an ensemble prediction try: sample = pd.read_parquet(f"../working/{comp_name}_ens_{exp_no}.parquet") except: raise Exception(f"Not created ensemble of ens no: {exp_no}") sample.to_csv(f"../working/submission.csv",index=False) print(sample.head(3)) if mode == "manual": val = input(f"""You are about to make submission to {comp_name} with: \nFile named: {comp_name}_sub_e_{exp_no}_single.parquet Press Y/y to continue or press N/n to prevent submission!: """) if val.upper() == "Y": os.system(f"kaggle competitions submit {comp_full_name} -f ../working/submission.csv -m {message}") # #submit your submission.csv") else: time.sleep(1) os.system(f"kaggle competitions submit {comp_full_name} -f ../working/submission.csv -m {message}") # #submit your submission.csv") if pred_type == "single": try: sample= pd.read_parquet(f"../working/{comp_name}_sub_e_{exp_no}_single.parquet") except: # It is not predicted yet # run seed_it script s = seeds(exp_no=exp_no) # last exp s.run_seeds() del s # It must be true now sample= pd.read_parquet(f"../working/{comp_name}_sub_e_{exp_no}_single.parquet") sample.to_csv(f"../working/submission.csv",index=False) # {comp_name}_sub_e_{exp_no}_single.csv") # we print(sample.head(3)) if mode == "manual": val = input(f"""You are about to make submission to {comp_name} with: \nFile named: {comp_name}_sub_e_{exp_no}_single.parquet Press Y/y to continue or press N/n to prevent submission!: """) if val.upper() == "Y": os.system(f"kaggle competitions submit {comp_full_name} -f ../working/submission.csv -m {message}") # #submit your submission.csv") else: time.sleep(1) os.system(f"kaggle competitions submit {comp_full_name} -f ../working/submission.csv -m {message}") # #submit your submission.csv") if pred_type == "all": try: sample= pd.read_parquet(f"../working/{comp_name}_sub_e_{exp_no}_all.parquet") except: # It is not predicted yet # run seed_it script s = seeds(exp_no=exp_no) # last exp s.run_seeds() del s # It must be true now sample= pd.read_parquet(f"../working/{comp_name}_sub_e_{exp_no}_all.parquet") sample.to_csv(f"../working/submission.csv",index=False) # {comp_name}_sub_e_{exp_no}_all.csv") # we print(sample.head(3)) if mode == "manual": val = input(f"""You are about to make submission to {comp_name} with: \nFile named: {comp_name}_sub_e_{exp_no}_all.parquet Press Y/y to continue or press N/n to prevent submission!: """) if val.upper() == "Y": os.system(f"kaggle competitions submit {comp_full_name} -f ../working/submission.csv -m {message}") # #submit your submission.csv") else: time.sleep(1) os.system(f"kaggle competitions submit {comp_full_name} -f ../working/submission.csv -m {message}") # #submit your submission.csv") if pred_type.startswith("f"): #"fold3", "fold5" try: sample= pd.read_parquet(f"../working/{comp_name}_sub_e_{exp_no}_{pred_type}.parquet") except: # It is not output yet yet try: dummy = load_pickle(f"../configs/configs-{comp_name}/test_preds/test_pred_e_{exp_no}_{pred_type}.pkl") del dummy gc.collect() except: # It is also not predicted yet # predict it p = predictor(exp_no=exp_no) # last exp fold_name = pred_type #fold5" #"fold5" # "fold3" , "fold5", "fold10" , "fold20", "" p.run_folds(fold_name) del p # output it # need to add which fold prediction to output file_type = "parquet" # "csv" o = out( pred_type , exp_no, file_type) o.dump() del o # It must be true now sample= pd.read_parquet(f"../working/{comp_name}_sub_e_{exp_no}_{pred_type}.parquet") sample.to_csv(f"../working/submission.csv",index=False) # {comp_name}_sub_e_{exp_no}_ingle.csv") # we print(sample.head(3)) if mode == "manual": val = input(f"""You are about to make submission to {comp_name} with: \nFile named: {comp_name}_sub_e_{exp_no}_{pred_type}.parquet Press Y/y to continue or press N/n to prevent submission!: """) if val.upper() == "Y": score= os.system(f"kaggle competitions submit {comp_full_name} -f ../working/submission.csv -m {message}") # #submit your submission.csv") else: time.sleep(1) os.system(f"kaggle competitions submit {comp_full_name} -f ../working/submission.csv -m {message}") # #submit your submission.csv") ###################################################### # Makes sure submission score is out # ###################################################### global_variables.done = False t = threading.Thread(target=animate) t.start() #long process here time.sleep(5) # save logs # update submission_log # https://askubuntu.com/questions/420981/how-do-i-save-terminal-output-to-a-file#:~:text=the%20shortcut%20is%20Ctrl%20%2B%20Shift,or%20as%20HTML%20including%20colors! #!kaggle competitions submissions amex-default-prediction --csv > ../working/amex-default-prediction_submission_logs.csv os.system(f"kaggle competitions submissions {comp_full_name} --csv > ../working/{comp_full_name}_submission_logs.csv") log = pd.read_csv(f"../working/{comp_full_name}_submission_logs.csv") latest_message = log.loc[0, 'description'] status = log.loc[0, "status"] while latest_message != message or status != "complete": time.sleep(3) os.system(f"kaggle competitions submissions {comp_full_name} --csv > ../working/{comp_full_name}_submission_logs.csv") log = pd.read_csv(f"../working/{comp_full_name}_submission_logs.csv") latest_message = log.loc[0, 'description'] status = log.loc[0, "status"] global_variables.done = True # display submission_log os.system(f"kaggle competitions submissions {comp_full_name}") # update the table #update_table_with_submission_log(comp_full_name) current_update_only = True update_table_with_submission_log(comp_full_name, current_update_only) if __name__ == "__main__": with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() comp_full_name = "amex-default-prediction" #exp_no = -1 #0,1,2 exp_no = 44 pred_type = "single" #"fold10" #"fold5" # "all" "fold" "fold3", "ens" mode = "auto" # "manual" # manual asks for a prompt just before submitting # auto don't asks for a prompt # both can make prediction file if already not created submit_pred(comp_full_name, exp_no, pred_type, mode) # if calling from here update, itterate through whole log file current_update_only = False #update_table_with_submission_log(comp_full_name, current_update_only) #e138 #[0.7984387250317737, 0.794395022818941, 0.7928167917014163, 0.797858209410043, 0.7950284894125992] # # [0.7978218889227393, 0.7940426440210139, 0.7932858172582967, 0.7982026580647962, 0.7945527414758347] --> 0.97

File no 40: /src-framework3/submit_one.py
""" This does nothing but submits one prediction. """ from utils import * #pd.set_option("display.max_columns", None) pd.set_option("display.max_rows", None) class Moderator: def __init__(self, comp_full_name, explore_range= ["pblb_all_fold"], sort_by_col = "bv", mode="auto"): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() self.locker = load_pickle(f"../configs/configs-{comp_name}/locker.pkl") self.Table = load_pickle(f"../configs/configs-{comp_name}/Table.pkl") self.comp_full_name = comp_full_name self.explore_range = explore_range self.sort_by_col = sort_by_col self.mode = mode self.ascending = False if sort_by_col in ["fold_std", "seed_std"]: self.ascending = True # filter by THRESHOLD # bv self.Table = self.Table[self.Table.bv > 0.75] # no_iterations #self.Table = self.Table[self.Table.no_iterations > 5] # fold_mean #self.Table = self.Table[self.Table.fold_mean > 0.75] # fold_std #self.Table = self.Table[self.Table.fold_std < 0.001] # seed_mean #self.Table = self.Table[self.Table.seed_mean > 0.79] # seed_std #self.Table = self.Table[self.Table.seed_std < 0.005] # Sort by: self.Table = self.Table.sort_values(by=[sort_by_col], ascending=self.ascending) self.show() def query(self): print("Type:") print("sub: to submit in this exp") print("down: to move to next row") text = input() if text.lower() == "down": print() return 0 elif text.lower() == "sub": print("+.."*40) # submit this print("Type prediction type for which you want to make submission!: ") print("single, all, fold3, fold5, fold10, fold20") fold_type = input() # fold_type : "all", "single", "fold3", "fold10" from submit import submit_pred submit_pred(self.comp_full_name, self.target_exp_no, fold_type, self.mode) self.submitted = True return 1 raise Exception("Should never reach here, Not a valid input") def submit(self): self.submitted = False pos = 0 size = self.Table.shape[0] while self.submitted is False and pos< size : self.target_exp_no = self.Table.iloc[pos,0] oof_fold_name_list = self.Table.loc[self.target_exp_no, "oof_fold_name"] # ["fold5", "fold3", "fold10"] print("="*40) print("Experiment no:",self.target_exp_no) explore_range = ["pblb_all_fold", "pblb_single_seed","pblb_all_seed"] cv_cols = ["oof_fold_name", "fold_mean","fold_std", "seed_mean", "seed_std"] print(self.Table[self.Table.exp_no== self.target_exp_no][["bv"] +explore_range+ cv_cols]) if self.mode == "manual": no= self.query() if no == 0: pos += 1 continue elif no == 1: break #submitted = True for feat in self.explore_range: # "pblb_all_fold", "pblb_single_seed","pblb_all_seed" val= self.Table.loc[self.target_exp_no, feat] if isinstance(val, list) and None in val: # so we need to repeat it for that # [ 0.23, None, None, 0.22,0.56,None] for i in range(len(val)): if val[i] is None: # automatically submit this as not submitted before from submit import submit_pred submit_pred(self.comp_full_name, self.target_exp_no, fold_type, self.mode) # stopping execution since submitted return if val in [None, []]: # at present it deals with only [] oof_fold_name .i.e no prediction is made yet [None, 0.9, None] won't work for now # None, [] if feat.split("_")[-1] == "fold": # no prediction is made yet print("Type prediction fold_name for which you want to make submission!: ") print("fold3, fold5, fold10, fold20") fold_type = input() # fold_type : "all", "single", "fold3", "fold10" from submit import submit_pred submit_pred(self.comp_full_name, self.target_exp_no, fold_type, self.mode) self.submitted = True break fold_type = feat.split("_")[1] # "single", "all" :-> seed # automatically submit this as not submitted before from submit import submit_pred submit_pred(self.comp_full_name, self.target_exp_no, fold_type, self.mode) # stopping execution since submitted return pos += 1 def show(self): # anything extra added will cause it to move below: "seed_std", "fold_std" print(self.Table[["exp_no","model_name","no_iterations",self.sort_by_col] + self.explore_range + ["fold_mean", "oof_fold_name", "seed_mean"]].set_index('exp_no')) submissions = ["seed_mean", "seed_std","fold_mean","fold_std","pblb_single_seed","pblb_all_seed","pblb_all_fold"] general = ["prep_list","opt_fold_name","oof_fold_name", "fold_no","no_iterations"] base = ['exp_no',"bv"] pblb_cols = ["pblb_all_fold","pblb_single_seed","pblb_all_seed"] cv_cols = ["oof_fold_name", "fold_mean","fold_std", "seed_mean", "seed_std"] opt_cols = ["bv","no_iterations","prep_list","opt_fold_name","fold_no", "with_gpu"] #, "bp", "random_state", "metrics_name", "feaures_list" # things to reproduce prediction from an experiment if __name__ == "__main__": comp_full_name = "amex-default-prediction" # explore range searches for what we have to submit # if set only "fold" It will look if fold public score is there or not # If not it will prompt/auto submit. explore_range = ["pblb_all_fold", "pblb_single_seed","pblb_all_seed"] #["fold", "seed_single", "seed_all"] sort_by_col = "bv" # "bv", "", "fold_mean", "fold_std", "seed_mean", "seed_std" # if it is sort by "bv" then no issue # but if is is by fold/seed then we may have not done "predict.py"/"seed_it.py" mode = "auto" # "auto", "manual" # auto: looks for the first None and sends it for submission , It asks only once when we encounter [], which fold_name to predict # manual: asks at each step what to select m = Moderator(comp_full_name, explore_range, sort_by_col, mode) m.submit()

File no 41: /src-framework3/update_table.py
import os import sys from utils import * import pandas as pd import ast pd.set_option("display.max_columns", None) pd.set_option("display.max_rows", None) def update_table_function( Table, exp_no, pblb_single_seed=None, pblb_all_seed=None, pblb_all_fold=None ): # pblb_single_seed pblb_all_seed pblb_all_fold Table.loc[exp_no, "pblb_single_seed"] = pblb_single_seed Table.loc[exp_no, "pblb_all_seed"] = pblb_all_seed Table.loc[exp_no, "pblb_all_fold"] = [[i for i in pblb_all_fold]] return Table def update_score( exp_no=-1, pblb_single_seed=None, pblb_all_seed=None, pblb_all_fold=None ): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() # load Table = load_pickle(f"../configs/configs-{comp_name}/Table.pkl") if exp_no == -1: # pick the last one exp_no = Table.iloc[-1, :].exp_no # before print( Table[Table.exp_no == exp_no][ ["exp_no", "pblb_single_seed", "pblb_all_seed", "pblb_all_fold"] ] ) print() # -> Sanity check: if pblb_single_seed != None: assert Table.loc[exp_no, "pblb_single_seed"] == None if pblb_all_seed != None: assert Table.loc[exp_no, "pblb_all_seed"] == None if pblb_all_fold != None: assert Table.loc[exp_no, "pblb_all_fold"] == [] # None # update Table = update_table_function( Table, exp_no, pblb_single_seed, pblb_all_seed, pblb_all_fold ) # after print( Table[Table.exp_no == exp_no][ ["exp_no", "pblb_single_seed", "pblb_all_seed", "pblb_all_fold"] ] ) print("=" * 40) print("Type Y/y to update, N/n to reject") text = input() if text.upper() == "Y": # Dump back save_pickle(f"../configs/configs-{comp_name}/Table.pkl", Table) print("Updated!") else: print("Cancelled Update!") def change_table(exp_no, feature_name, value): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() # load Table = load_pickle(f"../configs/configs-{comp_name}/Table.pkl") old_value= Table.loc[exp_no, feature_name] print(f"Do you want to replace \n{old_value} \n\nwith: \n{value} \nfor:") print(f"Exp no: {exp_no} and col: {feature_name}") print("=" * 40) print("Type Y/y to update, N/n to reject:") text = input() if text.upper() == "Y": # Dump back # old : Table.loc[exp_no, feature_name] = value Table = get_table(Table, exp_no, feature_name, value) save_pickle(f"../configs/configs-{comp_name}/Table.pkl", Table) print("Updated!") else: print("Cancelled Update!") def get_table(Table, exp_no, feature_name, value): raw_row = Table.loc[exp_no, :].copy() raw_row[feature_name] = value Table.loc[exp_no, :] = raw_row.values return Table def change_table_custom(): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() # load Table = load_pickle(f"../configs/configs-{comp_name}/Table.pkl") Table.exp_no = Table.exp_no.astype(int) Table.level_no = Table.level_no.astype(int) Table.no_iterations = Table.no_iterations.astype(int) Table.random_state = Table.random_state.astype(int) print(Table.head(2)) print("=" * 40) print("Type Y/y to update, N/n to reject:") text = input() if text.upper() == "Y": # Dump back save_pickle(f"../configs/configs-{comp_name}/Table.pkl", Table) print("Updated!") else: print("Cancelled Update!") if __name__ == "__main__": # exp_no = -1 # keep it as default exp_no = 70 pblb_single_seed = None pblb_all_seed = None pblb_all_fold = [83.00393] #83.22706 update_score(exp_no, pblb_single_seed, pblb_all_seed, pblb_all_fold) # exp_no = 269 # feature_name = "no_iterations" # value = 30 # change_table(exp_no, feature_name, value) # exp_no = 18 # feature_name = "bp" # value = {'objective': 'binary', 'metric': 'binary_logloss', 'boosting': 'dart', 'learning_rate': 0.010230005490999927, 'seed': 241, 'num_leaves': 105, 'feature_fraction': 0.18956831457766554, 'bagging_freq': 12, 'bagging_fraction': 0.5115517394581794, 'n_jobs': -1, 'lambda_l2': 1, 'min_data_in_leaf': 35} # change_table(exp_no, feature_name, value) #change_table_custom() # {'n_estimators': 500, 'learning_rate': 0.27981914604335584, 'max_depth': 7, 'loss': 'squared_error', 'criterion': 'mse', 'max_features': 'auto', 'min_sample_split': 0.11249062083311395, 'subsample': 0.95} # exp_no = 17 # feature_name = "bp" # value = {'n_estimators': 500, 'learning_rate': 0.27981914604335584, 'max_depth': 7, 'loss': 'squared_error', 'criterion': 'mse', 'max_features': 'auto', 'min_samples_split': 0.11249062083311395, 'subsample': 0.95} # change_table(exp_no, feature_name, value)

File no 42: /src-framework3/utils.py
import pickle import json import os import numpy as np import pandas as pd import math import random import tensorflow as tf import sys import gc import tracemalloc import global_variables import gzip from datetime import datetime # for animation import itertools import threading import time import sys import global_variables # https://stackoverflow.com/questions/47152610/what-is-the-difference-between-xgb-train-and-xgb-xgbregressor-or-xgb-xgbclassif import xgboost as xgb # when calling the low level api # dart callback import joblib def save_pickle(path, to_dump): with open(path, "wb") as f: pickle.dump(to_dump, f) def load_pickle(path): with open(path, "rb") as f: o = pickle.load(f) return o def save_gzip(path, to_dump): with gzip.open(path, "wb") as f: f.write(to_dump) def load_gzip(path): with gzip.open(path, "rb") as f: o = f.read(f) return o def save_json(path, to_dump): json.dump( to_dump, open( path, 'w' ) ) def load_json(path): return json.load( open( path) ) def coln_3_1(arr): # array with three columns return np.array(arr).reshape(-1) def fix_random(seed): os.environ["PYTHONHASHSEED"] = str(seed) np.random.seed(seed) random.seed(seed) tf.random.set_seed( seed ) # f"The truth value of a {type(self).__name__} is ambiguous. " return seed # np.random.randint(3, 1000) # it should return 5 def true_random(size): val = os.urandom(100) val = str(val) total = 0 for i,v in enumerate(val): total += (i+1)*ord(v) return int(total) # sanity check # https://www.kaggle.com/code/sietseschrder/xgboost-starter-0-793/notebook # NEEDED WITH DeviceQuantileDMatrix BELOW class IterLoadForDMatrix(xgb.core.DataIter): def __init__(self, df=None, batch_size=256*1024): #self, df=None, features=None, target=None, batch_size=256*1024): # self.features = features # self.target = target self.df = df # is a numpy 2D array (no_of_data_pts, [features,target]) self.it = 0 # set iterator to 0 self.batch_size = batch_size self.batches = int( np.ceil( df.shape[0] / self.batch_size ) ) super().__init__() def reset(self): '''Reset the iterator''' self.it = 0 def next(self, input_data): '''Yield next batch of data.''' if self.it == self.batches: return 0 # Return 0 when there's no more batch. a = self.it * self.batch_size b = min( (self.it + 1) * self.batch_size, self.df.shape[0] ) dt = cudf.DataFrame(self.df[a:b]) # can use it without cudf input_data(data=dt[:,:-1], label=dt[:, -1]) #, weight=dt['weight']) # may need to reset few self.it += 1 return 1 # generte random no time based but not good, repeats in fractions of seconds def generate_random_no(adder="--|--"): # makes sure each time we are at differnet random state # random_state should only be used for reproducibility and should not give a better model seed = int(datetime.now().strftime("%H%M%S")) # seed selected based on current time if adder != "--|--": seed += adder # datetime can't give new no in vicinity of fraction of seconds so introducing this adder os.environ["PYTHONHASHSEED"] = str(seed) np.random.seed(seed) random.seed(seed) tf.random.set_seed( seed ) # f"The truth value of a {type(self).__name__} is ambiguous. " return seed # np.random.randint(3, 1000) # it should return 5 # https://stackoverflow.com/questions/24455615/python-how-to-display-size-of-all-variables # https://stackoverflow.com/questions/633127/viewing-all-defined-variables def sizeof_fmt(num, suffix="B"): """by Fred Cirera, https://stackoverflow.com/a/1094933/1870254, modified""" for unit in ["", "Ki", "Mi", "Gi", "Ti", "Pi", "Ei", "Zi"]: if abs(num) < 1024.0: return "%3.1f %s%s" % (num, unit, suffix) num /= 1024.0 return "%.1f %s%s" % (num, "Yi", suffix) # https://stackoverflow.com/questions/22029562/python-how-to-make-simple-animated-loading-while-process-is-running #here is the animation def animate(): for c in itertools.cycle(['|', '/', '-', '\\']): if global_variables.done: break sys.stdout.write('\rRefreshing ' + c) sys.stdout.flush() time.sleep(0.1) sys.stdout.write('\rDone! ') sys.stdout.write('\n') def check_memory_usage(title, object_name, verbose=1): # https://stackoverflow.com/questions/24455615/python-how-to-display-size-of-all-variables # https://stackoverflow.com/questions/633127/viewing-all-defined-variables if verbose > 0: print() print("*" * 40) print("{:>30}".format(title)) print("*" * 40) mem = tracemalloc.get_traced_memory() print( "current, peak: {:>8} : {:>8}".format( sizeof_fmt(mem[0]), sizeof_fmt(mem[1]) ) ) else: mem = tracemalloc.get_traced_memory() print( "{}=> current usage, peak usage: {:>8} : {:>8}".format( title, sizeof_fmt(mem[0]), sizeof_fmt(mem[1]) ) ) if verbose > 0: print() # # Locals # print("Local Variables") # for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()), # key= lambda x: -x[1])[:10]: # print("{:>20}: {:>8}".format(name, sizeof_fmt(size))) # # globals # print("Global Variables") # for name, size in sorted(((name, sys.getsizeof(value)) for name, value in globals().items()), # key= lambda x: -x[1])[:10]: # print("{:>30}: {:>8}".format(name, sizeof_fmt(size))) if object_name != "--|--": # object print("object variables") for name, size in sorted( ( (name, sys.getsizeof(value)) for name, value in object_name.__dict__.items() ), key=lambda x: -x[1], )[:10]: print("{:>30}: {:>8}".format(name, sizeof_fmt(size))) # #==> # print("List of object") # [o for o in gc.get_objects() if isinstance(o, Foo)] # for all_objects in gc.get_objects(): # for o in all_object # name = o # size = sys.getsizeof(o) # print("{:>30}: {:>8}".format(name, sizeof_fmt(size))) print("-" * 40) print() def cosine_decay(epoch): if global_variables.epochs > 1: w = (1 + math.cos(epoch / (global_variables.epochs - 1) * math.pi)) / 2 else: w = 1 return w * global_variables.lr_start + (1 - w) * global_variables.lr_end def exponential_decay(epoch): # v decays from e^a to 1 in every cycle # w decays from 1 to 0 in every cycle # epoch == 0 -> w = 1 (first epoch of cycle) # epoch == epochs_per_cycle-1 -> w = 0 (last epoch of cycle) # higher a -> decay starts with a steeper decline a = 3 CYCLES = global_variables.epochs //5 # 5 epochs in one cycle assert CYCLES < global_variables.epochs epochs_per_cycle = global_variables.epochs // CYCLES epoch_in_cycle = epoch % epochs_per_cycle if epochs_per_cycle > 1: v = math.exp(a * (1 - epoch_in_cycle / (epochs_per_cycle - 1))) w = (v - 1) / (math.exp(a) - 1) else: w = 1 n = w * global_variables.lr_start + (1 - w) * global_variables.lr_end print("lr-->", n) return n def get_test(input_dict,comp_name, useful_features, no_rows): # optimize_on : [3,1] features_in = [] new_array = np.array([], dtype=np.int8).reshape(no_rows,0) for key,feat_list in input_dict.items(): if key == "id_folds_target": continue if all(x in useful_features for x in feat_list): # pull whole set features_in += feat_list d1 = pd.read_parquet(f"../input/input-{comp_name}/test_{key}.parquet") new_array=np.concatenate((new_array, np.array(d1.values)), axis=1) # xtrain = np.array(my_folds[values].values) # xtrain= np.concatenate((xtrain, np.array(rest_train)), axis=1) elif any(x in useful_features for x in feat_list): # partial set present = list(set(feat_list) & set(useful_features)) features_in += present d1 = pd.read_parquet(f"../input/input-{comp_name}/test_{key}.parquet")[present] new_array=np.concatenate((new_array, np.array(d1.values)), axis=1) else: # none of values present pass # it is asking for full dataset return new_array, features_in def bottleneck_test(comp_name, useful_features, return_type, verbose=0): ordered_list = [] # There is a risk that we may not have same order of items in test vs train/valid # RETURNS TEST # base is a list base = load_pickle(f"../configs/configs-{comp_name}/useful_features_l_1.pkl") input_dict = load_pickle(f"../input/input-{comp_name}/input_dict.pkl") # just picking a random set no_rows = pd.read_parquet(f"../input/input-{comp_name}/test_{list(input_dict.keys())[1]}.parquet").shape[0] if all(x in base for x in useful_features): # all the features are in my_folds # we need xtest preprocessed xtest = None #xtest = pd.read_parquet(f"../input/input-{comp_name}/test.parquet") xtest, ordered_list = get_test(input_dict,comp_name, useful_features, no_rows) xtest= pd.DataFrame(xtest, columns= ordered_list)[useful_features] #print(xtest.iloc[:5,:7]) xtest = np.array(xtest.values) return xtest , useful_features elif any(x in base for x in useful_features): # part in my_folds part outside # we need xtest preprocessed xtest = None #xtest = pd.read_parquet(f"../input/input-{comp_name}/test.parquet") #features_in = list(set(input_dict['base']) & set(useful_features)) features_in = list(set(base) & set(useful_features)) #features_in = list(set(xtest.columns) & set(useful_features)) features_out = list(set(useful_features) - set(base)) xtest, features_in = get_test(input_dict,comp_name, features_in, no_rows) #xtest = np.array(xtest[features_in].values) rest_test = [] ###################################################################################################### """ is it in oof or in features useful_features: pred_e_121_fold5.pkl # experiment no is the unique identifier # one experiment can have multiple preds feat_l_1_f_12_std # feature no is the unique identifier # level no is good since we can have std feature # at two levels and it is good to seperate our features based on at which leve we are working # for predictions the TABLE has a level column for identifying level feat_l_1_f_12_mean """ # first filter features based on where they come from. useful_features_pred = [item for item in features_out if item.split("_")[0]=="pred"] useful_features_feat = [item for item in features_out if item.split("_")[0]=="feat"] # sanity check assert len(useful_features_pred) + len(useful_features_feat) == len(features_out) # collecting preds for f in useful_features_pred: try: rest_test.append(load_pickle(f"../configs/configs-{comp_name}/test_preds/test_{f}.pkl")) ordered_list.append(f) except: raise Exception(f"Feature: {f} not found") # collecting feats for f in useful_features_feat: try: rest_test.append(load_pickle(f"../configs/configs-{comp_name}/test_feats/test_{f}.pkl")) ordered_list.append(f) except: raise Exception(f"Feature: {f} not found") if rest_test == []: raise Exception("Error in bottleneck") rest_test = np.array(rest_test).T #rest_test = np.array(rest_test).reshape(-1,len(features_out)) del useful_features_feat, useful_features_pred gc.collect() ################################################################################################### xtest = np.concatenate((xtest, rest_test), axis=1) ordered_list = features_in + ordered_list xtest= pd.DataFrame(xtest, columns= ordered_list)[useful_features] xtest = np.array(xtest.values) return xtest, useful_features else: # all the features are present outside xtest = None # we need xtest preprocessed rest_test = [] ###################################################################################################### """ is it in oof or in features useful_features: pred_e_121_fold5.pkl # experiment no is the unique identifier # one experiment can have multiple preds feat_l_1_f_12_std # feature no is the unique identifier # level no is good since we can have std feature # at two levels and it is good to seperate our features based on at which leve we are working # for predictions the TABLE has a level column for identifying level feat_l_1_f_12_mean """ # first filter features based on where they come from. useful_features_pred = [item for item in useful_features if item.split("_")[0]=="pred"] useful_features_feat = [item for item in useful_features if item.split("_")[0]=="feat"] # sanity check assert len(useful_features_pred) + len(useful_features_feat) == len(useful_features) # collecting preds for f in useful_features_pred: try: rest_test.append(load_pickle(f"../configs/configs-{comp_name}/test_preds/test_{f}.pkl")) ordered_list.append(f) except: raise Exception(f"Feature: {f} not found") # collecting feats for f in useful_features_feat: try: rest_test.append(load_pickle(f"../configs/configs-{comp_name}/test_feats/test_{f}.pkl")) ordered_list.append(f) except: raise Exception(f"Feature: {f} not found") if rest_test == []: raise Exception("Error in bottleneck") xtest = np.array(rest_test).T #xtest = np.array(rest_test).reshape(-1,len(useful_features)) del useful_features_feat, useful_features_pred gc.collect() ################################################################################################### xtest= pd.DataFrame(xtest, columns= ordered_list)[useful_features] xtest = np.array(xtest.values) return xtest, useful_features def get_train(input_dict,id_folds_target, comp_name, useful_features, fold_name, istrain,optimize_on=[]): # optimize_on : [3,1] no_cols = id_folds_target.shape[1] features_base = list(id_folds_target.columns) features_in = [] for key,feat_list in input_dict.items(): if key == "id_folds_target": continue # if all(x in useful_features for x in feat_list): # pull whole set features_in += feat_list d1 = pd.read_parquet(f"../input/input-{comp_name}/train_{key}.parquet") id_folds_target=np.concatenate((id_folds_target, np.array(d1.values)), axis=1) # xtrain = np.array(my_folds[values].values) # xtrain= np.concatenate((xtrain, np.array(rest_train)), axis=1) elif any(x in useful_features for x in feat_list): # partial set present = list(set(feat_list) & set(useful_features)) features_in += present d1 = pd.read_parquet(f"../input/input-{comp_name}/train_{key}.parquet")[present] id_folds_target=np.concatenate((id_folds_target, np.array(d1.values)), axis=1) else: # none of values present pass d1 = None if fold_name is not None: # we are taking partial fold id_folds_target = pd.DataFrame(id_folds_target, columns=features_base+features_in) if istrain: # so train set everythign except id_folds_target = np.array(id_folds_target[~id_folds_target[fold_name].isin(optimize_on) ].values) else: # valid set id_folds_target = np.array(id_folds_target[id_folds_target[fold_name].isin(optimize_on) ].values) return id_folds_target[:,no_cols:],id_folds_target[:,no_cols-1], features_in else: # it is asking for full dataset return id_folds_target[:,no_cols:], id_folds_target[:,no_cols-1],features_in # xtrain, ytrain, features_in def bottleneck(comp_name,useful_features, fold_name, optimize_on, _state, return_type, verbose=0): ordered_list=[] # RETURNS XTRAIN, XVALID, YTRAIN, YVLID, XVALID_IDX locker = load_pickle(f"../configs/configs-{comp_name}/locker.pkl") #my_folds = pd.read_parquet(f"../input/input-{comp_name}/my_folds.parquet") #test = pd.read_parquet(f"../input/input-{comp_name}/test.parquet") # base is a list # base is the combined input_dict is splitted base = load_pickle(f"../configs/configs-{comp_name}/useful_features_l_1.pkl") id_folds_target = pd.read_parquet(f"../input/input-{comp_name}/id_folds_target.parquet") input_dict = load_pickle(f"../input/input-{comp_name}/input_dict.pkl") """ # NO NEED OF THIS CAME # it is a dictonary for each set of features created # each set has a unique name as key "l_1_f_0": ['column names generate', 'columns used to generate', 'title give to the process'] "e_30" : [column_names_generated, columns_name_used_to_generate] "nan_count": [['nan_f34','nan_32'], ['f_1', 'f_2']] "base" : [['f_1', 'f_4'], 0] : column names generate pred_l_1_e_0, mean_f_24 etc : columns used to generate : title given nan_count, exp_2 etc """ # First aim is to find where we have to search for the features is it in # my_folds or it is not # base is the list of all the column of my_folds a = load_pickle(f"../configs/configs-{comp_name}/features_dict.pkl") if all(x in base for x in useful_features): # all the features are in my_folds if verbose != 0: print("all the features are in my_folds") #my_folds = pd.read_parquet(f"../input/input-{comp_name}/my_folds.parquet") input_dict = load_pickle(f"../input/input-{comp_name}/input_dict.pkl") val_idx = None if _state in ["fold", "opt"]: # for seed we don't need to find val_idx val_idx = id_folds_target[id_folds_target[fold_name].isin(optimize_on)][locker["id_name"]].values.tolist() #val_idx = my_folds[my_folds[fold_name] == optimize_on][locker["id_name"]].values.tolist() if _state == "seed": xtrain, ytrain, temp_features = get_train(input_dict,id_folds_target.copy(), comp_name, useful_features, None, True,optimize_on) xtrain = pd.DataFrame(xtrain, columns=temp_features)[useful_features] xtrain = np.array(xtrain.values) #xtrain = np.array(my_folds[useful_features].values) #ytrain = np.array(my_folds[locker["target_name"]].values) xvalid = None yvalid = None val_idx = None return val_idx, xtrain, xvalid, ytrain, yvalid, useful_features # xvalid [2] xtrain [0,1,2,3,4] #fold5 # # # fold_nos = [i for i in range(int(fold_name.split("d")[1]))] # train_fold_nos = list(set(fold_nos) - set(optimize_on)) # print("This is train fold nos") # print(train_fold_nos) # print(optimize_on) # print("done") xtrain, ytrain, temp_features= get_train(input_dict,id_folds_target.copy(), comp_name, useful_features, fold_name, True,optimize_on) xtrain = pd.DataFrame(xtrain, columns=temp_features)[useful_features] xtrain = np.array(xtrain.values) #xtrain = np.array(my_folds[my_folds[fold_name] != optimize_on][useful_features].values) #ytrain = np.array(my_folds[my_folds[fold_name] != optimize_on][locker["target_name"]].values) xvalid, yvalid, temp_features= get_train(input_dict,id_folds_target.copy(), comp_name, temp_features, fold_name, False,optimize_on) xvalid = pd.DataFrame(xvalid, columns=temp_features)[useful_features] xvalid = np.array(xvalid.values) #xvalid = np.array(my_folds[my_folds[fold_name] == optimize_on][useful_features].values) #yvalid = np.array(my_folds[my_folds[fold_name] == optimize_on][locker["target_name"]].values) return val_idx, xtrain, xvalid, ytrain, yvalid, useful_features elif any(x in base for x in useful_features): # part in my_folds part outside if verbose != 0: print("part in my_folds part outside") #my_folds = pd.read_parquet(f"../input/input-{comp_name}/my_folds.parquet") features_in = list(set(base) & set(useful_features)) features_out = list(set(useful_features) - set(base)) rest_train = [] ###################################################################################################### """ is it in oof or in features useful_features: pred_e_121_fold5.pkl # experiment no is the unique identifier # one experiment can have multiple preds feat_l_1_f_12_std # feature no is the unique identifier # level no is good since we can have std feature # at two levels and it is good to seperate our features based on at which leve we are working # for predictions the TABLE has a level column for identifying level feat_l_1_f_12_mean """ # first filter features based on where they come from. useful_features_pred = [item for item in features_out if item.split("_")[0]=="pred"] useful_features_feat = [item for item in features_out if item.split("_")[0]=="feat"] # sanity check assert len(useful_features_pred) + len(useful_features_feat) == len(features_out) # This is the order they are accessed #ordered_list = features_in + useful_features_pred + useful_features_feat # collecting preds for f in useful_features_pred: try: rest_train.append(load_pickle(f"../configs/configs-{comp_name}/oof_preds/oof_{f}.pkl")) except: raise Exception(f"Feature: {f} not found") ordered_list += useful_features_pred # collecting feats for f in useful_features_feat: try: rest_train.append(load_pickle(f"../configs/configs-{comp_name}/train_feats/train_{f}.pkl")) except: raise Exception(f"Feature: {f} not found") ordered_list += useful_features_feat if rest_train == []: raise Exception("Error in bottleneck") rest_train = np.array(rest_train).T #rest_train = np.array(rest_train).reshape(-1,len(features_out)) #del useful_features_feat, useful_features_pred gc.collect() ################################################################################################### if _state == "seed": xtrain, ytrain, features_in= get_train(input_dict,id_folds_target.copy(), comp_name, features_in, None, True,optimize_on) #xtrain = np.array(my_folds[features_in].values) xtrain= np.concatenate((xtrain, np.array(rest_train)), axis=1) #ytrain = np.array(my_folds[locker["target_name"]].values) xvalid = None yvalid = None val_idx = None ordered_list += features_in xtrain= pd.DataFrame(xtrain, columns= ordered_list)[useful_features].values return val_idx, xtrain, xvalid, ytrain, yvalid, useful_features #xtrain, ytrain, features_in= get_train(input_dict,id_folds_target.copy(), comp_name, features_in, fold_name, True,optimize_on) #xtrain = np.array(my_folds[my_folds[fold_name] != optimize_on][features_in].values) #ytrain = np.array(my_folds[my_folds[fold_name] != optimize_on][locker["target_name"]].values) #xvalid, yvalid, features_in= get_train(input_dict,id_folds_target.copy(), comp_name, features_in, fold_name, False,optimize_on) #xvalid = np.array(my_folds[my_folds[fold_name] == optimize_on][features_in].values) #yvalid = np.array(my_folds[my_folds[fold_name] == optimize_on][locker["target_name"]].values) #mask = np.array((my_folds[fold_name] != optimize_on).values, dtype=bool) mask = np.array(~id_folds_target[fold_name].isin(optimize_on).values, dtype=bool) xtrain, ytrain, features_in= get_train(input_dict,id_folds_target.copy(), comp_name, features_in, fold_name, True,optimize_on) #xtrain= np.concatenate((xtrain, rest_train[mask].values), axis=1) xtrain= np.concatenate((xtrain, rest_train[mask]), axis=1) ordered_list = features_in + ordered_list #xtrain= np.concatenate((xtrain, np.array(rest_train[(my_folds[fold_name] != optimize_on).values])), axis=1) #ytrain = np.array(my_folds[my_folds[fold_name] != optimize_on][locker["target_name"]].values) mask = np.array(id_folds_target[fold_name].isin(optimize_on).values, dtype=bool) xvalid, yvalid, features_in= get_train(input_dict,id_folds_target.copy(), comp_name, features_in, fold_name, False,optimize_on) xvalid = np.concatenate((xvalid, rest_train[mask]), axis=1) #xvalid = np.array(my_folds[my_folds[fold_name] == optimize_on][features_in].values) #xvalid = np.concatenate((xvalid, np.array(rest_train[(my_folds[fold_name] == optimize_on).values])), axis=1) #yvalid = np.array(my_folds[my_folds[fold_name] == optimize_on][locker["target_name"]].values) val_idx = None if _state in ["fold", "opt"]: # for seed we don't need to find val_idx val_idx= id_folds_target[id_folds_target[fold_name].isin(optimize_on)][locker["id_name"]].values.tolist() #val_idx = my_folds[my_folds[fold_name] == optimize_on][locker["id_name"]].values.tolist() xtrain= pd.DataFrame(xtrain, columns= ordered_list)[useful_features].values xvalid= pd.DataFrame(xvalid, columns= ordered_list)[useful_features].values return val_idx, xtrain, xvalid, ytrain, yvalid, useful_features else: # all the features are present outside #my_folds = pd.read_parquet(f"../input/input-{comp_name}/my_folds.parquet") if verbose != 0: print("all the features are present outside") rest_train = [] # append 1D arrays ###################################################################################################### """ is it in oof or in features useful_features: pred_e_121_fold5.pkl # experiment no is the unique identifier # one experiment can have multiple preds feat_l_1_f_12_std # feature no is the unique identifier # level no is good since we can have std feature # at two levels and it is good to seperate our features based on at which leve we are working # for predictions the TABLE has a level column for identifying level feat_l_1_f_12_mean """ # first filter features based on where they come from. useful_features_pred = [item for item in useful_features if item.split("_")[0]=="pred"] useful_features_feat = [item for item in useful_features if item.split("_")[0]=="feat"] # sanity check assert len(useful_features_pred) + len(useful_features_feat) == len(useful_features) #ordered_list = useful_features_pred + useful_features_feat # collecting preds for f in useful_features_pred: try: rest_train.append(load_pickle(f"../configs/configs-{comp_name}/oof_preds/oof_{f}.pkl")) except: raise Exception(f"Feature: {f} not found") ordered_list += useful_features_pred # collecting feats for f in useful_features_feat: try: rest_train.append(load_pickle(f"../configs/configs-{comp_name}/train_feats/train_{f}.pkl")) except: raise Exception(f"Feature: {f} not found") ordered_list += useful_features_feat if rest_train == []: raise Exception("Error in bottleneck") # print(np.array(rest_train)[:3,:]) # print("check") # print(np.array(rest_train).T[:3,:]) # print("check2") # rest_train = np.array(rest_train).reshape(-1,len(useful_features)) # print(rest_train[:3,:]) rest_train = np.array(rest_train).T #del useful_features_feat, useful_features_pred #del useful_features gc.collect() ################################################################################################### if _state == "seed": xtrain= rest_train xtrain= pd.DataFrame(xtrain, columns= ordered_list)[useful_features].values ytrain = np.array(my_folds[locker["target_name"]].values) xvalid = None yvalid = None val_idx = None return val_idx, xtrain, xvalid, ytrain, yvalid , useful_features mask = np.array(~id_folds_target[fold_name].isin(optimize_on).values, dtype=bool) #mask = np.array((my_folds[fold_name] != optimize_on).values, dtype=bool) xtrain= rest_train[mask] ytrain = np.array(id_folds_target[mask][locker["target_name"]].values) mask = np.array(id_folds_target[fold_name].isin(optimize_on).values, dtype=bool) xvalid = rest_train[mask] yvalid = np.array(id_folds_target[mask][locker["target_name"]].values) val_idx = None if _state in ["fold", "opt"]: # for seed we don't need to find val_idx val_idx= id_folds_target[id_folds_target[fold_name].isin(optimize_on)][locker["id_name"]].values.tolist() #val_idx = my_folds[my_folds[fold_name] == optimize_on][locker["id_name"]].values.tolist() xtrain= pd.DataFrame(xtrain, columns= ordered_list)[useful_features].values xvalid= pd.DataFrame(xvalid, columns= ordered_list)[useful_features].values return val_idx, xtrain, xvalid, ytrain, yvalid , useful_features # def bottleneck_test(comp_name, useful_features, return_type, verbose=0): # ordered_list = [] # # There is a risk that we may not have same order of items in test vs train/valid # # RETURNS TEST # # base is a list # base = load_pickle(f"../configs/configs-{comp_name}/useful_features_l_1.pkl") # if all(x in base for x in useful_features): # # all the features are in my_folds # # we need xtest preprocessed # xtest = None # xtest = pd.read_parquet(f"../input/input-{comp_name}/test.parquet") # xtest = np.array(xtest[useful_features].values) # return xtest , useful_features # elif any(x in base for x in useful_features): # # part in my_folds part outside # # we need xtest preprocessed # xtest = None # xtest = pd.read_parquet(f"../input/input-{comp_name}/test.parquet") # features_in = list(set(xtest.columns) & set(useful_features)) # features_out = list(set(useful_features) - set(xtest.columns)) # xtest = np.array(xtest[features_in].values) # rest_test = [] # ###################################################################################################### # """ # is it in oof or in features # useful_features: # pred_e_121_fold5.pkl # experiment no is the unique identifier # one experiment can have multiple preds # feat_l_1_f_12_std # # feature no is the unique identifier # level no is good since we can have std feature # # at two levels and it is good to seperate our features based on at which leve we are working # # for predictions the TABLE has a level column for identifying level # feat_l_1_f_12_mean # """ # # first filter features based on where they come from. # useful_features_pred = [item for item in features_out if item.split("_")[0]=="pred"] # useful_features_feat = [item for item in features_out if item.split("_")[0]=="feat"] # # sanity check # assert len(useful_features_pred) + len(useful_features_feat) == len(features_out) # # collecting preds # for f in useful_features_pred: # try: # rest_test.append(load_pickle(f"../configs/configs-{comp_name}/test_preds/test_{f}.pkl")) # ordered_list.append(f) # except: # raise Exception(f"Feature: {f} not found") # # collecting feats # for f in useful_features_feat: # try: # rest_test.append(load_pickle(f"../configs/configs-{comp_name}/test_feats/test_{f}.pkl")) # ordered_list.append(f) # except: # raise Exception(f"Feature: {f} not found") # if rest_test == []: # raise Exception("Error in bottleneck") # rest_test = np.array(rest_test).reshape(-1,len(features_out)) # del useful_features_feat, useful_features_pred # gc.collect() # ################################################################################################### # xtest = np.concatenate((xtest, rest_test), axis=1) # ordered_list = features_in + ordered_list # xtest= pd.DataFrame(xtest, columns= ordered_list)[useful_features].values # return xtest, useful_features # else: # # all the features are present outside # xtest = None # # we need xtest preprocessed # rest_test = [] # ###################################################################################################### # """ # is it in oof or in features # useful_features: # pred_e_121_fold5.pkl # experiment no is the unique identifier # one experiment can have multiple preds # feat_l_1_f_12_std # # feature no is the unique identifier # level no is good since we can have std feature # # at two levels and it is good to seperate our features based on at which leve we are working # # for predictions the TABLE has a level column for identifying level # feat_l_1_f_12_mean # """ # # first filter features based on where they come from. # useful_features_pred = [item for item in useful_features if item.split("_")[0]=="pred"] # useful_features_feat = [item for item in useful_features if item.split("_")[0]=="feat"] # # sanity check # assert len(useful_features_pred) + len(useful_features_feat) == len(useful_features) # # collecting preds # for f in useful_features_pred: # print(f,"got it") # try: # rest_test.append(load_pickle(f"../configs/configs-{comp_name}/test_preds/test_{f}.pkl")) # ordered_list.append(f) # except: # raise Exception(f"Feature: {f} not found") # # collecting feats # for f in useful_features_feat: # try: # rest_test.append(load_pickle(f"../configs/configs-{comp_name}/test_feats/test_{f}.pkl")) # ordered_list.append(f) # except: # raise Exception(f"Feature: {f} not found") # if rest_test == []: # raise Exception("Error in bottleneck") # xtest = np.array(rest_test).reshape(-1,len(useful_features)) # del useful_features_feat, useful_features_pred # gc.collect() # ################################################################################################### # xtest= pd.DataFrame(xtest, columns= ordered_list)[useful_features].values # return xtest, useful_features # def bottleneck(comp_name,useful_features, fold_name, optimize_on, _state, return_type, verbose=0): # ordered_list=[] # # RETURNS XTRAIN, XVALID, YTRAIN, YVLID, XVALID_IDX # locker = load_pickle(f"../configs/configs-{comp_name}/locker.pkl") # #my_folds = pd.read_parquet(f"../input/input-{comp_name}/my_folds.parquet") # #test = pd.read_parquet(f"../input/input-{comp_name}/test.parquet") # # base is a list # base = load_pickle(f"../configs/configs-{comp_name}/useful_features_l_1.pkl") # """ # # NO NEED OF THIS CAME # # it is a dictonary for each set of features created # # each set has a unique name as key # "l_1_f_0": ['column names generate', 'columns used to generate', 'title give to the process'] # "e_30" : [column_names_generated, columns_name_used_to_generate] # "nan_count": [['nan_f34','nan_32'], ['f_1', 'f_2']] # "base" : [['f_1', 'f_4'], 0] # : column names generate pred_l_1_e_0, mean_f_24 etc # : columns used to generate # : title given nan_count, exp_2 etc # """ # # First aim is to find where we have to search for the features is it in # # my_folds or it is not # # base is the list of all the column of my_folds # a = load_pickle(f"../configs/configs-{comp_name}/features_dict.pkl") # if all(x in base for x in useful_features): # # all the features are in my_folds # if verbose != 0: # print("all the features are in my_folds") # my_folds = pd.read_parquet(f"../input/input-{comp_name}/my_folds.parquet") # val_idx = None # if _state in ["fold", "opt"]: # # for seed we don't need to find val_idx # val_idx = my_folds[my_folds[fold_name] == optimize_on][locker["id_name"]].values.tolist() # if _state == "seed": # xtrain = np.array(my_folds[useful_features].values) # ytrain = np.array(my_folds[locker["target_name"]].values) # xvalid = None # yvalid = None # val_idx = None # return val_idx, xtrain, xvalid, ytrain, yvalid, useful_features # xtrain = np.array(my_folds[my_folds[fold_name] != optimize_on][useful_features].values) # ytrain = np.array(my_folds[my_folds[fold_name] != optimize_on][locker["target_name"]].values) # xvalid = np.array(my_folds[my_folds[fold_name] == optimize_on][useful_features].values) # yvalid = np.array(my_folds[my_folds[fold_name] == optimize_on][locker["target_name"]].values) # return val_idx, xtrain, xvalid, ytrain, yvalid, useful_features # elif any(x in base for x in useful_features): # # part in my_folds part outside # if verbose != 0: # print("part in my_folds part outside") # my_folds = pd.read_parquet(f"../input/input-{comp_name}/my_folds.parquet") # features_in = list(set(my_folds.columns) & set(useful_features)) # features_out = list(set(useful_features) - set(my_folds.columns)) # rest_train = [] # ###################################################################################################### # """ # is it in oof or in features # useful_features: # pred_e_121_fold5.pkl # experiment no is the unique identifier # one experiment can have multiple preds # feat_l_1_f_12_std # # feature no is the unique identifier # level no is good since we can have std feature # # at two levels and it is good to seperate our features based on at which leve we are working # # for predictions the TABLE has a level column for identifying level # feat_l_1_f_12_mean # """ # # first filter features based on where they come from. # useful_features_pred = [item for item in features_out if item.split("_")[0]=="pred"] # useful_features_feat = [item for item in features_out if item.split("_")[0]=="feat"] # # sanity check # assert len(useful_features_pred) + len(useful_features_feat) == len(features_out) # # This is the order they are accessed # ordered_list = features_in + useful_features_pred + useful_features_feat # # collecting preds # for f in useful_features_pred: # try: # rest_train.append(load_pickle(f"../configs/configs-{comp_name}/oof_preds/oof_{f}.pkl")) # except: # raise Exception(f"Feature: {f} not found") # # collecting feats # for f in useful_features_feat: # try: # rest_train.append(load_pickle(f"../configs/configs-{comp_name}/train_feats/train_{f}.pkl")) # except: # raise Exception(f"Feature: {f} not found") # if rest_train == []: # raise Exception("Error in bottleneck") # rest_train = np.array(rest_train).reshape(-1,len(features_out)) # del useful_features_feat, useful_features_pred # gc.collect() # ################################################################################################### # if rest_train == []: # raise Exception("Error in bottleneck") # rest_train = np.array(rest_train).reshape(-1,len(features_out)) # if _state == "seed": # xtrain = np.array(my_folds[features_in].values) # xtrain= np.concatenate((xtrain, np.array(rest_train)), axis=1) # ytrain = np.array(my_folds[locker["target_name"]].values) # xvalid = None # yvalid = None # val_idx = None # xtrain= pd.DataFrame(xtrain, columns= ordered_list)[useful_features].values # return val_idx, xtrain, xvalid, ytrain, yvalid, useful_features # xtrain = np.array(my_folds[my_folds[fold_name] != optimize_on][features_in].values) # ytrain = np.array(my_folds[my_folds[fold_name] != optimize_on][locker["target_name"]].values) # xvalid = np.array(my_folds[my_folds[fold_name] == optimize_on][features_in].values) # yvalid = np.array(my_folds[my_folds[fold_name] == optimize_on][locker["target_name"]].values) # mask = np.array((my_folds[fold_name] != optimize_on).values, dtype=bool) # xtrain= np.concatenate((xtrain, np.array(rest_train[(my_folds[fold_name] != optimize_on).values])), axis=1) # ytrain = np.array(my_folds[my_folds[fold_name] != optimize_on][locker["target_name"]].values) # xvalid = np.array(my_folds[my_folds[fold_name] == optimize_on][features_in].values) # xvalid = np.concatenate((xvalid, np.array(rest_train[(my_folds[fold_name] == optimize_on).values])), axis=1) # yvalid = np.array(my_folds[my_folds[fold_name] == optimize_on][locker["target_name"]].values) # val_idx = None # if _state in ["fold", "opt"]: # # for seed we don't need to find val_idx # val_idx = my_folds[my_folds[fold_name] == optimize_on][locker["id_name"]].values.tolist() # xtrain= pd.DataFrame(xtrain, columns= ordered_list)[useful_features].values # xvalid= pd.DataFrame(xvalid, columns= ordered_list)[useful_features].values # return val_idx, xtrain, xvalid, ytrain, yvalid, useful_features # else: # # all the features are present outside # my_folds = pd.read_parquet(f"../input/input-{comp_name}/my_folds.parquet") # if verbose != 0: # print("all the features are present outside") # rest_train = [] # append 1D arrays # ###################################################################################################### # """ # is it in oof or in features # useful_features: # pred_e_121_fold5.pkl # experiment no is the unique identifier # one experiment can have multiple preds # feat_l_1_f_12_std # # feature no is the unique identifier # level no is good since we can have std feature # # at two levels and it is good to seperate our features based on at which leve we are working # # for predictions the TABLE has a level column for identifying level # feat_l_1_f_12_mean # """ # # first filter features based on where they come from. # useful_features_pred = [item for item in useful_features if item.split("_")[0]=="pred"] # useful_features_feat = [item for item in useful_features if item.split("_")[0]=="feat"] # # sanity check # assert len(useful_features_pred) + len(useful_features_feat) == len(useful_features) # ordered_list = useful_features_pred + useful_features_feat # # collecting preds # for f in useful_features_pred: # try: # rest_train.append(load_pickle(f"../configs/configs-{comp_name}/oof_preds/oof_{f}.pkl")) # except: # raise Exception(f"Feature: {f} not found") # # collecting feats # for f in useful_features_feat: # try: # rest_train.append(load_pickle(f"../configs/configs-{comp_name}/train_feats/train_{f}.pkl")) # except: # raise Exception(f"Feature: {f} not found") # if rest_train == []: # raise Exception("Error in bottleneck") # rest_train = np.array(rest_train).reshape(-1,len(useful_features)) # del useful_features_feat, useful_features_pred # del useful_features # gc.collect() # ################################################################################################### # if _state == "seed": # xtrain= rest_train # xtrain= pd.DataFrame(xtrain, columns= ordered_list)[useful_features].values # ytrain = np.array(my_folds[locker["target_name"]].values) # xvalid = None # yvalid = None # val_idx = None # return val_idx, xtrain, xvalid, ytrain, yvalid , useful_features # mask = np.array((my_folds[fold_name] != optimize_on).values, dtype=bool) # xtrain= rest_train[(my_folds[fold_name] != optimize_on).values] # ytrain = np.array(my_folds[my_folds[fold_name] != optimize_on][locker["target_name"]].values) # xvalid = rest_train[(my_folds[fold_name] == optimize_on).values] # yvalid = np.array(my_folds[my_folds[fold_name] == optimize_on][locker["target_name"]].values) # val_idx = None # if _state in ["fold", "opt"]: # # for seed we don't need to find val_idx # val_idx = my_folds[my_folds[fold_name] == optimize_on][locker["id_name"]].values.tolist() # xtrain= pd.DataFrame(xtrain, columns= ordered_list)[useful_features].values # xvalid= pd.DataFrame(xvalid, columns= ordered_list)[useful_features].values # return val_idx, xtrain, xvalid, ytrain, yvalid , useful_features # https://www.kaggle.com/competitions/amex-default-prediction/discussion/332575#1829172 import pathlib class SaveModelCallback: def __init__(self, models_folder: pathlib.Path, fold_id: int, min_score_to_save: float, every_k: int, order: int = 0): self.min_score_to_save: float = min_score_to_save self.every_k: int = every_k self.current_score = min_score_to_save self.order: int = order self.models_folder: pathlib.Path = models_folder self.fold_id: int = fold_id def __call__(self, env): iteration = env.iteration score = env.evaluation_result_list[3][2] if iteration % self.every_k == 0: print(f'iteration {iteration}, score={score:.05f}') if score > self.current_score: self.current_score = score for fname in self.models_folder.glob(f'fold_id_{self.fold_id}*'): fname.unlink() print(f'High Score: iteration {iteration}, score={score:.05f}') joblib.dump(env.model, self.models_folder / f'fold_id_{self.fold_id}_{score:.05f}.pkl') def save_model2(models_folder: pathlib.Path, fold_id: int, min_score_to_save: float = 0.78, every_k: int = 50): return SaveModelCallback(models_folder=models_folder, fold_id=fold_id, min_score_to_save=min_score_to_save, every_k=every_k) def save_model1(): def callback(env): with open(os.path.join(sys.path[0], "ref.txt"), "r") as x: for i in x: comp_name = i x.close() max_score = global_variables.max_score iteration = env.iteration score = env.evaluation_result_list[0][2] if iteration % 100 == 0: print('iteration {}, score= {:.05f}'.format(iteration,score)) if score > max_score: max_score = score path = f"../models/models-{comp_name}/callback_logs/lgb_models_e_{global_variables.exp_no}_f_{global_variables.counter}_{global_variables._state}/" #'Models/' # if path don't exists create one # if exists throw error or delete it first for fname in os.listdir(path): if fname.startswith("fold_{}".format(global_variables.fold)): os.remove(os.path.join(path, fname)) print('High Score: iteration {}, score={:.05f}'.format(iteration, score)) joblib.dump(env.model, f"../models/models-{comp_name}/callback_logs/lgb_models_e_{global_variables.exp_no}_f_{global_variables.counter}_{global_variables._state}/"+'{:.05f}.pkl'.format(score)) global_variables.max_score = max_score callback.order = 0 return callback def mkdir_from_path(path): if not os.path.exists(path): os.mkdir(path) else: print(f"Folder: {path} already exists, do you want to overwrite?") print("Enter Y/y for overwrite or any other button to terminate") print(": ",end="") a = input() if a.upper() == "Y": # overrite print("Overwritting...") else: raise Exception("Folder already exists so process Terminated!!!") return path if __name__ == "__main__": comp_name= 'amex3' useful_features=['B_1_last', 'B_11_last', 'B_12_last','pred_e_5_fold5'] fold_name = "fold3" optimize_on= [2] _state = "seed" # opt: idx, train, valid, [test] # fold: idx, train, valid, test # seed: train, test return_type = "numpy_array" # self.val_idx, self.xtrain, self.xvalid, self.ytrain, self.yvalid, self.ordered_list_train val_idx, xtrain, xvalid, ytrain, yvalid, my_list = bottleneck(comp_name,useful_features, fold_name, optimize_on, _state, return_type,0) print(val_idx) print() print(xtrain) print() print(xvalid) print() print(ytrain) print() print(yvalid) print() print(my_list)

File no 43: /README.md
[Framework3] Machine Learning Pipeline The Pipeline is quite modular in nature following py37 guidelines. It uses OOPs concepts which makes it quite robust and easy to debug. I have tried to make this pipeline not too abstract, so that the user can have full control over it. The main goal of it is to help keep track of all the experiements in an organized way and automate repetitive tasks like creating folds, creating OOF and TEST predictions from an experiment. Models Supported lgr,lir, xgb,xgbc, xgbr, cbc, cbr, mlpc, rg, ls, knnc, dtc, adbc, gbmc , gbmr, hgbc, lgb, lgbmc, lgbmr, rfc , rfr, tabnetr, tabnetc, k1, k2, k3, tez1, tez2, p1, pretrained Key Features Of The Pipeline 1.Visualize Experimentations Visualize effect of various feature groups and and preprocessing techniques on the score. Visualize how various algorithms [here optuna] does hyperparameter tuning through various trials. Demo: https://www.kaggle.com/code/raj401/eda-experiments-tmay 2.Automates Hyperparameter tuning, experimentation, OOF and TEST predictions. Demo: https://www.kaggle.com/code/raj401/inference-mnist !python experiment.py # finds optimal hyperparameters !python predict.py # creates OOF and TEST predictions !python output.py # creates submission.csv file 3.Maintains a table of all the experiments. Demo: https://www.kaggle.com/code/raj401/eda-experiments-tmay How to use it Environment setup Folder Structure It allows working on different competitions together. Here example: tmay and amex Framework3/ | ├── configs/ | |__ configs-tmay/ | |__ configs-amex/ | ├── input/ | |__ input-tmay/ | |__ input-amex/ | ├── models/ | |__ models-tmay/ | |__ models-amex/ | ├── src_framework3/ | └── working/ Change your working directory to src_framework3. src_framework3 ├── create_folds.py ├── custoom_classes.sh ├── custom_models.py ├── experiment.py ├── feature_generator.py ├── feature_picker.py ├── grab.py ├── info.txt ├── keys.py ├── metrics.py ├── model_dispatcher.py ├── optuna_search.py ├── output.py ├── predict.sh ├── ref.txt ├── run.sh └── utils.py Dependencies Install below libraries manually or run command sh run.sh. Note: to run the sh script make sure your current working directory is src_framework3 pip install optuna pip install catboost pip install timm pip install pretrainedmodels pip install -Iv tez==0.6.0 pip install torchcontrib pip install iterative-stratification TABULAR COMPETITION (classification/regression) [STEPS] STEP-->1 : First set ref STEP-->2 : run init_folders.py --> create_datasets.py STEP-->3 : put train.parquet, test.parquet, sample.parquet in input folder [requirement] train:- id_col, features, target ( train may or may not have id column but test must have id column) [if not in parquet then convert using csv_to_parquet.py] [Now see once input content train, sample, test using show_input.py] test:- id_col, features sample:- id_col, target STEP-->4 : run keys.py STEP-->5 : run create_folds.py [ Create New id columns for train] [ No need to sort test as test will always have submission with id column just never RESHUFFLE] [ Sort train by id column, if not create one by reshuffling since when we get folds we will also sort them] [No need to sort test, we just predict on them, no training] train--------[ converted to ]------------->my_folds After this what should we have these files in input folder # what we did is train------------------->my_folds my_folds:- id_col, features, target , fold_cols test:- id_col, features sample:- id_col, target_col [You may now remove train folder] check once everything is as it should be using show_input.py STEP-->6 : run experiment.py / auto_exp.py we should pass list of optimize_on in run() We won't be predicting for all the experiments we do. So keep log_exp_22.pkl file in a subfolder also make seperate folder for preds: oof_preds, test_preds After 5-6 hr do plot the auto table to see which set performs well and you can limit the search in that direction Like "f_base", "f_max" performs quite well STEP-->7 : run predict.py or run seed_it.py # calls opt() function but not run() so keep it that way as obj() don't require optimize_on Note : run() takes list optimize_on : obj() takes single integer optimize_on for each fold : both takes fold_name STEP-->8 : run output.py after running predict.py STEP-->9 : make submission submit.py kaggle competitions list kaggle competitions leaderboard amex-default-prediction --show | --download kaggle competitions submisssions amex-default-prediction kaggle competitions submit ventilator-pressure-prediction -f submission.csv -m "exp_{}_fold/single/all" #submit your submission.csv Note: all parquet file contains id and target , all pkl files contain 1d prediction STEP-->10 : auto_exp , It automatically does experimentations by selecting different subsets of feature groups and preprocessing techniques. IMAGE COMPETITION (classification) [STEPS] [Case1] Image stored as dataframe STEP-->1 : First set ref [note: image_df: image pixels are stored as dataframe ] STEP-->2 : move train.csv ,test.csv, sample.csv to models_ [make name train,test,sample] STEP-->3 :decide: id_name : as that of train id columns target_name : as that of sample target column -----------Make format like below exactly ---------------------- train: ImageId, Label, pixel0, pixel1, pixel2, ... , pixel200, test: ImageId, pixel0, pixel1, pixel2, ... , pixel200 sample: ImageId, Label STEP-->4 :run keys.py after setting appropriate name of variables STEP-->5 :run create_folds.py to create [my_folds.csv] [Case1] Image path stored in dataframe image_path: there is train.csv and sample.csv folder which contains image name and there are image folders initially> (before putting image ID column do sample(frac=1)) train.csv: image_id, target sample.csv: image_id, fake_target target_name >> sample target name id_name >> sample id name STEP-->1 : move train.csv to models_ by first rename id_name to image1.jpeg STEP-->2 : move sample.csv to models_ as test.csv by first renaming id_name to image2.jpeg STEP-->3 : move sample to models_ [image_id, target] STEP-->4 : run keys.py after setting appropriate name of variables STEP-->5 : run create_folds.py to create [my_folds.csv] [Case3] Image Folder ..[work in progress..] Go You can’t perform that action at this time.

4: Project Name : CutMix-Regularization-Strategy-to-Train-Strong-Classifiers-with-Localizable-Features
This project contains 1 main files, namely /README.md
Below we will get details about each of these files one by one:
	
File no 1: /README.md
CutMix-Regularization Through this project we will try to understand CutMix Augmentation by implementing it on a simple problem of cat-vs-dog classification. Implementation I have used keras to implement the cutmix augmentation. Dataset used: https://www.kaggle.com/competitions/dogs-vs-cats/data. In CutMix we mix two images and their labels respectively in order to generate new data point. To implement this, I have created a custom datagenerator. This custom datagenerator takes batches from two seperate keras datagenerator and returns one batch by applying cutmix on it. train datagenerator (custom) train_dataset = CutMixImageDataGenerator( generator1= train_datagen1, #train_generator1, generator2= train_datagen2, #train_generator2, img_size=128, batch_size=32, ) valid datagenerator (default) valid_dataset = valid_datagen.flow_from_dataframe( dataframe=validate_df, directory=image_path, target_size=(128, 128), x_col='filename', y_col='category', batch_size=32, seed=42, shuffle=True, class_mode="categorical", ) Results Type Without CutMix With CutMix Output Training and Validation Accuracy plot Score loss: 0.7671 - acc: 0.6406 val_loss: 0.9630 - val_acc: 0.6445 loss: 0.1226 - acc: 0.9538 val_loss: 0.4688 - val_acc: 0.8438 #epochs 50 50 Early Stop at 28 15 Conclusion We have two notebooks 1. Without CutMix and 2. With CutMix. Both notebooks have everything exactly same(seed, batchsize, #epochs, rescaling etc) only except the training datagenerator. And we notice that using CutMix augmentation has really improved the validation score making the model more regularized and robust in comparision to model which is trained without using CutMix augmentation. Reference original paper: https://arxiv.org/abs/1905.04899 Go You can’t perform that action at this time.
